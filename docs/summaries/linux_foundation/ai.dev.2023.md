## Keynote: How LLMs Have Changed the Way Organizations Do AI - Robert Nishihara, Anyscale

URL: [https://www.youtube.com/watch?v=-bowcqswAbI](https://www.youtube.com/watch?v=-bowcqswAbI)

 * Robert Nishara, co-founder and CEO of Anyscale, discusses how large language models (LLMs) are changing the way organizations approach AI
* Previously, AI capabilities were limited by the size of data that could be processed on a single GPU; LLMs have made it possible to work with larger datasets and multimodal data (text, image, video)
* Organizations have been adopting AI for years, but the transition from classical machine learning models to deep learning has required significant investment in resources and capabilities
* Generative AI is becoming increasingly important, and organizations are starting to use LLMs throughout their businesses to extend platform capability
* Infrastructure challenges are growing as more complex applications require diverse hardware ecosystems; GPUs are getting better but still not enough for some use cases
* The future of AI adoption requires readiness for the speed at which it's changing
* Many organizations have been practicing AI for a long time, but may find they lack the capability to fully take advantage of LLMs
* The process of adopting AI involves a transition from building models to building features and shipping products
* Organization-wide adoption of LLMs often happens in two phases: model training and deployment, followed by customization and optimization for specific use cases
* Experimentation speed and validation of product market fit are crucial factors in the second phase of the journey
* Companies need to invest heavily in evaluation and iteration to keep up with the rapidly changing landscape of LLMs.
* Quality is now a top consideration when evaluating models, but cost, latency, and user experience are also important dimensions.
* Finetuning has become a pervasive technique for improving AI applications, with finetuned LLM models outperforming earlier versions at a fraction of the cost.


## Keynote: Enabling AI with Cloud Native - Jorge Castro, Cloud Native Computing Foundation

URL: [https://www.youtube.com/watch?v=7vgoR4U2CTI](https://www.youtube.com/watch?v=7vgoR4U2CTI)

 * George Castro, Cloud Native Computing Foundation: Enabling AI with Cloud Native
* Worked in Linux era, moved to Kubernetes and Cloud Native projects
* CNCF project team member for 174 projects, focusing on innovation and sustainability
* Discussing use of cloud native technologies for production-level High Throughput Computing (HTC) and AI workloads
* Over 20,000 nodes with 100,000 jobs scheduled per minute
* Kubernetes turning 10 next year: Primitives, extensibility, API driven, user friendly
* Hardware enablement: Dynamic resource allocation (alpha), multitenancy, improved multi-node alignment
* Workflow enablement: Volcano project for batch workloads, massively parallel processing
* Application development with CNCF projects like Carada and Kubernetes Incubation
* Discussing Cube Edge, Sedna, and pushing AI workload to the edge
* Relationship between OCI artifacts and container registries
* Invitation for building scaffolding and collaboration in the Kubernetes community.


## Keynote: State of Open GenAI - Jim Zemlin, Executive Director, The Linux Foundation

URL: [https://www.youtube.com/watch?v=JX_b3pdGc9U](https://www.youtube.com/watch?v=JX_b3pdGc9U)

 * Jim Zemlin, Executive Director of The Linux Foundation, discussing the importance of openness in generative AI
* Generative AI is impacting various industries, including mundane tasks and medical research
* Openness is crucial for generative AI as it was for the early days of the internet
* There's a trend towards closed Foundation models, which could stifle innovation and competition
* Regulatory concerns around open source generative AI are unconvincing compared to real concerns like bias, ethics, hallucination, and data privacy
* Openness allows for transparency, trust, and attribution in the generative AI ecosystem
* Data ownership and participation value chain are important issues in the generative AI economy
* The Linux Foundation is studying the big picture perspective on generative AI and openness
* Security and data privacy concerns are holding some organizations back from adopting generative AI
* Open source tools are preferred over proprietary solutions for building generative AI technology
* Transparency from open source gives confidence to businesses in making decisions about using generative AI
* European Union Regulators have released a draft regulation that restricts open source Foundation models, but the final legislation is not yet clear
* The Linux Foundation aims to continue providing a neutral home for first-class open source tools in the generative AI space.


## Keynote: Enterprise Value with Generative AI - Gourab De, Weights and Biases

URL: [https://www.youtube.com/watch?v=LPMP7hYDvtI](https://www.youtube.com/watch?v=LPMP7hYDvtI)

 * Gourab De from Weights and Biases speaks about the evolving landscape of generative AI in enterprise value
* Generative AI is constantly evolving, with open source models becoming more common and complex
* Challenges of working with generative AI include:
  + Choosing the best solution among a vast number of options
  + Interacting with the model in a constantly changing environment
  + Ensuring continuous evaluation, documentation, governance, and security
* Orchestration systems are needed to help manage the complexity and scale of working with generative AI models
* Evaluation is important for understanding the performance and behavior of foundation models
* Flexibility and automation are key for effectively using generative AI in a variety of contexts
* Different tools may be needed to capture different aspects of experiments and results
* Continuous monitoring, debugging, and visualization are necessary for effective use of generative AI in large-scale environments.


## Keynote : The Open Source AI Alliance - Anthony J. Annunziata, Director, AI Open Innovation, IBM

URL: [https://www.youtube.com/watch?v=MTonKJaDM48](https://www.youtube.com/watch?v=MTonKJaDM48)

 * Introduction to the AI Alliance program, a collaboration between various organizations
* Discussion on the shift from proprietary AI models to open source in recent years
* Examples of successful open source AI projects like Llama and Hugging Face
* Challenges with open source AI including resources required, safety concerns, and regulatory environment
* Importance of open source AI for research and development, education, and advocacy
* Goals of the AI Alliance: enabling innovation, open science, and open source AI; making technologies accessible and usable; advocating for policy makers and business leaders.
* The Alliance's vision is to bring people and resources together to enhance and grow open source AI development.
* Structured as a self-driven organization with projects spanning six categories: education and skill building, model development, application development, safety and trust, deployment, and enabling choice.
* The program aims for a lightweight approach with strong governance and collaboration across the ecosystem.
* Goals include building solutions, standards, benchmarks, and tools; ensuring existing frameworks work together; and democratizing access to different forms of hardware.
* Importance of language culture alignment and advocacy for open source AI in the broader community.


## Keynote: Building AI with Open Source & Hugging Face - Jeff Boudier, Product & Growth, Hugging Face

URL: [https://www.youtube.com/watch?v=O7KbwmaK9Ck](https://www.youtube.com/watch?v=O7KbwmaK9Ck)

 * Hugging Face: mission is to democratize machine learning, built on Community First principle and Ethics first
* Reasons for using open source to build AI applications with Hugging Face:
  + Community progress: access to top models and benchmarks, fine-tuning techniques like DPO
  + Cost efficiency: control costs by running models efficiently on laptops or servers
  + Security: host models securely without sending customer information to third parties
  + Version Control: track every change made to publicly visible models
  + Trust: verify and trust open models with openly accessible training data sets and code
* Hugging Face is known for its popular Transformers Library, a prolific open source developer in the machine learning space
* Hugging Face hosts over a million repository models and datasets, covering every modality and discipline of machine learning (NLP, Vision, Audio)
* The platform generates enormous activity with 10 million downloads of Transformer models per day
* Reasons to use open source for building AI applications:
  + Future-proof technology: not locked into particular vendor
  + Control over model infrastructure, security, compliance, and versioning
  + Ability to trust, verify, and build upon open models with publicly accessible training data sets and code
* Hugging Face offers a large language model leaderboard for constant evaluation of thousands of industry benchmarks
* Text generation inference production solutions: Max throughput model production workbox and Mistral latency-optimized hardware solutions (including Nvidia GPUs, AMD GPUs, AWS Inferentia)
* Hugging Face offers an inference endpoint for easy deployment without dealing with hardware
* The platform also provides a chat application called Hugging Face ChatOps for text generation and question answering using models like Llama 2, Falcon, and Diffusion.
* Hugging Face offers user interfaces (like the illusion diffusion demo) to easily build and showcase machine learning models in popular spaces like image generation
* The platform provides resources for front-end developers, such as the Hugging Face JS library, to run models in a browser or host them directly
* Backend development: Hugging Face is building a new framework called Candle (base in Rust) for developers to build machine learning natively in Rust
* Illusion Fus diffusion: popular space for quick AI model building with Hugging Face
* Hugging Face offers expert support services through Gabrieli and Jeff.


## Keynote: Data = AI: Leading the Future of Gen AI with Apache Cassandra - Chet Kapoor, DataStax

URL: [https://www.youtube.com/watch?v=flaFAjXXuK8](https://www.youtube.com/watch?v=flaFAjXXuK8)

 * Chet Kapoor, DataStax CEO, began by joking about AI replacing humans
* Cassandra Summit 2023 brings community back together after a long hiatus
* Cassandra started in 2012 as a small company, has grown significantly since
* DataStax has contributed to numerous innovations in the data stack
* Vinod Kosior's quote "the job market will beget a getting machine" inspires Chet that AI will bring massive change
* Companies are adopting AI at different rates, with Enterprises being more cautious due to regulatory pressure
* Gen Z companies are embracing AI for transformative revenue use cases like human-agent interactions and recommendation systems
* The CIO's role is evolving, with developers leading the decision on AI stacks
* Open source databases like Cassandra will be crucial in the future of AI and data clouds
* Companies like Amazon are using Cassandra to build gen apps quickly and efficiently
* Innovations like instantaneous indexing, relevance, and affordability are important for gen apps
* Performance testing results show that Cassandra, based on Apache Cassandra, outperforms other databases significantly.


## Keynote: Unlocking Corporate Data with Retrieval-Augmented Generation - Jerry Liu, LlamaIndex

URL: [https://www.youtube.com/watch?v=lsKUqO5R8Ig](https://www.youtube.com/watch?v=lsKUqO5R8Ig)

 * Jerry Liu, CEO of LlamaIndex, discussing Retrieval-Augmented Generation (RAG) for complex question answering
* L index framework connects data to Language Models (LM), creating production-grade applications
* RAG is a conceptual framework for QA systems made up of two main concepts: data injection and parsing, as well as data querying
* Source document processing involves putting the document into a vector database like Sandra, which contains embeddings
* Retrieval synthesis pulls information from the storage system and puts it into a context window model framework
* Llama Index provides a simple starting point with five lines of code for RAG pipelines
* Use cases include Enterprise, where there are challenges building RAG systems, especially for complex questions
* Naive RAG setups tend to work well for simple questions and specific facts within documents
* For complex questions or open-domain chat-like queries, naive RAG setups fail
* Llama Index helps address pain points by extending RAG with an agent layer that understands the query and generates sub-questions
* Agent extension RAG puts a layer in front of the RAG pipeline to interact with the data system
* The agent reasoning loop crunches input tasks and generates responses dynamically
* Llama Index is a general framework for thinking about agents that interact with data systems, including an underlying storage system and query data example
* Resources for getting started include SEC Insight, which allows asking questions of financial documents within LW index, and a full stack application template called LW index Plus.


## Keynote: Build Accelerated LLM Applications with Open Source Tools from NVIDIA - Ankit Patel, NVIDIA

URL: [https://www.youtube.com/watch?v=nSSuHY2rRb4](https://www.youtube.com/watch?v=nSSuHY2rRb4)

 * Ankit Patel from NVIDIA, senior director of product marketing for software development kits, talks about using open source tools from NVIDIA to build AI projects
* NVIDIA has over 500 open source projects and serves various industries with its software
* Discusses the importance of understanding the full stack in accelerated computing
* Mentions some popular open source tools used for data processing: NVIDIA Rapids (ETL, ML), Accelerator CUDF (accelerates Pandas code), Spark
* Megatron LM is a project to train massive language models using thousands of GPUs, sharing work across multiple computers
* Nemo toolkit is an open source toolkit for ASR, TTS, and NLP tasks
* Triton Inference Server is an open source inference serving solution that supports various modalities and processors
* RAG (Rapids Accelerated Graph) is a reusable accelerated function tool for vector database and embedding models
* Nemo Guardrail is a tool to control large language model applications by creating instruction rails to prevent toxicity or bias.
* The talk covers the basics of an LLM workflow, from data processing to model training and inference.


## Keynote: Accelerating AI/ML Development with Open Source Software - Brian Granger, AWS

URL: [https://www.youtube.com/watch?v=qKuqxM2O55s](https://www.youtube.com/watch?v=qKuqxM2O55s)

 * Brian Granger, senior principal technologist at AWS, discussing accelerating AI/ML development using open source software on AWS
* Open source software is popular in AI/ML due to rapid innovation and access to innovation, but customers face challenges with configuration, deployment, operation, and sustainability
* AWS SageMaker addresses these challenges by offering integration of popular open source software like PyTorch, Jupyter Notebook, Spark, and Transformers library (hugging face)
* AWS also participates in and contributes to open source projects such as PyTorch, Jupyter Notebook, and Langchain
* Project Jupiter: Jupyter Notebook extension for scheduling notebook jobs, vendor neutral with support for multiple model providers, transparent and traceable, and extensible.
* AWS SageMaker offers integration with Jupyter Notebook AI playground and chat UI for generative AI assistant code generation and collaboration.
* AWS contributes to open source projects like Jupyter Notebook, PyTorch, and Langchain, focusing on areas like scheduling, model support, and transparency.
* AWS also offers high-performance S3 connector for PyTorch for faster data loading and model checkpointing in S3 instead of local disk.


## Keynote: Introducing Apache Cassandra® 5! - Patrick McFadin, Scott Andreas & Ekaterina Dimitrova

URL: [https://www.youtube.com/watch?v=xwMr5aH3Sqk](https://www.youtube.com/watch?v=xwMr5aH3Sqk)

 * Cassandra 5 beta 1 has been released, with RC and GA coming soon
* Asset transaction portion of Cassandra decoupled from CTM (Transactional Cluster Metadata) in version 5.1
* Cassandra 3x branch will reach end-of-life soon; new certification available with updated features including asset transaction support
* MVP program updated, first batch of nominations accepted
* New features include: storage attached indexing, unified compaction strategy, tightly coupled storage engine, and Vector search
* Documentation has been updated on the Apache Cassandra website
* Conflict recording is available for sessions at the conference
* Future features include: distributed transaction database protocol (APDC), a leaderless consensus library, atomic visibility protocol, and large-scale simulation infrastructure.
* Cassandra supports scaling to petabytes of storage and single DB cluster without bottlenecking or specialized hardware.
* Open source nature of Apache Cassandra allows users to avoid cloud provider lock-in.
* Cassandra has already served over one million queries per second with 99.999% availability.
* Future plans include running data centers on public clouds and executing distributed transactions across entire database regions.
* The Apache Cassandra project is a great place for engineers to learn, innovate, and contribute.


## Keynote: Scaling Open Source Large Language Models - Charles Srisuwananukorn, Together

URL: [https://www.youtube.com/watch?v=6L36RkL8EhI](https://www.youtube.com/watch?v=6L36RkL8EhI)

 * The speaker is the VP of Engineering at Together AI, a research-driven startup focused on optimizing large language models.
* They believe the future of AI will involve both commercial models and open source models.
* Reasons for open source large language models include transparency, control, and privacy.
* Transparency allows users to understand how the model is behaving, including data used for training and methods used.
* Control allows users to finetune and deploy the model as they see fit, including infrastructure choice and deployment location.
* Privacy ensures that users maintain control over their input data and user responses.
* Together has partnered with Stanford Research Institute and Helm for model development and benchmarking.
* Challenges encountered while scaling an open source large language model service include autoscaling, inference stack optimization, bin packing versus robustness, timeouts, and continuous batching.
* Autoscaling is important to handle demand peaks but challenging due to the inference stack and model size.
* Inference stack optimization is crucial for reducing startup time and serving traffic quickly.
* Bin packing versus robustness tradeoff can make it difficult to scale larger models efficiently while maintaining robustness.
* Timeouts are necessary to prevent errors but require specific tuning for different parts of the data path.
* Continuous batching is essential to reduce latency, handle multiple requests per server, and make the service scalable.


## Keynote: Teaching LLMs New Knowledge - Sharon Zhou, Co-founder & CEO, Lamini

URL: [https://www.youtube.com/watch?v=gF_pdRJjdRc](https://www.youtube.com/watch?v=gF_pdRJjdRc)

 * Sharon Zhou: Co-founder & CEO of Lamini, background in product, academia, and generative AI with a PhD from Stanford
* Lini is an open-source model for running large language models (LLMs) at scale for production workloads
* Goal is to make superintelligence customizable and accessible
* Scaling LLMs: guarantee JSON call for instantiating the model, output type in INT with age unit as a string
* Collecting data for the model: learning from millions of data points, correcting old incorrect information
* Training methods: pretraining, fine-tuning, and prompting (Rag finetuning)
* Creating differentiated LLMs: open AI, GitHub Copilot, and Luminize
* Best practices learned over the last two decades: creating a successful LLM doesn't always require fine-tuning or re-training
* Last step: fine-tuning large language models can be challenging, but taking courses like those offered by Andrew Ng on Corsera is a good resource.


## Keynote: What's New in AI - Eve Phillips, Director of Product Management, Google

URL: [https://www.youtube.com/watch?v=p9bUuOzpBGE](https://www.youtube.com/watch?v=p9bUuOzpBGE)

 * Eve Phillips, Director of Product Management at Google, discusses recent advancements in AI and its impact on solving human needs
* Large language models (LLMs) have gained popularity due to their ability to generate human-like responses to a wide variety of questions
* LLMs are pretrained on large corpora of text and can be fine-tuned for specific tasks
* Transformers architecture, introduced in 2017, made possible the processing of longer sequences and solving memory issues encountered in earlier models
* Google has introduced several state-of-the-art LLMs including T5, LaMDA, and Gemini
* Advancements in computational power, ML techniques, and data access have made new innovations possible
* Large amounts of data, particularly text, are necessary for model performance improvement
* Open source platforms like Hugging Face and TensorFlow have made it easier for developers to innovate faster
* The integration of large models into applications has solved diverse customer problems across various tasks in everyday life
* The availability of open-source models and easier access to large amounts of compute power means that almost anyone can build AI solutions
* Recent advancements include real-time editing features, multi-framework machine learning capabilities, and the ability to run large-scale model training on devices
* Google's latest announcement is Gemini, a next-phase journey around making AI helpful for everyone, regardless of technical expertise.


## Keynote: Next-Gen AI Applications via Multi-Agent Conversation - Victor Dibia, Microsoft

URL: [https://www.youtube.com/watch?v=sQl4EaC6exc](https://www.youtube.com/watch?v=sQl4EaC6exc)

 * Victor Dibia from Microsoft discusses Autogen, a framework for building multi-agent AI applications
* Autogen is a collaborative effort across multiple institutions including Microsoft Research
* Future AI agents will help with increasingly complex tasks, such as writing novel software or solving supply chain problems
* Agents will be able to reason, plan, act, use external tools, and communicate, both with each other and humans
* Multi-agent approaches can improve factuality reasoning and provide validation
* Autogen is designed for conversational paradigm and supports sharing states among agents
* It has a flexible API that enables communication across multiple agents and humans within a loop
* Agents can be set up to work together to solve complex tasks, with one agent writing code and another visualizing or critiquing it
* Autogen ecosystem supports multimodal agents, allowing for collaboration and better outcomes
* Autogen is open source and has seen significant growth since being released in March 2021.


## Executorch: Enabling and Accelerating PyTorch Models on Edge Devices - Chen Lai, Meta

URL: [https://www.youtube.com/watch?v=2yJvkeG-OWc](https://www.youtube.com/watch?v=2yJvkeG-OWc)

 * Chen Lai, Meta software engineer, introduces Pytorch's solution for enabling and accelerating Pytorch models on edge devices
* Edge devices: laptop, AR glasses, embedded devices, mobile devices, wearables (ex. smartwatches)
* Advantages of running models locally: privacy, security, performance, energy savings
* Explanation of cloud-based model running vs local model running
* Discussion on device types and their unique challenges
* Overview of Pytorch ecosystem and current landscape
* Importance of family apps for device AI in mobile devices and AR/VR
* Overview of Torchstack, PyTorch's integrated partner platform, to streamline development process
* Explanation of PyTorch's Autograd and PyTorch Scripting (PyTorch 2.0) and their role in model deployment on edge devices
* Discussion on the challenges of deploying models across multiple boundaries and vendor-specific toolchains
* Overview of PyTorch's focus on lowering the barrier to entry for developers with a composable nature solution
* Comparison of Pytorch versus TensorFlow in terms of deployment experience, performance, and community involvement.


## Lightning Talk: GPU.x - Dynamic GPU Fraction and Sharing on ML Mainstream Framework - Tiejun Chen

URL: [https://www.youtube.com/watch?v=5BNW-f6L5N8](https://www.youtube.com/watch?v=5BNW-f6L5N8)

 * Speaker from VM team focusing on machine learning acceleration
* Talking about GPU fraction sharing in machine learning frameworks background
* Exponential growth of machine learning, typically use GPUs for deep learning workloads
* Problem: Low GPU utilization and long wait times for availability
* Previous solutions: GPU virtualization, passthrough, Mig, VGP API
* Limitations: Passthrough cannot share multiple instances in a group, limited hardware partitioning cannot be reallocated or reassigned dynamically
* Complicated APIs and difficult to implement
* Proposed solution: GPU.x interposer for machine learning frameworks
* Ability to interpose calls to machine learning frameworks for dynamic GPU memory management
* Brings live isolation mechanism, separate running environments with guaranteed resources
* Demonstration: Allocating GPU memory in different terminal shells, showing ability to share and reconfigure GPUs dynamically.


## Crafting Superior ML Pipelines with SapientML - Kosaku Kimura, Fujitsu

URL: [https://www.youtube.com/watch?v=6telp96NAkw](https://www.youtube.com/watch?v=6telp96NAkw)

 * Kosaku Kimura from Fujitsu introduced SapientML, an open-source project by Fujitsu
* Two open source projects launched by Fujitsu: one for intersectional fairness technology and the other for automating machine learning pipelines with SapientML
* Shortage of AI talent and difficulty in recruiting data scientists are challenges in implementing AI applications
* Automated Machine Learning (AutoML) is a solution to automate UI utilization process in data science workflow, but it faces issues of being time-consuming and intensive on computational resources, and having a blackbox nature
* SapientML aims to address these issues by developing expertise from large source code corpus and conducting data comprehension projects to generate source code, including data preprocessing, exploratory data analysis, model building, and evaluation
* Demonstration using Kaggle competition dataset: FL target feature task classification using CatBoost classifier and evaluating F1 score accuracy
* SapientML generates source code quickly and obtains a trained model with the result
* Second demo using Google Colaboratory and Python APIs to split train data set, generate source code for training and calling prediction function, and obtain R2 score from the generated CLS model.


## Enter the Brave New World of GenAI with Vector Search - Mary Grygleski, DataStax

URL: [https://www.youtube.com/watch?v=7ELb0d_Eigo](https://www.youtube.com/watch?v=7ELb0d_Eigo)

 * Mary Grygleski, Senior Developer Advocate at DataStax, discussing entering the new era of generative AI and Vector search using DataStax's Astra DB and Cassandra
* Background on AI history from ancient Greece to modern day, focusing on milestones like IBM Deep Blue and OpenAI's GPT-3
* Generative AI explained as a disruptive technology that can interpret and respond to human language input, unlike traditional predictive AI
* Generative AI applications include creative tasks such as writing essays, poems, or designing images
* DataStax Astra DB is a database built specifically for complex machine learning operations, with Vector databases being an important component for similarity search using techniques like approximate nearest neighbor and cosine matching.
* DataStax has implemented vector data search in its Cassandra database and offers a free trial with no credit card required.
* Vector databases store data in multidimensional space, allowing for efficient handling of complex data types required by machine learning.
* DataStacks is offering a slide deck and QR code for further information, as well as streaming live coding sessions in 2024.


## The Evolution of ONNX: Driving Interoperability in AI... Sai Kishan Pampana & Ramakrishnan Sivakumar

URL: [https://www.youtube.com/watch?v=N0wDg5oMHSA](https://www.youtube.com/watch?v=N0wDg5oMHSA)

 * ONNX (Open Neural Network Exchange) is an open format for representing neural networks, enabling interoperability between different deep learning frameworks like PyTorch and TensorFlow.
* ONNX acts as a bridge, allowing users to train models in one framework and deploy them in another without modification, making it portable across different platforms.
* ONNX ecosystem is diverse and growing, with tools like model converters, optimization libraries, and execution engines.
* The compute landscape for AI is rapidly evolving, with various software and hardware components, requiring efficient connectivity between them.
* ONNX runtime supports running models on CPUs, GPUs, and other accelerators like FPGAs and NPUs.
* ONNX plugins allow easy integration of specific hardware-software stacks, such as AMD's Ryzen AI plugin for AMD GPUs and NPUs.
* The ONNX model zoo contains over 2000 models available for use, making it an essential resource for developers.
* ONNX facilitates interoperability between different deep learning frameworks, enabling quicker production of machine learning applications by reducing the need to retrain or convert models for each framework.
* ONNX supports various deployment scenarios, including cloud and edge computing environments.
* Intel has optimized ONNX runtime for their hardware, ensuring optimal performance on Intel CPUs, GPUs, and other accelerators like Neural Compressor for model compression and optimization.
* ONNX runtime is versatile and flexible, making it a valuable tool for developers working in the AI ecosystem.


## Retrieval Augmentation and Semantic Search at Scale - Ash Vardanian, Unum Cloud

URL: [https://www.youtube.com/watch?v=ODTpIbJ-Vks](https://www.youtube.com/watch?v=ODTpIbJ-Vks)

 * Ash Vardanian from Unum Cloud discussed Retrieval Augmentation and Semantic Search at Scale
* Goal is to improve model output quality, use larger context windows for retrieval, and make it scalable for large datasets
* Modern hardware is capable of handling complex computations, but software still lags behind in taking advantage of this power
* Brute Force search method uses vector comparison with cosine similarity metric
* Intel CPUs have specialized instructions for matrix multiplication and dense computations
* Vector databases are used for large-scale indexing and efficient retrieval
* Approximate search solutions like HNSW, Big Ann, and SimCLR are necessary for datasets beyond 1 million entries
* Modern CPUs waste a lot of cycles on idle CPU waiting and memory fetching
* Object-oriented software incurs high overhead due to dynamic structure allocation and irregular memory usage
* Ash open-sourced a vector search library to address these issues
* Facebook's Vector Search Library may perform well for smaller datasets but struggles with large ones due to indexing requirements
* Commercial solutions like MKL, Apache 2, and HNW offer optimized math libraries and approximate nearest neighbor search
* 16-bit floating point numbers are becoming more popular in deep learning models
* Inference engines like TensorFlow, PyTorch, and ONNX support various Precision levels
* AI research usually focuses on larger models with specific sizes
* Alibaba's Quen model is a large 18 billion parameter model that performs well in various tasks
* Unum's domain-specific search solution uses embeddings for text and image data, focusing on retrieval augmentation for generative models.


## Lightning Talk: FastTrackML: Experiment Tracking at Hyper Scale - Dave Gantenbein, G-Research

URL: [https://www.youtube.com/watch?v=OfO-fH79NZY](https://www.youtube.com/watch?v=OfO-fH79NZY)

 * Dave Gantenbein, director of Open Source development at G-Research, discussed FastTrackML, an open source experiment tracker they built
* Goal was to create a highly performing experiment tracker for their large financial prediction models that utilize a lot of data and need quick iteration
* Settled on MLflow due to its good UI, integration with many frameworks, and existing community support
* Wanted to make migration from MLflow as painless as possible
* Also built tooling to allow users of MLflow to bring their code and data into FastTrackML easily
* Interested in MLflow's real-time experiment information and time series data features
* Mentioned using Major League Hacking to find a junior engineer to help build the project
* Future development includes namespace feature for segregating experiment data, optimizing UI performance, and considering migrating from SQL light to DuckDB for better performance
* Aspiration is for FastTrackML to become a commodity experiment tracker that doesn't require much thought or customization.


## Lightning Talk: Grounding LLM to Avoid Hallucinations Using Vector S... Venkata Karthik Penikalapati

URL: [https://www.youtube.com/watch?v=WJKwk3F0U0I](https://www.youtube.com/watch?v=WJKwk3F0U0I)

 * Karthik from Salesforce relevance team discussing grounding LLM (Language Model) in Enterprise setting
* Retrieval Augmented Generation (RAG) concept used to make LLM usable
* RAG provides LLM context from external sources, prevents it from being static and trained on a frozen dataset
* Drawbacks of LLM: lacks context for private data or specific question domains, may give hallucinated answers in production applications
* Challenges in Enterprise setting include the need for citations and auditing purposes
* RAG framework merges neural retrieval model and knowledge base, converting documents into embeddings (high-dimensional vectors)
* Techniques for embedding creation: uniform chunking or sentence-based chunking
* Context-aware LLM using RAG improves response accuracy in QA chatbots, content creation applications.


## Human-Centered AI: What I've Learned From Building Two LLM-Powered Products Thi... Margaret Jennings

URL: [https://www.youtube.com/watch?v=YbeoS9Nsm34](https://www.youtube.com/watch?v=YbeoS9Nsm34)

 * Margaret Jennings, AI program chair at the Linux Foundation, shares her journey in building two LLM-powered products
* Started as a lowly investment analyst at Stanford in 2014, got excited about AI and data quality
* Studied management science engineering at UCL London due to Brexit, then launched Deep Mine AI seed fund
* Became interested in human-centered AI after seeing the potential of expert loop and midwife assistants in Indonesia
* During the pandemic, started working on Holodok, a telemedicine platform in Southeast Asia with 35 million monthly active users
* Believes that human-AI collaboration is the future, especially for small tasks and conversational interfaces
* Mentioned various applications of LLMs such as code interpreter, celebrity artist interface, and conversational agent
* Recommends collecting interaction data within products to make models better and providing convenience and utility to users
* Encourages experimenting with different product designs and playing around with open source models for better performance and reliability.


## The Future is Fine-Tuned: Deploying Task-specific LLMs - Devvret Rishi, Predibase

URL: [https://www.youtube.com/watch?v=Za9HavaK9ks](https://www.youtube.com/watch?v=Za9HavaK9ks)

 * Devvret Rishi discussed the future of fine-tuned task-specific LLMs in AI.
* He mentioned that smaller, fine-tuned models are becoming increasingly popular and valuable in production.
* The LLM Revolution started with pretrained deep learning models like GPT3 and GPT4, which have impressive generative capabilities.
* Fine-tuning these models for specific tasks can be done using frameworks like Ludwig and Lorax, making it easier to train and serve many different models.
* Fine-tuning models can improve performance significantly, especially in specific use cases.
* Smaller, fine-tuned models are rightsized for individual tasks and can save costs by requiring fewer resources.
* The challenges of fine-tuning include training complex models with large datasets and achieving good GPU utilization.
* Open source projects like Ludwig and Lorax simplify the process of fine-tuning and serving many different models.
* Fine-tuning models also allows for more efficient control over output and better end-to-end agency in building AI solutions.


## Lunch & Learn VC Panel: Fundraising for Your GenAI Startup in the Current Environment

URL: [https://www.youtube.com/watch?v=drQ3UhJb0Rw](https://www.youtube.com/watch?v=drQ3UhJb0Rw)

 * The panelists are investment directors and managing partners from various venture capital firms focusing on generative AI startups.
* Intel Capital invests in cloud software and hardware, with a focus on series B stage companies. Zeta Venture Partners focuses exclusively on seed-stage AI companies. Redo Ventures invests in infrastructure, developer tools, AI applications, and security.
* Investment dollars have been flowing into generative AI, particularly foundation models. Large companies are also investing in this space.
* Open source is a significant trend, with many open source projects inspiring startups.
* Defensibility is important for investment, as there is competition in the market and incumbents may be difficult to displace. Unique teams and relationships can provide an advantage.
* Milestones are important when pitching to investors, especially for open source-based startups.
* Feedback loops and open source can provide a competitive advantage, but there are challenges such as monetization and finding product-market fit.
* Legal considerations are important in areas like copyright law and regulation, particularly in the crypto space.
* Regulation may evolve quickly in AI, and responsible VCs will ask questions and do diligence to understand the regulatory landscape.
* There is potential for disruption in various industries with AI-native and AI-enabled companies.
* Investors are interested in the long-term potential of generative AI companies.


## Milvus Vector Database: Unlocking the Future of Open Source Vector Databases - Frank Liu, Zilliz

URL: [https://www.youtube.com/watch?v=kVoRP2glJ8s](https://www.youtube.com/watch?v=kVoRP2glJ8s)

 * Milvus is a widely adopted open source Vector database with over 25,000 stars on GitHub.
* Zilliz, the company behind Milvus, has been building Vector search technology and Vector databases since 2018.
* Milvus is best for storing and indexing large quantities of unstructured data such as images, videos, and text.
* Vector search allows for semantic analysis by converting input data into vectors using embedding models and deep learning.
* Milvus supports various modalities like image, audio, and text, and can perform cross-modality retrieval.
* Milvus is distributed, scalable, and high performance, with the ability to add metadata and scalar fields.
* It uses a distributed architecture with a proxy layer, query node, index node, data node, and object store.
* Milvus supports real-time queries and can handle large workloads with resource optimization.
* Future developments for Milvus include adding sparse Vector support and multiple modality support in a single row database.
* Milvus is designed to enable moving data around for optimization and large scale.
* Milvus supports compaction, which merges small segments into larger ones and writes back to S3 for querying.


## Unveil the Magic Without Hoodini: Transform Your Machine Learning Pipelines with Apa... Nadine Farah

URL: [https://www.youtube.com/watch?v=pUZHotLdkjU](https://www.youtube.com/watch?v=pUZHotLdkjU)

 * Nadine Farah is speaking about transforming machine learning pipelines using Apache Hoodie without relying on Houdini.
* The Medallion architecture of machine learning pipelines typically involves ingesting, managing, and processing data to create models and deploy them.
* The Medallion architecture consists of Raw, Bronze, Silver, and Gold layers for unprocessed, processed, cleaned, and analyzed data, respectively.
* Manual processes like file size management, data cleaning, and indexing can be time-consuming and complex.
* Automated table services like Hoodie offer features to manage metadata, optimize data layout, and ensure concurrency control for efficient table management.
* The incremental processing framework in Hoodie enables faster updates without reprocessing the entire dataset, saving computational resources for large-scale data.
* POI Hoodie is an extension of Apache Hoodie that provides advanced features like indexing, streaming workload support, and out-of-order data handling.
* The Lakehouse format in Hoodie offers a comprehensive platform designed for data ingestion, processing, and feature services to maximize efficiency.
* Hoodie's incremental framework allows faster updates and lower processing times for analytical workloads, enabling real-time data analysis.
* Hoodie supports multiple services, including indexing, concurrency control mechanisms, and integrations with popular computational tools like Presto.
* The multiversion concurrency control in Hoodie ensures consistency across multiple operations on the data successfully ingested into the transactional layer.


## Workshop: GenOps: Building a MLOps Platform to Support GenAI Workloads with Open... Farshad Ghodsian

URL: [https://www.youtube.com/watch?v=w8a7Pu7n5Nc](https://www.youtube.com/watch?v=w8a7Pu7n5Nc)

 * Farshad Ghodsian from Source Group discussing GenOps and building a MLOps platform for generative AI workloads using Open Source tooling, specifically Cube Flow
* Cube Flow is an open-source MLops platform built on Kubernetes that supports the entire ML lifecycle
* Cost savings: open source, no licensing fees or additional cloud charges for ML instances and storage
* Portability: run Cube Flow in various clouds (AWS, Azure, GCP) and on-premises
* Usability: customizable UI, integrates with popular data science tools and services like Spark, TensorFlow, and VS Code Server
* Upgradability: source access to codebase for customization and contributing back to the community
* Familiarity: runs on Kubernetes, which many organizations are already using
* Data loading phase: convert Corpus documents to vectors, use vector DBs like Chroma or Cassandra, add documentation and metadata
* Model service: save model responses in tools like Label Studio and provide a web UI for human feedback and annotation
* GPU time sharing: increase GPU utilization by allowing multiple pods to share a single GPU
* Rapid scaling: use persistent volumes and read many volumes to avoid downloading large files each time a new pod is added
* Direct preference optimization (DPO): simpler and cheaper alternative to reinforcement learning for saving and using human feedback
* Cube Flow architecture: use Lang chain orchestration, save model responses directly in label Studio, export CSV files for datasets, use vector DBs for efficient querying.


## NeMo: Open Source Toolkit for Conversational AI - Elena Rastorgueva, NVIDIA

URL: [https://www.youtube.com/watch?v=yCqf_sZuVKs](https://www.youtube.com/watch?v=yCqf_sZuVKs)

 * Elena Rastorgueva from NVIDIA presented Nemo Toolkit, an open source conversational AI toolkit with automatic speech recognition (ASR), text-to-speech (TTS), and coming soon multimodal capabilities.
* ASR section has extra focus as Elena has experience with it.
* Installation: use pip or Docker container. Pretrained models can be loaded from NVIDIA NGC Hub or Hugging Face Model Hub.
* Nemo originated from a team focused on making composable building blocks.
* Training is easy using PyTorch Lightning and Megatron for large-scale training.
* ASR pipeline converts speech signals into spectrograms, which are then processed by various types of models like CTC transducer or attention-based encoder-decoder model.
* Metrics for evaluation include word error rate (WER) and real-time factor (RTF).
* Nemo also supports adjacent tasks such as speaker diarization, speech translation, and various types of speech classification.
* Demonstrated ASR model using Hugging Face's open source best performing models for English language.
* Online ASR demo: streaming speech recognition with a model specially trained for transcribing audio in real-time.
* Nemo supports various types of architectures and training modes like pretraining, supervised fine tuning, and parameter efficient fine tuning.
* Also supports model alignment using the Nemo aligner tool.
* Demonstrated a simple fun model using voice activity detection and keyboard spotting for voice control.
* Nemo makes use of Megatron core and Transformer engine, both open source software from NVIDIA.
* Simple alternative to rhf called stlm was mentioned.
* Released a model fine-tuned using sterm that outperforms Llama 270b base model on MTR13 benchmark.
* Two types of TTS methods were mentioned: one model turning transcript into spectrogram and another model directly generating speech signal from text.
* Demonstrated speaker interpolation using a TTS model that accepts speaker embeddings as input and generates new speaker embeddings to create a new synthetic voice.
* Q&A session covered various topics like GPU requirements, training, inference, and alternatives to whisper.


## Load Management for AI Models - Managing OpenAI Rate Limits with Request Prioritization- Harjot Gill

URL: [https://www.youtube.com/watch?v=0r0Tmb9pPx4](https://www.youtube.com/watch?v=0r0Tmb9pPx4)

 * Speaker is the co-founder and CEO of Flux Ninja, discussing load management for generative AI models using OpenAI as an example
* Generative AI workload has different latency profiles compared to web-scale APIs (e.g., mouse versus elephant)
* OpenAI rate limiting is aggressive due to high demand and limited compute resources
* Prioritization and fairness become important for providers in managing access to their API services
* OpenAI offers different tiered rate limits, with token buckets used to manage usage
* Consumers need to manage load and prioritize requests effectively to avoid hitting rate limits
* Case study: Code Rabbit, a code review tool that uses multiple models, faces challenges in managing high demand and prioritizing workloads
* Aperture is an open-source platform designed for observability-driven load management and provides request prioritization and caching capabilities through sidecar agents
* Aperture includes distributed token bucket algorithms with a weighted fair queuing scheduler to manage requests effectively
* Aperture allows defining labels to track different workloads, rate limits, and priorities, making it easier for developers to manage their API usage.


## A New Stack for the Gen AI Era - Alan Ho, DataStax

URL: [https://www.youtube.com/watch?v=4NC3gd9W1CM](https://www.youtube.com/watch?v=4NC3gd9W1CM)

 * Alan Ho discusses the evolution of AI stacks, specifically focusing on Google's latest large language model project, Gemini
* Gemini is a natively multimodal language model that can understand text, audio, and video
* The future AI system is likely to have a brain made up of mimicked artificial intelligence with separation of memory and models
* Data infrastructure systems will play an important role in the new AI stack, including databases like Cassandra for structured and unstructured data
* New primitives, such as vector databases, are expected to evolve in the next quarter
* Multiagent orchestration is becoming increasingly important for handling complex workflows, with tools like Lang chain and Lambda being used
* The importance of LLM monitoring and ops is growing as AI systems become more sophisticated
* Cassandra's ability to write and index data immediately allows for caching and cache coherence, which is important for consistent answers in an application
* Multiagent orchestration will likely be a major battlefield in the coming years, with Lang chain and hyperscalers being key players.


## Tricks Learned from Scaling WhisperSpeech Models to 80k+ Hours of Speech - Jakub Cłapa, Collabora

URL: [https://www.youtube.com/watch?v=6Fr-rq-yjXo](https://www.youtube.com/watch?v=6Fr-rq-yjXo)

 * Jakub Cłapa discussed scaling Whisper speech models to over 80,000 hours of speech data
* Open source model trained properly and licensed for commercial use
* Whisper speech model is built on two Transformer models: encoder and decoder
* Encoder converts text input to phonetic representation
* Decoder generates actual sound from the phonetic representation
* Challenges included handling big data, processing variable speed text, and improving performance
* Collabora helped with deployment and avoiding vendor locking
* Whisper speech model uses a neural audio codec and Vocoder for speech encoding and decoding
* Whisper model was built by adapting existing open source models like Wav2Letter, Tacotron 2, and Hubert
* The team used a different approach to build the top existing open source model first
* Challenges included data size, download speed, and file organization
* They optimized the inference pipeline for faster performance and lower latency.


## Breakdown New Data Silos in Generative AI - Junping Du, Datastrato

URL: [https://www.youtube.com/watch?v=EfDPuU3mFXQ](https://www.youtube.com/watch?v=EfDPuU3mFXQ)

 * Junping Du, founder and CEO of Data Strato, discusses breaking down data silos in generative AI
* Data Foundation renamed to Deplan AI, focusing on merging AI and data
* Data gravity is a key principle leading to increasing volume and variety of data
* Data silos have been an issue for 30 years, but new technologies like data lakes are helping merge data across systems
* Multicloud strategy and distributed data are adding complexity, requiring solutions like Data Stratosphere for metadata management and querying heterogeneous databases efficiently
* Gravitino is an open-source project that aims to build a cross-cloud layer data fabric with a unified metadata lake and federated query engine
* Gravitino allows easy querying of multiple cloud providers, reducing the need for ETL pipelines and enabling data discovery across the organization
* Data Stratosphere provides a simplified interface for dealing with metadata and data governance challenges in multicloud environments.


## Consume and Serve Using Caikit and ModelMesh - Rafael Vasquez & Christian Kadner, IBM

URL: [https://www.youtube.com/watch?v=FQYVDqwfoW4](https://www.youtube.com/watch?v=FQYVDqwfoW4)

 * Rafael Vasquez and Christian Kadner from IBM discussing model serving using ModelMesh and KaKit
* Outline: need for model production, introducing ModelMesh and KaKit, moving to model inferencing side, features of ModelMesh
* Model serving is deploying trained models for production use, making them available for consumers
* Challenges in deploying models include containerization, handling different frameworks and model formats, scaling and resource management
* ModelMesh: open source project, cloud-agnostic model inference platform based on Kubernetes
* Provides standardized inference protocol, supports TensorFlow and PyTorch among other frameworks
* Features include intelligent placement, cache management, and request load balancing
* KaKit: AI toolkit, simplifies the process of creating and integrating models into applications for various roles
* Two main concepts in KaKit: concept of runtime, where model is deployed with different runtimes for training and inference
* Demonstration: using Hugging Face model, cloning repository, running sentiment analysis task, using Gradio UI to test the model.


## Insights from The State of AI Report 2023 - Corina Gurau, Air Street Capital

URL: [https://www.youtube.com/watch?v=Ge99fqjD8HY](https://www.youtube.com/watch?v=Ge99fqjD8HY)

 * Corina Gurau from Air Street Capital presented highlights from the State of AI Report 2023.
* The report analyzes developments in AI research, industry, safety, and policy.
* GBd4 model outperformed LLM in various tasks, including the bar exam and coding, despite having similar architecture to GPT-3.
* Open source community responded with excitement to advancements in smaller models fine-tuned on specialized datasets and downstream applications.
* Microsoft showed that a specialized model using quantum annealing data set can rival larger models, but overlooked the importance of high-quality training data.
* Research suggests that synthetic data generation could help train language models effectively and efficiently.
* Google trained a model on a synthetic version of ImageNet and it outperformed the model trained on original data for image classification tasks.
* Microsoft's Vega model showed iterative prompting capabilities, producing code using Minecraft API.
* Nvidia's Spring model beat reinforcement learning models in reading academic papers and playing games.
* Vision language action models provide driving commentary and explain driving models, but are not interpretable.
* Research is being done on embodied AI, including vision-language action models for robots.
* Text-to-video generation has seen recent advancements using diffusion and mask Transformers.
* Nvidia's GPUs dominate the AI research landscape, with alternatives like FPGAs and TPUs emerging.
* Companies are adapting to AI by improving learning and pivoting their businesses.
* Hugging Face is keeping AI models accessible, but faces challenges with retaining active users.
* The regulatory approach to AI is diverging, with some relying on existing laws and others proposing specific legislative frameworks.
* There is concern about the potential for misuse of large language models and the need for transparency and control.
* Safety and alignment research has gained attention in recent years, but there is still much work to be done.
* There are many layers in large language models, with application layer being the easiest to build.
* People are building Foundation model buildings and improving existing models through fine-tuning.
* Accuracy versus speed is an ongoing debate, with speed being a quicker way to add new knowledge.
* Nvidia is raising significant amounts of money for compute power and access to it.
* The standard body ISO and IEEE are playing roles in AI development.


## Beyond Transformers - Intro to RWKV Architecture & The World To... Eugene Cheah & Harrison Vanderbyl

URL: [https://www.youtube.com/watch?v=I-HMKky7Qsw](https://www.youtube.com/watch?v=I-HMKky7Qsw)

 * Eugene and Harrison from RWKV team at Linux Foundation introduce Linear Transformers, a new architecture for AI models that is linear in cost, energy efficient, and potentially infinite in capacity.
* Linear Transformers address the quadratic problem of attention in previous Transformer architectures by using a KV cache system that allows parallel processing and reduces dependency on previous tokens.
* The KV cache system also enables faster training and inference, allowing for deployability on CPUs, GPUs, and FPGAs.
* Linear Transformers are the greenest AI model with the lowest carbon footprint per token generated.
* RWKV architecture is interchangeable with previous transformer architectures and can potentially run faster due to its ability to perform well theoretically without relying on previous tokens.
* The architecture includes a new token shift time mix system that provides long-term and short-term memory, allowing information to travel in a diagonal pattern like in convolutional neural networks.
* Linear Transformers can match the performance of previous generation models like T5 and BERT with comparable size and scaling laws.
* The open source project is currently being developed by a diverse team from around the world, with a user base primarily in Asia.
* The architecture is cheaper to run on low-end hardware due to its reliance on linear cost computation and memory size.
* RWKV architecture was inspired by the need for multilingual support and the lack of support for European languages in existing models.
* The new tokenizer, called world tokenizer, allows the model to push information more effectively and is particularly useful for handling character languages that are difficult for English-based tokenizers to process.
* Efficiency gains from Linear Transformers include a reduction in the number of tokens required and significant improvements in memory usage.
* The effective context length has been significantly improved in new architecture, allowing for better handling of long contexts and larger models.
* The V5 model is able to support context lengths of up to 8K-16K approximately, which brings it closer to competing with Transformers in terms of range and scale.
* Linear Transformers have the potential to handle memory more efficiently and directly compete with Transformers in terms of range and effectiveness.


## When AI Became ML, and then AI Again: Tracing the Evolution of Terms and Technologies - Marcus Eagan

URL: [https://www.youtube.com/watch?v=K4pmOWD_8Gc](https://www.youtube.com/watch?v=K4pmOWD_8Gc)

 * Marcus Eagan discusses evolution of AI and ML terms and technologies
* Worked on building robotic systems, open source projects, advised companies
* AI became ML around 10 years ago with the rise of AI-powered search engines
* Mechanical Turk was a precursor to AWS's Mechanical Turk service
* Early AI development included work on chess programs and language reasoning
* First AI boom in late 50s, followed by an AI winter in the 80s
* Dragon Dictation revolutionized voice recognition transcription technology and led to another AI boom in the late 90s
* Distinction between AI and ML became blurred around 2010 as more companies started selling AI features
* Recent advances in generative AI have led to new applications and companies, such as OpenAI and Pendragon Security
* GPT-3 is a large language model that can generate responses to queries
* Deep learning has gone beyond traditional machine learning and created a new field
* Working towards AGI (Artificial General Intelligence) and GENAI (Generalized Neural Architectures)
* Entrepreneurial interest in building efficient, cost-effective AI systems to avoid another AI winter.


## The Copyright Office is Wrong About AI-Assisted Works - Van Lindberg, OSPOCO

URL: [https://www.youtube.com/watch?v=LGpbKb-_GXY](https://www.youtube.com/watch?v=LGpbKb-_GXY)

 * The speaker argues that the US Copyright Office's stance on AI-assisted works being uncopyrightable is wrong and unsustainable.
* They use examples of cases where non-human authors or creators have been involved in copyright disputes, such as a monkey taking a selfie and an artwork made by an elephant using an AI.
* The speaker believes that the current US copyright law, which is designed to secure the author's right to inventor or discovery, does not apply to AI-generated content since it replaces mental work with autonomous creation.
* They mention the case of Chris Cantanova and his comic book "Zaria Dawn," where the Copyright Office initially denied registration due to the work being generated by an AI, but later reversed their decision.
* The speaker discusses the impact of computerized printing on the letterpress industry and argues that AI will lead to a reduction in the number of people working in certain intellectual output fields.
* They mention economic concepts such as induced demand and the automation of least helpful parts of development.
* The speaker references various court cases related to copyrightability, including those involving photographs and artwork.
* They argue that the current legal standard for copyright is incorrect and that control over the output does not necessarily equate to copyright protection.
* The speaker discusses the potential disruption that AI will bring to the copyright system and the economic forces that enable it.
* They mention that even open-source projects rely on the underlying legal structure of copyright.
* The speaker argues that generative AI tools are essential for creativity and productivity, but also pose a risk to copyright holders and creators.
* They discuss the idea of forced change in the copyright field and the potential for multiple rounds of court cases and Supreme Court involvement.
* They mention the importance of understanding the underlying structure and purpose of copyright and the possibility of temporary trade secret protection for AI models.
* The speaker suggests that businesses will need to build structures around AI-generated content and think differently about licensing and ownership.
* They discuss a case where an organization indemnified AI-generated output, which raises questions about copyrightability and potential infringement.
* The speaker mentions the importance of being careful with term usage and understanding value in the context of copyright law.
* They mention that they are developing a case related to stable diffusion general mid-journey piece art and a copyright office denial.
* They discuss potential negative side effects of AI suddenly becoming copyrightable and the impact on existing works declared in the public domain.


## How Use of Generative AI Affects Both Open Source Developers and their Downstrea... Roman Shaposhnik

URL: [https://www.youtube.com/watch?v=MS7Q5TTQOU0](https://www.youtube.com/watch?v=MS7Q5TTQOU0)

 * Roman Shaposhnik, co-founder of an AI company and VP of Legal Affairs at Apache Software Foundation, discusses impact of generative AI on open source developers.
* Generative AI tools like GitHub Copilot and Amazon Whisper can be used to generate code, error messages, debugging suggestions, and even patents.
* These tools can make developers more productive but also introduce legal challenges such as intellectual property rights, licensing, and potential misuse of data.
* Open source projects like Apache Software Foundation have formal structures for handling contributions, including attribution, licensing, and governance.
* Generative AI models can be used to create copyrightable works, but the legal landscape around this is unclear and evolving.
* Corporations are using generative AI tools in different ways, from permitting use by developers with certain credentials to enforcing strict policies.
* The open source community is actively discussing and evolving guidelines for using generative AI tools in open source projects.
* Legal landscape around generative AI is fast-moving and constantly changing, with policy makers in Europe and the US making announcements.
* Linux Foundation and Apache Software Foundation provide guidance on using generative AI tools in open source projects.
* Preapproved lists of AI tools and machine-readable attribution may be useful for addressing legal challenges around generative AI use in open source.


## Democratization of ML Pipelines and Bringing ML Workflows to Heterogeneous... Yihong Wang & Tommy Li

URL: [https://www.youtube.com/watch?v=ZhoIUMJke_w](https://www.youtube.com/watch?v=ZhoIUMJke_w)

 * Tommy Li and Yihong Wang presented on democratizing ML pipelines and bringing ML workflows to heterogeneous cloud-native platforms
* Different teams involved in ML life cycle have different responsibilities, leading to fragmented workflows and overhead
* Automation of different sections could reduce this fragmentation
* Tekton and Argo are popular open-source projects for creating and orchestrating workflows on Kubernetes
* Tekton provides a containerized workflow engine that can handle complex ML workflows and share metadata among teams
* Argo supports CI/CD-style pipelines and provides a nice UI for managing components
* QR pipeline is another option for connecting different ML workflows, providing an ML framework-agnostic solution
* IR (Intermediate Representation) can be used to promote an open standard for defining ML workflows and sharing components among teams
* Introducing IR allows for easier composition of pipelines and better integration with underlying engines and metadata services
* IR also enables the creation of a central repository for storing and exchanging ML components, potentially building an ecosystem around them.


## Panel Discussion: The Impact of AI Regulation on Open Innovation in AI

URL: [https://www.youtube.com/watch?v=aivtToJqfXk](https://www.youtube.com/watch?v=aivtToJqfXk)

 * Amanda Brock (CEO of Open UK) moderates panel discussion on the impact of AI regulation on open innovation in AI
* Ben Brooks (Public Policy, Stability AI) and Peter Shan (GitHub) are panelists
* AI regulation is already impacting open source software, particularly in the context of cybersecurity, copyright challenges, and security concerns
* Peter Shan focuses on policy maker perspective and infrastructure role in the open AI ecosystem, including GitHub's involvement in facilitating the use and distribution of open source models
* Ben Brooks discusses Stability AI's role in the open AI ecosystem and advocacy for policymakers to provide infrastructure support and clarity around regulations
* The conversation covers the historical context of open source software, concerns around "open washing" and misuse of the term, and the distinction between open source and closed source models
* EU AI Act is mentioned as an example of regulation that could impact open innovation in AI, with potential implications for downstream developers and users
* Discussion also touches on the importance of transparency, competition, and capacity building in the context of AI regulation and open innovation.


## Building a Custom AI Chatbot with Spring Boot, React, and LangChain4j - Marcus Hellberg, Vaadin

URL: [https://www.youtube.com/watch?v=ffNeV4DpNzg](https://www.youtube.com/watch?v=ffNeV4DpNzg)

 * Marcus Hellberg presented a HandsOn coding session at the conference on building a custom AI chatbot using Spring Boot, React, and LangChain4j.
* He mentioned his background as a Java developer and his curiosity about cool technologies.
* The application will be built using the Hilla framework for orchestrating front-end and back-end, with a React front-end and Spring Boot back-end.
* The chatbot will simulate a car rental company called Miles Smiles with a customer service agent handling booking changes and cancellations.
* The application will use LangChain4j for natural language processing and streaming responses from the assistant service chat method.
* The Spring Boot application will define models, configure the streaming chat language model, and handle injections.
* The tokenizer model will be used to count tokens and manage chat history using a chat memory provider.
* The LangChain J library will provide an interface for interacting with the assistant agent service and returning token streams.
* The application will use a builder pattern to create and configure various components, including the streaming chat language model builder and the embedding model.
* The text segment retriever will take an embedding model and embedding store as parameters and return the embedding store retriever.
* The application will ingest documents, split them into small sections, and use LangChain document splitters to create an ingestor.
* The booking tool, a plain Spring component, will be injected with the backend service and provide methods for getting booking details and canceling bookings.
* The presentation included a live demo of the application in action, allowing users to ask specific business questions and cancel reservations within certain time frames.


## Beyond Parquet and ORC: Upgrading Data Infrastructure for Multi-modal AI with Lance Col... Chang She

URL: [https://www.youtube.com/watch?v=qw2CHyib-Xo](https://www.youtube.com/watch?v=qw2CHyib-Xo)

 * Lance Chang, CEO and co-founder of Lance DB, discusses the need for a new columnar format for AI workloads
* Parquet and ORC have been dominant data formats for managing tabular data and ML workloads, but they were designed over a decade ago and are not optimized for modern AI workloads
* Modern AI workloads involve larger, more complex data types like floating point numbers, embeddings, long form text, images, video, and Point clouds
* These workloads require random access performance for tasks like training, vector search, exploratory data analysis, and active learning
* Parquet engines do not support these transformations efficiently and require significant modification to accommodate new formats
* Arrow is a potential solution but requires integration with existing APIs and involves migration overhead
* Lance DB has developed a new columnar format called Lance that addresses the needs of modern AI workloads
* Lance includes fast point query functionality, lightweight table format, and supports secondary indexes and versioning
* Lance is designed to be vendor-neutral and integrates with popular data frame frameworks like Spark, DuckDB, and Pandas
* Lance DB also offers a vector search Vector database and provides cost-effective storage for large scale image data sets.


## Building Ethical AI: The Power of Open Source and Education - Ofer Hermoni, PieEye

URL: [https://www.youtube.com/watch?v=0A8t2QYUvyM](https://www.youtube.com/watch?v=0A8t2QYUvyM)

 * Ofer Hermoni is a consultant with a background in computer science and experience in product management, starting companies, and working for the Linux Foundation AI organization.
* He currently leads the education outreach committee for the Trusted AI initiative within the Linux Foundation AI Commons.
* The Linux Foundation AI (LF AI) is an open source collaborative organization focused on data technology and infrastructure. It has 6150 members, contributing over 226 million lines of code across 58 projects.
* The Trusted AI committee promotes trusted and responsible AI through initiatives like webinars, podcasts, technical integration, model explainability, and open source projects.
* A new initiative, Generative AI Commons, is focused on promoting trustworthy generative AI in a community-driven, open membership setting. It includes four workstreams: Frameworks, Model Data, Education Outreach, and Legislation & Policy.
* The Education Outreach team promotes responsible use of AI through education and outreach to various audiences, including developers, policymakers, and the general public. They prioritize targeting developers first, as they are key to promoting open source principles and adopting trusted AI practices.
* Hermoni believes it's important for everyone to understand the risks associated with generative AI, especially since it may become a part of everyday life for many people who don't fully understand its capabilities or implications. He advocates for educating people at all levels, from elementary school to college, about responsible use and ethical considerations related to generative AI.
* Hermoni also stresses the importance of democratizing access to open source generative AI tools and resources, as they can have significant impact on various industries and society as a whole. He believes that by involving communities and organizations in the development and adoption of trusted AI, we can ensure it's used responsibly and ethically.


## Responsibility in Depth: Layering Licensing, Regulation, and More - Luis Villa, Tidelift, Inc.

URL: [https://www.youtube.com/watch?v=0WLp3lEny1g](https://www.youtube.com/watch?v=0WLp3lEny1g)

 * Luis Villa from Tidelift, Inc. discussing responsible AI and the role of licensing and regulation
* He mentions being overwhelmed by the topic but shares his background in open source and collaboration
* In 1997, Microsoft was a major obstacle to collaboration due to their monopolistic practices, but open source licenses helped solve the problem
* The magic wand analogy is used to discuss how simple licensing solutions may not be enough for complex issues like responsible AI
* He mentions the Swiss Cheese model of risk management and the importance of multiple layers of defense
* Discusses the challenges of building responsible AI, including industrialists wanting profitable monopolies, and the need for transparency and innovation
* Mentions recent cases of AI being used for nonconsensual pornography and racist policing as examples of irresponsible AI use
* The importance of creating a community around responsible AI is emphasized
* Discusses the role of government in regulation and enforcement, and the need for a multifaceted approach to building a responsible AI ecosystem.


## Safeguarding Generative AI Applications: Navigating Security Challenges and Best Pract... Neta Haiby

URL: [https://www.youtube.com/watch?v=3iFB1bJLqjA](https://www.youtube.com/watch?v=3iFB1bJLqjA)

 * Neta Haiby, Microsoft director of security for Microsoft Office and previously head of product for Azure OpenAI Data Labeling, discusses safeguarding generative AI applications.
* Generative AI adoption is evolving rapidly, with unprecedented growth in comparison to other technologies.
* Generative AI can be used for various applications such as natural language processing, code generation, image and video creation, and more.
* However, this new technology brings new risks, including shadow AI usage without visibility, data leakage, compliance regulations, and model theft or poisoning.
* Threat modeling involves defining threats, mitigation strategies, and responsibilities for different layers of AI usage: user application, application, and platform.
* Examples of threats include prompt injection attacks, document or email poisoning, and model theft or poisoning.
* To protect against these threats, organizations need to ensure secure development practices, such as red team testing, safe prompt validation, access control, and log monitoring.
* Best practices for building generative AI applications include understanding potential risks, implementing validation, checking sources, providing transparency, and mitigating overreliance on AI.
* Microsoft is addressing security concerns in its Azure OpenAI platform with built-in security features and responsible AI practices.
* Continuous learning and evolution of security measures are necessary as generative AI technology continues to advance.


## The Power of RAG at Scale: A Year of Explosive Growth and Observab... Ricardo Aravena & Joshua Reini

URL: [https://www.youtube.com/watch?v=5SZp_AIv_o4](https://www.youtube.com/watch?v=5SZp_AIv_o4)

 * Ricardo Aravena and Joshua Reini discussing RAG (Rapid Answers from Graphs) at scale
* RAG solves problem of machine learning models hallucinating by using True Lens, an open-source project for evaluating language model experiments
* True Lens logs input-output metadata and adds a feedback function to understand quality of responses
* Evaluates groundedness, relevance, and answer accuracy of model responses
* Discusses importance of scalability in observability platforms for large language models and applications
* Mentioned the need for reliable, comprehensive, extensible, and cost-effective evaluation frameworks
* Demonstrated running True Lens application using Docker container and Kubernetes deployment
* Talked about custom model evaluation, logging, and database considerations when scaling evaluations
* Discussed limitations of large open LLM providers and the importance of evaluating bandwidth and latency
* Emphasized the need for a community-driven approach to solve common problems in scaling machine learning AI.


## OpenFL: Building Better AI Models with Private Data - Niroop Ammbashankar, Intel

URL: [https://www.youtube.com/watch?v=6mr1w97ITXM](https://www.youtube.com/watch?v=6mr1w97ITXM)

 * Niroop Ammbashankar from Intel discussing Federated Learning (FL) and OpenFL for building better AI models using private data
* Agenda: Overview of FL, Introducing OpenFL, FL architecture and security extension in OpenFL, Real-world usage of OpenFL
* Challenges with centralized learning in healthcare and other industries due to data distribution, replication, movement, deidentification, and sovereignty
* Fed Learning Initiative (FED tumor initiative) using OpenFL for brain tumor segmentation across 71 sites in six continents and 20+ countries
* Centralized learning vs Federated Learning: In centralized learning, data is sent to a central site for model training; in Federated Learning, the local data is used for training and only weights are shared with an aggregator
* Collaborative Learning techniques like incremental learning in Federated Learning
* Comparison of Fed Learning and traditional centralized learning in terms of convergence quality and access to more data for generalizing models
* Introduction of OpenFL: An open-source software system for federated machine learning, started by Intel in 2018 as a Linux Foundation project
* OpenFL's architecture includes aggregator, collaborator (institution), and model owner; it uses a secure connection with mutual TLS and supports various frameworks like TensorFlow, PyTorch, etc.
* Federated Learning involves the following steps: federation plan definition, experiment start, collaboration, aggregation, and validation
* Security concerns in Federated Learning include model IP theft, unauthorized data access, and malicious entity participation; OpenFL provides security features through Intel's SGX software guard extension and Trusted Execution Environment (TEE) to protect code and data confidentiality.


## Algorithmic Challenges. An Educational Framework for AI & Ethics Literacy - Aldo Pisano

URL: [https://www.youtube.com/watch?v=AS4w-zL7-5I](https://www.youtube.com/watch?v=AS4w-zL7-5I)

 * Alo Pisano, PhD student and high school teacher from Italy, discusses the importance of ethics literacy in AI education
* Ethics literacy means fostering critical thinking and awareness to ensure responsible use of AI in future generations
* The prevailing mathematical model in education runs the risk of weakening critical thinking and debate skills
* Democratic approach is important for finding different solutions to ethical challenges through open dialogue
* AI can be useful in education, particularly in data analysis and personalized learning
* Ethical considerations are crucial when building new rules and regulations for AI use in society
* The European AI Act aims to regulate AI usage with principles such as transparency, accountability, and human-centeredness
* Industries must be responsible for preventing unacceptable risks, such as manipulation and social scoring practices
* Ethical education can help students develop critical thinking skills and understand the importance of respecting freedom in technological development
* The ancient Greek concept of "pidea" model still applies to education today, promoting autonomous individuals and a democratic society
* Students need to be educated about the potential biases and risks associated with AI decision-making tools
* Critical thinking and fact-checking are essential skills for navigating AI's impact on information and social spaces.


## A Deep Dive Into Retrieval-Augmented Generation with LlamaIndex - Laurie Voss, LlamaIndex

URL: [https://www.youtube.com/watch?v=CDfpJp8IpV8](https://www.youtube.com/watch?v=CDfpJp8IpV8)

 * Laurie Voss, VP Developer Relations at Llama Index, discussing Retrieval-Augmented Generation (RAG) using Llama Index
* RAG: core idea is to augment prompts with relevant data from a vector space index
* Llama Index: open source framework for building RAG applications, available in Python, TypeScript, and JavaScript
* Llama Hub: software registry that provides connectors, tools for building and deploying RAG applications
* Vector store index: takes care of embedding data into a vector space for retrieval
* Indexing: loading and embedding data from various sources (Connectors in Llama Hub)
* Querying: retrieving relevant data based on user queries, combining context data with LLM output
* Advanced query strategies: multi-stage querying, synthesizing responses, metadata filtering, hybrid search
* Basic querying: simple retrieval of top relevant context data and sending to LLM for response generation
* Naive RAG stack: simple query engine that may need customization depending on data shape and size
* Pipeline building: configuring transformations, caching, and rerunning pipelines
* Metadata filtering: hybrid search using metadata filtering in vector databases like Cassandra
* Prompting: customizing prompts using the query engine to get a better LLM response
* Basic form querying: simple retrieval of context data using default settings
* Customized querying: retrieving larger context data and configuring synthesizer engines
* Advanced retrieval strategies: embedding retrieval, recursive retrieval, text-to-SQL multidocuments, agent-based querying.


## Chatbot Arena: An Open Crowdsourced Platform for Human Feedback on LLMs - Wei-Lin Chiang

URL: [https://www.youtube.com/watch?v=Ktby6XR3xII](https://www.youtube.com/watch?v=Ktby6XR3xII)

 * Wayan Chiang, UC Berkeley LMS team member, introduces Chatbot Arena, an open crowdsourced platform for human feedback on language models (LLMs)
* Team's goal: build an open model, open data set, and serving system evaluation platform for LLMs
* Developed Mikuna long-chat model and Cha Arena benchmark
* Open data set collected from Allen's Chat 1M conversations
* Longer context lens model makes a significant difference
* Launched Viona project in March 2023 with significant improvement over Lama, Apaka models
* Two major issues: evaluating LLMs' properties and collecting high-quality human conversation data
* Evaluating LLMs' properties: handle general tasks, interact with users in multiturn conversations, handle unstructured text input/output, and avoid memorizing training data
* Collecting high-quality data: use real-world human conversation feedback through Chatbot Arena to improve models and understand use cases better
* Platform launched in May 2023, collecting 100,000 user votes and developing a scalable model ranking system using an ELO rating system
* Industry recognition from Greg Brockman, Andrew Ng, Retreat, Anthropic, Hugging Face, and more
* Benefits: evaluate rank models, collect valuable human feedback, prevent cheating, filter low-quality prompts, and develop efficient sampling algorithms.


## AutoGen: A Multi-Agent AI Framework - Victor Dibia & Chi Wang, Microsoft

URL: [https://www.youtube.com/watch?v=MCYzYzaWChY](https://www.youtube.com/watch?v=MCYzYzaWChY)

 * Victor Dibia and Chi Wang presented Autogen, a multi-agent AI framework by Microsoft
* Autogen enables next-generation AI applications through coordination of multiple intelligent agents
* Agents can perform reasoning, solve complex tasks, learn, and understand context
* Autogen follows the philosophy of greater sum being greater than the individual parts
* Autogen is designed to integrate large models, different tools, and even humans into multi-agent corporations
* Key concepts include conversation programming, flexible conversation patterns, and generic abstraction
* Autogen extends the developer's choice in building agents with conversational capabilities
* Conversation programming is a new programming paradigm centered around inter-agent conversation
* Autogen has gained significant interest in the community and received numerous awards since its launch
* The framework offers various features, including dynamic group chat and conversational workflows.


## Panel Discussion: Why a Universal Definition of 'Open Source AI' is Essential for Humanity

URL: [https://www.youtube.com/watch?v=Pv6qXfrKVos](https://www.youtube.com/watch?v=Pv6qXfrKVos)

 * Panel discussion on the need for a universal definition of Open Source AI
* Three panelists: Sal Kim (developer community, startup), Roman Chashnik (Apache Software Foundation), Dasha Iskander (ai startup, venture capitalist)
* Importance of understanding open source culture and benefits in the context of AI development
* Apache Software Foundation encourages community collaboration and transparency in open source projects
* Different AI open source communities have developed their own rules and licenses over the years, leading to confusion
* The need for a clear definition of open source AI is essential to ensure innovation, maintain trust, and avoid regulatory issues
* OSI (Open Source Initiative) is working on defining an open source AI license for October 2024
* Open source software has its origins in the software world and faces challenges in the context of AI, including data sets, models, algorithms, and ethical considerations
* The need for a practical approach to open source AI that balances openness and privacy, security, and toxicity
* Different countries and regulations may impact the definition and implementation of open source AI
* Emerging technologies like AI require a new regulatory framework, and it's important to work together to define open source AI in a way that benefits everyone.


## How to Adopt Emerging Federated Learning Technologies - Myungjin Lee, Cisco Research

URL: [https://www.youtube.com/watch?v=QAhtCIhxMTg](https://www.youtube.com/watch?v=QAhtCIhxMTg)

 * Meong Jin introduces Federated Learning (FL), a collaborative machine learning technique where data remains on devices and models are trained locally without sharing data
* FL is different from centralized learning where data is shipped to a central location for training
* Canonical use case of federated learning is keyword recommendation on mobile devices
* The challenge with Federated Learning is privacy and data sovereignty issues
* Federated Learning can be used in various types of scenarios, including cross-device, cross-silo, vertical, synchronous, and asynchronous approaches
* FLAMe is an open-source federated learning project by Cisco Research to facilitate experimentation with different federated learning technologies
* The goal of FLAMe is to allow ML engineers to focus on model development while handling monitoring, training functionality, multicloud support, and various topologies
* FLAMe provides easy topology update and communication abstraction without disrupting the system structure.
* To use FLAMe, a user can build a simple two-tier topology using an SDK that supports aggregator and trainer classes and inherit their behaviors. The user also interacts with the control plane to submit jobs and save states in a database.
* In a demo, Meong Jin shows how to create a design, set up roles, add code files, and create and launch a job using FLAMe's UI and CLI tools.
* Federated learning is a good solution for privacy-related challenges where data may be reluctant to share or be leaked. It can also be used in the IoT space for predictive maintenance use cases.
* Researchers are putting a lot of effort into federated learning, particularly in addressing user data release and model leakage issues using differential privacy algorithms.


## Enable Generative AI Everywhere with Ubiquitous Hardware and Open Software - Guobing Chen, Intel

URL: [https://www.youtube.com/watch?v=XtDEoN1DpvI](https://www.youtube.com/watch?v=XtDEoN1DpvI)

 * Guobing Chen from Intel discusses enabling generative AI everywhere using hardware and open software
* Both hardware (CPU, GPU, XPU) and software are necessary for AI
* Intel provides hardware optimizations like vectorization instructions, new Matrix-based computation modules (AMX accelerator), and optimization libraries (Intel extension) for AI workloads
* Software ecosystem includes popular frameworks like TensorFlow and PyTorch, as well as Intel's open-source optimization library called Intel Graph Analytics SDK
* Intel Extension provides latest optimizations for CPU-XPU environments and reduces overhead
* Large models (like Llama GPJ) can be run efficiently on Intel hardware using low precision computation and indirect access techniques
* Stable Diffusion, a generative AI model, can generate images faster than human reading speed on single CPU with Intel hardware
* Intel aims to provide simple solutions for running AI models in the cloud (AWS, GCP) and easily accessible through software like PAEx.


## Securing LLMs in Kubernetes: Best Practices - Meenakshi Kaushik & Jayanth Srinivasa, Cisco

URL: [https://www.youtube.com/watch?v=ZF9rPkm20NY](https://www.youtube.com/watch?v=ZF9rPkm20NY)

 * Meenakshi Kaushik from Cisco's Emerging Technology Incubation Unit,Shift, discusses securing Large Language Models (LLMs) in Kubernetes.
* Three challenges to securing LLMs: new and rapidly evolving security measures, data flow, and blast radius.
* Traditional application security measures don't work for LLMs due to their unique data flow, which can scrape kubernetes data and prompt responses from the infrastructure.
* Prevention and mitigation techniques: supply chain security, API security, model denial service, identity access management, and Ingress filtering.
* Kubernetes Security Gateway is recommended to secure LLMs, with an enterprise application front end talking to the actual LLM and performing request response filtering and caching.
* Different types of egress gateways may be needed for different LLMs due to their unique requirements.
* Use Envoy filter in ESS Gateway for simple Lua filters and proxy conf functions.
* Monitoring services can detect prompt injection errors, leakage, and toxicity using libraries like vogs or entity recognition models.
* Jailbreak attempts can be detected by comparing LLM responses and input.
* Using cosine similarity and contextual embedding matching for hallucination relevance detection is effective.
* Standard library match and RNN model are useful for detecting PII information leakage and toxicity.
* Large language models may describe larger contexts, making sense if used appropriately, but can also be a security risk.
* Continuously evolving measurement techniques are important in securing LLMs.
* Existing Kubernetes tools, such as ESS Gateway and Envoy filter, can help jumpstart LLM security.


## End-to-End MLOps with MLflow and Kubeflow - Nick Chase, CloudGeometry

URL: [https://www.youtube.com/watch?v=dIamK2NA-G8](https://www.youtube.com/watch?v=dIamK2NA-G8)

 * Speaker: Nick Chase, senior director of product management at CloudGeometry, with background in AI and infrastructure
* Topic: End-to-End MLOps with MLflow and Kubeflow
* MLops overview: similar to DevOps for machine learning, involves tracking experiments, deploying models, securing the supply chain, and monitoring applications
* MLflow: open-source project for managing machine learning experiments and models, provides a digital lab notebook, model registry, and model serving
	+ Digital lab notebook: tracks experiments, saves training results, groups runs, and provides search functionality
	+ Model registry: stores and registers trained models, allows sharing models with others, and provides versioning
	+ Model serving: serves machine learning models, allows predictions to be made remotely
* Kubeflow (CP Flow): MLops platform based on Kubernetes, allows creating and managing Kubernetes resources, integrates with various machine learning frameworks, and supports preprocessing, training, and serving models
* MLflow and Kubeflow integration: MLflow can be used to track experiments in a Kubeflow pipeline, and Kubeflow can be used for scaling machine learning models using MLflow
* Key benefits of MLflow and Kubeflow:
	+ Trackability for existing processes
	+ Lightweight for single-person setup
	+ Integration with kubernetes (for CP Flow)
	+ Model management with registries (MLflow and CP Flow)
	+ Bridge between different environments and frameworks
	+ Scaling using Kubeflow and MLflow models in Docker containers.


## Empowering Open-Source Generative AI by Integrating the Wikidat... Jonathan Fraine & Lydia Pintscher

URL: [https://www.youtube.com/watch?v=fx5K_FRQ2eg](https://www.youtube.com/watch?v=fx5K_FRQ2eg)

 * Jonathan Fraine from Wikipedia Germany (Wikimedia Deutschland) talks about the importance of Wiki data in the open-source world and its integration with generative AI.
* Wiki data, a free, linked open data Knowledge Graph with over 100 million entries, is used extensively in various applications such as search engines, mobile apps, and information retrieval systems.
* Wikimedia Deutschland focuses on organizing Wikipedia and maintaining Wiki data, which includes software development, political education, scientific engagement, volunteer operations, and fundraising technology.
* The German speaking Wikipedia also operates a WikiBase Cloud, offering access to real-time multilingual information, and is currently hiring for positions related to Wiki data.
* Fraine discusses the benefits of using Wiki data in generative AI applications, such as question answering, entity recognition, and classification.
* He mentions using Sparkle Protocol, a recursive acronym for "Sparkling RDF Query Language," to access Wiki data and create a wrapper around it to make it more accessible and valuable for various use cases.
* Fraine also discusses the importance of trustability in using Wiki data, as well as potential applications in vandalism detection and community management.
* He emphasizes the need for collaboration between the open-source community and organizations to build better AI models and tools using Wiki data.


## Simplify End-To-End MLOps with PostgresML - Montana Low, PostgresML

URL: [https://www.youtube.com/watch?v=kK3W2qpVa8E](https://www.youtube.com/watch?v=kK3W2qpVa8E)

 * Montana Low, co-founder and CEO of PostgresML, discusses machine learning inside databases
* Database infrastructure can put additional strain on resources, especially GPUs
* Montana shares his experience building a software engineering company focusing on natural language processing for 20 years
* He open-sourced Instacart's Lore platform six years ago and encountered scalability issues when the company grew rapidly during the COVID-19 pandemic
* Ideally, he wants a database optimized for machine learning workloads with good maintainability and manageability
* He compares PostgreSQL (PostgrML) to popular databases like Redis, Cassandra, Elasticsearch, and others
* PostgreSQL can generate embeddings 10 times faster than OpenAI and offers model control without quota limits
* PostgresML adds vector database capabilities directly inside PostgreSQL, making it a more comprehensive solution
* Montana discusses the importance of sharding data for handling large-scale systems and managing one database effectively
* He mentions Amazon RDS as a common choice for machine learning applications but highlights considerations when selecting a database
* PostgresML aims to provide a single, efficient way to handle both transactional and analytical workloads in the same system
* Montana explains how machine learning data is typically denormalized, making it harder to manage and join, and discusses Cassandra's solution to this problem
* He also talks about the importance of real-time machine learning and how databases like PostgreSQL can help improve efficiency and reduce costs.


## The Case for Open Source in Autonomous AI Systems in the Edge - Tina Tsou, Arm & Sujata Tibrewala

URL: [https://www.youtube.com/watch?v=ku_pXLAGSNw](https://www.youtube.com/watch?v=ku_pXLAGSNw)

 * Tina Tsou and Sujata Tibrewala discuss the importance of open source in autonomous AI systems at the edge
* Open source is a catalyst for innovation in AI and edge computing, fostering collaboration and transparency
* Autonomous AI systems are designed to perform tasks without intervention, using sophisticated algorithms and vast data sets
* Early history of AI development was marked by limitations such as cost and memory capacity
* Recent advances like machine learning and deep learning have made real-time processing possible
* Open source projects like TensorFlow and PyTorch have played a key role in the advancement of AI
* Edge computing is important for security, privacy, and efficiency in data processing
* Pretraining and fine-tuning models with large datasets are crucial for improving performance
* Applications of edge AI include smart manufacturing, urban environments, healthcare, and customer service
* Open source projects like Fedora ML and LF Edge are driving innovation in edge computing and AI
* The future of AI is collaborative and open, inviting active participation from the community.


## The Future of Open Source AI & The Generative AI Commons - Matt White, Generative AI Commons

URL: [https://www.youtube.com/watch?v=kyYaYquzyUE](https://www.youtube.com/watch?v=kyYaYquzyUE)

 * Matt White, from Generative AI Commons at Linux Foundation, discussed the future of open source AI
* Open source trends in generative AI: 50 businesses already using it, ratio of open vs closed solutions is 4 to 1
* Benefits of open source for generative AI include flexibility and innovation potential
* Development of generative models using open source vectors databases like Milis Cassandra
* Generative model life cycle referred to as GenOps streamlines development, deployment, monitoring, and maintenance
* Managed model marketplaces offer availability, flexibility, reliability, and accessibility
* Open source movement in AI is shifting towards closed science and restricted data usage
* Challenges for open source generative AI community include commercialization, privacy preservation, fragmentation, and accessibility
* Lack of standards and interoperability within the field creates integration hurdles
* Concerns about bias, unintended consequences, and scalability in open source generative models
* Importance of addressing ethical and sustainable AI development
* Discussion on the convergence of open science and open source AI.
* Generative AI Commons initiative was launched to foster a responsible, trustworthy, and inclusive generative AI ecosystem.
* Key components of the initiative include model data workstream, frameworks workstream, application workstream, and education outreach workstream.
* The initiative encourages open science, neutral governance, collaboration, and transparency in the development of generative AI.


## Open Source DataOps and MLOps Strategies - Lisa Cao, SystemsNull

URL: [https://www.youtube.com/watch?v=nohEOrCrKBw](https://www.youtube.com/watch?v=nohEOrCrKBw)

 * Lisa Cao, SystemsNull product manager, discussing DataOps and MLOps strategies with a focus on open source technologies
* DataOps definition: automation of data operations, securing and managing the data supply chain, orchestration, pipeline implementation, metadata management, and data quality monitoring
* MLOps: specific culture around machine learning model deployment and management
* Open source frameworks and tools for DataOps and MLOps will be discussed exclusively
* DataOps components include CI/CD, containerization, documentation, and managing complex data pipelines
* Increased need for DataOps due to the move towards data platforms, data mesh, and cloud systems
* Importance of data governance and Legal compliance
* Data Ops strategies: data quality focus, visibility transparency, scalability, and infrastructure debt management
* Four strategies for companies: data quality focus, organization focus, business goal focus, and stakeholder buy-in focus
* Examples of open source tools for data observability, metadata collection, testing, and orchestration
* Gravitino, a metadata lake open sourced by Data Sto, to create a single source of truth and enable easy access to metadata across different data systems without complex join operations.


## Scaling Kubernetes Clusters for Generative Models: Managing GPU Resources for AI App... Jack Min Ong

URL: [https://www.youtube.com/watch?v=oewggfUIEcQ](https://www.youtube.com/watch?v=oewggfUIEcQ)

 * Jack Ong, machine learning engineer at Gina Ai, discussing scaling AI workloads on Kubernetes and managing GPU resources
* Reasons for using Kubernetes instead of bare metal: ease of deployment, scalability, simplified GPU management
* Multi-instance GPUs allow sharing of powerful GPUs among smaller models or tasks
* Sharing GPU resources in Kubernetes: GPU instance profiles, partitioning, compute instances
* Installing the Nvidia GPU operator for Kubernetes, managing resource limits and node selectors
* Best practices for scheduling non-GPU workloads on GPU nodes to avoid wasting resources
* Sharing GPUs among containers using fractional resources and time slicing
* Techniques for optimizing deep learning workloads: low precision arithmetic, attention slicing, speculative decoding
* Summary: Kubernetes makes it easy to share GPU resources among workloads through the Nvidia GPU operator and various management techniques. Options for managing GPUs include Mig partitioning and time slicing.


## Streamlining RAG Applications with Pinecone's OSS Framework: Canopy - Audrey Lorberfeld, Pinecone

URL: [https://www.youtube.com/watch?v=pLSDTTMhWhk](https://www.youtube.com/watch?v=pLSDTTMhWhk)

 * Canopy is an open-source RAG (Retrieval and Generation) framework developed by Pinecone, which abstracts away the complex tasks involved in creating a RAG application.
* It handles data chunking, vector SL embeddings generation, query optimization context generation, LLM orchestration, and management using the Pinecone API key and environment.
* Canopy supports the use of various embedding models and allows developers to easily populate the index with data in Parquet, JSON, L, and CSV formats, which is backed by free storage and compute from Pinecone.
* The framework can handle large-scale production demands seamlessly and offers a nice dashboard for monitoring indices and managing HIPAA, GDPR, and SOC 2 compliance certifications.
* Canopy's two-step Gen workflow involves user queries being ported to the vector database, which then returns vector search results that are combined with context for the LLM to access and generate answers.
* The framework abstracts away heuristics for chunking data, embedding model selection, and vectorization, making it easier for developers to build RAG applications without having to deal with these complex tasks themselves.
* Canopy is fully open-source and offers easy configuration through a YAML file, allowing users to control various aspects of the framework such as deployment options, libraries, and stack components.
* The framework includes a chat engine for handling prompts, saving chat history, and managing multi-turn conversations, as well as a state context engine for building and managing context in the Pinecone Vector database.
* Without Canopy, building a RAG pipeline would involve significant time and research for tasks such as chunking data, selecting an embedding model, and configuring various settings.
* Canopy offers several advantages over other tools like Langchain, Llama Index, etc., including easier configuration, abstracted-away heuristics, and a more streamlined development process.


## Leveraging SRE and Observability Techniques for the Wild World of Building on LLMs - Christine Yen

URL: [https://www.youtube.com/watch?v=qKRIzIW-dLU](https://www.youtube.com/watch?v=qKRIzIW-dLU)

 * Christine Yen, CEO and co-founder of Honeycomb, discusses observability techniques for building on large language models (LLMs)
* LLMs are unpredictable and non-deterministic, making traditional testing and debugging approaches challenging
* Observability is essential to ensure high-quality user experience and prevent hallucinations in complex systems
* Capturing metadata and understanding what's happening in the application is key to building great experiences with LLMs
* Instrumentation and documentation are necessary for capturing logic, validating hypotheses, and debugging production issues
* Measuring outside of the LLM can help identify unexpected behavior and allow teams to iterate quickly
* Telemetry data can provide insights into user experience and performance, enabling teams to make improvements
* Modern observability tooling allows flexibility to add new fields and paths as needed
* Tools like Honeycomb can help teams investigate issues from beginning to end and determine if they're delivering value.


## Reducing Hallucinations and Evaluating LLMs for Production - Divyansh Chaurasia, Deepchecks

URL: [https://www.youtube.com/watch?v=unnqhKmMo68](https://www.youtube.com/watch?v=unnqhKmMo68)

 * Divyansh Chaurasia from Deepchecks discussed reducing hallucinations and evaluating Large Language Models (LLMs) for production readiness at LMOP space.
* Deepchecks helps with continuous validation of LLMs in the open-source community.
* Traditional evaluation methods and AI evaluation frameworks like Hugging Face Open AI leaderboard were discussed.
* The challenge is to minimize hallucinations (wrong outputs, biases, randomness) in LLMs for production applications.
* Difference between LLMs and comedians: Comedians start from scratch, while LLMs build on existing knowledge bases and pipelines.
* To prepare an LLM application for production, follow these steps:
  + Map relevant knowledge databases.
  + Build the application architecture.
  + Release and improve performance.
* Improving application releasing involves iterating and comparing multiple LLM models to choose the best one based on the use case.
* Hallucinations can be caused by various reasons:
  + Biased training data.
  + Overfitting due to limited training data or high model complexity.
  + Incorrect prompts.
* Minimizing hallucination requires a comprehensive evaluation method and understanding model performance.
* Evaluation criteria for LLMs include accuracy, relevance, coherence, creativity, grammatical correctness, and bias.
* Human evaluation is expensive but necessary for production-level applications.
* Open LLM leaderboards like Hugging Face can help evaluate multiple models and choose the best one based on a benchmark.
* Continuous validation is crucial to ensure app performance and mitigate hallucination issues.


## Lightning Talk: Turning the Internet Into 3D: 3D Content with NeRF and Gaussian Splat... Barkley Dai

URL: [https://www.youtube.com/watch?v=75t6M-SVWxo](https://www.youtube.com/watch?v=75t6M-SVWxo)

 * Barkley Dai from Luma AI, a 3D content format startup
* Goal is to make 3D interactive experiences a part of daily life, like dreaming in vivid 3D
* Previously, creating 3D content was a linear process with a lot of trial and error
* Luma AI platform allows users to capture 3D environments using images or videos, and create interactive 3D experiences
* Examples: Zay, the lead artist of One Direction, used Luma AI for a music video; Karen Chung, an Instagram artist, used it to create drone-like shots without a drone
* GAN Spatting (Gaussian Splatting) is a new technology that generates 3D models from point clouds without rendering
* With GAN Spatting, users can see the 3D version on their phone and interact with it instantly
* Luma AI aims to make 3D content creation more accessible and instantaneous, allowing for new use cases like interactive video and AR/VR experiences.


## Using Generative AI to Develop Open Source Code - Practical Guidance for Managing Lega... Joanna Lee

URL: [https://www.youtube.com/watch?v=Gtuvria9mgI](https://www.youtube.com/watch?v=Gtuvria9mgI)

 * Joanna Lee, VP Strategic Programs at Linux Foundation and Cloud Native Computing Foundation, discussed legal risks and compliance challenges related to using generative AI for developing open source software.
* Copyright law protects original expression in works like code, but not ideas or facts. AI-generated code may be eligible for copyright protection if it meets the minimal creativity requirement.
* Using AI tools to generate code from preexisting code raises potential copyright infringement and licensing issues. Make sure you have proper permissions or licenses for training data and the software being used.
* Be aware of contractual obligations and open source license compatibility when contributing AI-generated code to projects. Some open source licenses may not allow using tools that produce incompatible output.
* Keep up with evolving regulations, like the EU Artificial Intelligence Act and US executive orders, which may impose compliance obligations on AI providers and users.
* Companies need to manage trade secret risks when using AI tools, especially if they're provided by third-party vendors. Use features like code referencing and filtering to help mitigate these risks.
* Make sure you understand the open source project's policies around AI-generated content and follow them closely to ensure compliance. This may include adding notices or tags to indicate AI input.
* Some companies may prohibit or restrict the use of generative AI in code development due to concerns about copyright infringement, trade secrets, or ethical considerations. Others may have more permissive policies.
* Use features like safe coders and self-hosting to minimize risks when using AI tools for generating code. Be aware that best practices and regulations are evolving rapidly, so keep your policies up to date.


## Lightning Talk: AI Penetration Testing - Blake Turrentine, HotWAN

URL: [https://www.youtube.com/watch?v=NyIqMdI0oSQ](https://www.youtube.com/watch?v=NyIqMdI0oSQ)

 * Blake Turrentine from HotWAN presented on AI penetration testing
* Discussed various AI attack methods: prompt attack, extraction, backdoor, poisoning, aerial, and tax
* Demonstrated multimodal attack using file fuzzing and voice text
* Used Whisper model for text transcription from audio files
* Found vulnerabilities by bit flipping in audio files, causing stack traces
* Fuzzed FFMpeg parameters to cause segmentation faults and null pointer references
* Attempted to break the model by adding Echo Distortion to audio files, which was compensated for by the model using echo cancellation.


## Workshop: Empowering Collaboration: AI Developer Experience - Your Bridge from Mod... Cedric Clyburn

URL: [https://www.youtube.com/watch?v=VAM-FSz0oPg](https://www.youtube.com/watch?v=VAM-FSz0oPg)

 * Cedric Clyburn, Red Hat Developer Advocate, discussing AI developer experience for data scientists and application developers
* Discussing creating a whole story from model build to deployment on Kubernetes using Open Source projects like CP flow, Jupiter Hub, and various libraries
* Demonstrating the use of Jupyter Notebook with containerized environment for reproducibility and scalability
* Importance of collaboration in AI/ML development with different personas involved, including business leadership, data engineers, application developers, and ML engineers
* Challenges organizations face when operationalizing AI models and the role of open source tools and communities in solving these challenges
* Introducing Red Hat OpenShift Data Science built on top of Open Data Hub to simplify the development process
* Live demo using Jupyter Hub, TensorFlow, PyTorch, and model serving with Seldon Core
* Discussing the importance of automating ML operations for faster iterations and improved productivity.


## Workshop: Prompt Engineering with Watsonx - Rafael Vasquez, IBM

URL: [https://www.youtube.com/watch?v=fJe2UjTdv9A](https://www.youtube.com/watch?v=fJe2UjTdv9A)

 * Rafael Vasquez leads a workshop on prompt engineering with Watsonx
* Participants are encouraged to follow along in the lab workshop instruction online
* Prework involves signing up for a free trial IBM Cloud account and accessing the Watson Studio
* Overview of the lab includes exercises that allow users to experiment with different prompts and generate text using the language model
* Users can try generating simple outputs, comparing models, and tuning prompts for better results
* Demonstration is given on how to use structured view in the lab, where users can input a prompt, see the output, and modify the prompt as needed
* Users are reminded of the chat interface's expectation of coherent responses and advised to give clear instructions to the model
* Different models with various capabilities are introduced and users are encouraged to try different models for different tasks
* Users can save prompts in the notebook and view them later
* Users are shown how to generate text using Llama 2 and given an example of generating a list for a lemonade business
* Users are advised to experiment with various decoding methods, including greedy and sampling, and adjusting model parameters such as token length and temperature for creativity control
* The importance of random seed in ensuring reproducibility is discussed.
* Rafael shows an example of generating creative sentences using a donkey prompt and encourages users to try different prompts and examples.
* Users are advised to be mindful of token limits and given an example of how the model can decide when to stop outputting text based on sequence.
* Rafael demonstrates random seed variable in sampling and shows that every time generate, the user will get different outputs.
* He encourages users to explore different models, finetune them for specific tasks, and experiment with various techniques for better results.


## Workshop: Log Processing with Apache NiFi and Apache Kafka - Sophia Izokun, IBM

URL: [https://www.youtube.com/watch?v=gQzGSnu-yGA](https://www.youtube.com/watch?v=gQzGSnu-yGA)

 * Sophia Isoken, IBM data scientist, presents log analytics using Apache NiFi and Apache Kafka
* Log analytics: analyzing machine-generated data from various sources
* Challenges: massive volume, variety, lack of consistency, different formats, and inconsistent categories
* Apache NiFi: open-source tool for data ingestion and building data pipelines, real-time data processing
* Apache Kafka: distributed data storage system for real-time streaming data, acts as a central hub for log data
* OpenSearch: data set for this workshop, log data from NASA Kennedy Space Center web server (July 1995)
* Architecture: ingestion layer (Apache NiFi), data transportation layer (Apache Kafka), and analytics search layer (OpenSearch)
* Data set format: unstructured, one line per request with fields for host, time stamp, request, response code, and size
* Processing: Apache NiFi reads log data, Apache Kafka stores and processes the data, OpenSearch analyzes and visualizes the data
* Installation: ensure Java is installed, download Apache NiFi from website, configure properties, start NiFi using CLI
* Setup: create process group in NiFi, configure Apache Kafka connection, publish log data to Kafka topic, consume data from topic, create flow files, split text, and publish data to OpenSearch index
* Processing transformation layer: extract fields, update attributes, convert attributes to JSON, enrich data using scripts, route data based on conditions, and add custom attributes
* Data visualization: use OpenSearch dashboard for analysis and visualization, create charts and graphs.


## What Is It Like to Be an LLM? - Matt Butcher, Fermyon

URL: [https://www.youtube.com/watch?v=jIa3WxsU7bo](https://www.youtube.com/watch?v=jIa3WxsU7bo)

 * Matt Butcher, CEO of Fermyon and trained philosopher, discusses large language models (LLMs) and philosophy
* He explains that LLMs use philosophical thought experiments to understand complex concepts
* One example is the Evil Deceiver problem from Plato's Republic, where prisoners in a cave only see shadows on a wall and must question what reality is
* Another example is Descartes' evil demon experiment, which raises doubts about perception and the physical world
* Butcher argues that LLMs cannot truly understand consciousness or ethics as they lack human experience and cognitive capacity
* He also discusses the trolley problem ethical dilemma and how it challenges our understanding of ethical reasoning
* Butcher mentions that LLMs are limited to processing information given to them and cannot have experiences or emotions.


## Workshop: LLMs Fine Tuning and Inferencing Using ON... Abhishek Jindal, Sunghoon Choi & Kshama Pawar

URL: [https://www.youtube.com/watch?v=lOp8WK0fa94](https://www.youtube.com/watch?v=lOp8WK0fa94)

 * The speakers, Abhishek Jindal, Sunghoon Choi, and Kshama Pawar, discussed fine-tuning and inference using Onyx runtime for large language models.
* They used a Mistral 7 billion model as an example to demonstrate the benefits of using Onyx runtime and Deep Speed.
* The agenda covered code execution, use case improvement, training time reduction, performance comparison, and UI walkthrough.
* Onyx runtime is a cross-platform machine learning model accelerator that integrates with various frameworks like PyTorch, TensorFlow, and DFPSL. It optimizes the graph and runs the code faster using fusion and optimized kernels.
* The speakers demonstrated converting a PyTorch model to Onyx format using a simple example and showed how it optimized the single note using Onyx runtime.
* They also discussed Onyx inference, which uses a high-performance engine for deployment in production environments on various platforms like Linux, Windows, Mac, mobile, and web.
* The speakers compared the performance of PyTorch, TorchScript, and Onyx runtime for model inference using four models (Lama 2, Mistral, Orange, and a Vision model) and showed that Onyx runtime had better throughput and faster inference times.
* They also mentioned the benefits of using Azure Machine Learning (ML) with Onyx runtime for training, finetuning, and deployment.
* The speakers showed how to use an ACP (Azure Container Instances for PyTorch) image for fine-tuning models, and they demonstrated running a CLM (Causal Language Model) using Deep Speed Stage 2 with Onyx runtime, which had better performance compared to PyTorch.
* They also discussed the importance of reducing training time and compute costs with the increasing amount of large language models, and how Onyx runtime can accelerate training and reduce these costs by utilizing Cuda ROM acceleration and supporting model sizes up to GPd2 16 GB on a single GPU.


## Workshop: Define "Open AI" - Mer Joyce, Do Big Good & Ruth Suehle, SAS

URL: [https://www.youtube.com/watch?v=zRGqdoQqXAw](https://www.youtube.com/watch?v=zRGqdoQqXAw)

 * Welcome to a workshop on defining "Open Source AI"
* Mayor Joyce from Big Good and Ruth Suehle from SAS are facilitating the session
* Disclaimer: Mayors expertise is in collaborative decision making, not AI
* The Open Source Initiative (OSI) has been working for almost 2 years to define open source AI
* The primary use case for evaluating an open source AI system is whether it is licensed under an open source license
* Small groups will work together to define specific types of AI systems and their licensing perspectives
* There are multiple stakeholders in open source AI including creators, regulators, and end users
* Connection to the Open Source community is important for successful adoption of open source AI
* There has been a lot of discussion around open source AI since June 2021, with a goal of achieving shared understanding
* The legal landscape for open source AI is complicated due to issues around data ownership and privacy laws
* There are challenges around finding and curating unbiased data sets and models
* Community norms and acceptable behavior around publishing papers and code are still evolving
* Open Source AI adoption requires transparency and explainability, with a focus on meeting the needs of various communities and fields.


## Keynote: Learnings from Studying 800 Generative AI Open Source Repos - Chip Huyen, Claypot AI

URL: [https://www.youtube.com/watch?v=YSrTjnK1AKE](https://www.youtube.com/watch?v=YSrTjnK1AKE)

 * Chip Huyen discussed her experience analyzing 800 generative AI open source repositories on GitHub
* Categories of repos: application (chatbots, image editing), model development and engineering (model weight change, inference optimization), infrastructure (computation management, vector databases), tooling (language models, AI model probabilistic nature), problem solving (memory management), and engineering (structured output handling)
* Surprised by the high number of open source applications in engineering development
* Categorized subcategories: inference optimization, inference servers, compression, latency/cost optimization
* New techniques: faster decoders (Medusa framework), look-ahead token generation, dynamic sequential continuous batching, model adapter concatenation
* Tool category also important for problem printing and dealing with engineering issues
* Majority of repos hosted on individual accounts versus organizations
* Distribution of star/forks interesting: users spend weekends viewing popular applications, companies top contributors
* 10,000 developers contribute to 800 repos, some contributing to as many as 300 repos
* Many small contributions (typo fixing) make a difference in open source projects.
* Encourages people to get started with contributing to open source and see the impact they can make.


