## A perfect match: Dapr & Azure Container Apps - Sander Molenkamp - NDC London 2023

URL: [https://www.youtube.com/watch?v=fz389EfukQY](https://www.youtube.com/watch?v=fz389EfukQY)

 In this video transcript, Sander Molenkamp discusses Azure container apps, Dapper, and their use in developing advanced applications. Key points include:

- Azure container apps are a flexible platform for deploying containerized microservices.
- Dapper is a serverless, open-source framework that simplifies building microservices, with an opinionated way to build block-based applications.
- Dapper offers advanced features like traffic routing, state management, and built-in support for HTTP, gRPC, and Kafka.
- Dapper helps developers build flexible applications with simple, low-level abstractions.
- Azure container apps and Dapper work well together, enabling advanced features without dealing with complex infrastructure details.
- Dapper components can be configured at the component level, which aids in debugging and testing.
- Dapper's state management allows storing key-value pairs in a data store, with the ability to scale stateful services.
- Dapper's building blocks, such as state management, allow developers to create scalable, stateful applications.
- Dapper's input binding allows developers to define an input binding for different building blocks, like mqtt or websocket.
- Dapper's publisher-subscriber pattern helps integrate message brokers like mqtt.
- Dapper can be used with Azure container apps to manage traffic routing, state management, and other advanced features.
- Azure container apps and Dapper enable developers to deploy applications quickly, with minimal infrastructure configuration needed.

The presentation also demonstrates the setup and implementation of an example application using Azure container apps and Dapper, illustrating the benefits of using these technologies.


## You Keep Using That Word: Asynchronous And Interprocess Comms (Remote) - Sam Newman

URL: [https://www.youtube.com/watch?v=rg-qVok7B1g](https://www.youtube.com/watch?v=rg-qVok7B1g)

 In this transcript, Sam Newman discusses asynchronous and interprocess communication in the context of software architecture and microservices. Key points include:

- Asynchronous communication refers to the exchange of messages where sender and receiver are not in direct communication.
- Asynchronous communication is often seen as a better alternative to synchronous communication, as it allows for better handling of latency and the ability to continue processing without waiting for a response.
- Asynchronous communication can be implemented using various techniques such as message brokers, event-driven architectures, and reactive systems.
- The term "asynchronous" can have different meanings for different people, often depending on their background and experience.
- Asynchronous communication can help reduce the complexity of distributed systems by decoupling components and allowing them to work independently.
- The concept of temporal decoupling refers to the idea of separating the time at which sender and receiver communicate.
- Using the term "asynchronous" might not always be helpful, as it can lead to confusion and misinterpretation.
- The concept of asynchronous communication is closely related to the Reactive Manifesto, which emphasizes the importance of designing systems that are responsive, resilient, elastic, and message-driven.


## Exploring Serverless Options for .NET in Azure, AWS, and Beyond - Spencer Schneidenbach

URL: [https://www.youtube.com/watch?v=PbfeyIYjfH8](https://www.youtube.com/watch?v=PbfeyIYjfH8)

 Spoke at Spark Conference, speaker discusses three major cloud provider services for serverless computing: Azure Functions, AWS Lambda, and Google Cloud Functions.

- Azure Functions:
	+ Azure serverless offering
	+ Azure function, AWS Lambda, Google Cloud Function
	+ Azure App Service, Azure Cloud Apps
	+ Azure function, AWS Lambda, Google Cloud Function
	+ Azure Cloud Apps, AWS App Service
	+ Azure Cloud Apps
	+ Azure function, AWS Lambda, Google Cloud Function
	+ Azure Cloud Apps
	+ Azure function, AWS Lambda
- AWS Lambda:
	+ AWS serverless offering
	+ Azure Cloud Apps, AWS App Service
	+ Azure Cloud Apps, AWS App Service
	+ Azure Function, AWS Lambda, Google Cloud Function
	+ AWS Cloud Apps
	+ Azure Function, AWS Lambda, Google Cloud Function
- Google Cloud Functions:
	+ Google serverless offering
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps
	+ Azure Cloud Apps

The speaker discusses the following topics in their talk:

- Azure Function
- AWS Lambda
- Google Cloud Function

The speaker compares and contrasts the three serverless computing options, focusing on:

- Development experience
- Execution performance
- Cost
- Deployment process
- Scalability
- Monitoring
- Friction points
- Anecdote
- Opinion
- Narrative
- Best practice
- Complexity

They also share experiences with each cloud provider, discussing pros and cons.

- Azure Cloud Apps vs AWS App Service
- AWS Cloud Apps
- Azure Cloud Apps vs Google Cloud Apps
- Google Cloud Apps

They also share their experiences using each cloud provider and the lessons learned.

- Azure
- AWS
- Google Cloud

The speaker also discusses the experience using each cloud provider's serverless services and the lessons learned:

- Azure function
- AWS Lambda
- Google Cloud Function

Overall, the speaker discusses the pros and cons of each serverless computing option, and shares their experiences, including best practices, complexities, and lessons learned.

Overall, the speaker shares experiences and lessons learned using Azure Function, AWS Lambda, and Google Cloud Function, comparing them on aspects like development experience, execution performance, cost, deployment process, scalability, monitoring, friction points, anecdote, opinion, narrative, best practice, complexity, and cloud provider pros and cons.

I hope this gives you a concise understanding of the talk.

So, the speaker at this talk discussed their experiences with Azure Functions, AWS Lambda, and Google Cloud Functions, sharing pros and cons of each service, as well as best practices and lessons learned. They compared and contrasted each service on aspects such as development experience, execution performance, cost, deployment process, scalability, monitoring, friction points, anecdotes, opinions, narratives, best practices, and complexity.


## An Infrastructure as Code Bake-off, comparing ARM, Terraform and Bicep - Mike Benkovich

URL: [https://www.youtube.com/watch?v=QxBdzguISM4](https://www.youtube.com/watch?v=QxBdzguISM4)

 Here's a concise summary of the transcript:

1. The speaker discussed various infrastructure as code (IaC) tools and their use cases, focusing on Arm, Terraform, Bicep, and Azure CLI.
2. They gave an overview of each tool, mentioning their pros and cons, and explained how they can be used for different scenarios.
3. The speaker shared their personal experience with each tool, highlighting their preferred workflows and tools for specific tasks.
4. They discussed the importance of naming standards, tagging, and tooling for efficient IaC work.
5. They mentioned GitHub Copilot, a code assistant tool that can provide suggestions based on past code, and its impact on productivity.
6. The speaker emphasized the importance of choosing the right tool based on the specific needs of the team and project.
7. They concluded by stating that Arm is not going away anytime soon and that Bicep is a useful tool for making Arm templates more readable and easier to work with.

In summary, the speaker shared their experience with various IaC tools, highlighting their pros and cons, and discussed the importance of choosing the right tool for a specific project or team's needs. They also mentioned the potential benefits of using GitHub Copilot for increased productivity and emphasized the value of Arm and Bicep in infrastructure as code workflows.


## Adding Live, Interactive Video to Your Application with Amazon IVS - Todd Sharp - NDC London 2023

URL: [https://www.youtube.com/watch?v=yUd798ly5-U](https://www.youtube.com/watch?v=yUd798ly5-U)

 - Amazon Interactive Video Service (IVS) allows developers to create live streaming applications.
- IVS is suitable for full-focused user content like Twitch, YouTube Live, and Instagram Live.
- IVS provides SDKs for web, iOS, and Android platforms, supporting native features for live streaming and playback.
- Live video streaming has 80% watch time engagement, higher than on-demand content.
- Gen Z prefers using TikTok and Instagram for content discovery and consumption.
- IVS offers low-latency streaming with global infrastructure and private channel streams.
- IVS supports interactive chat, which enhances audience engagement.
- IVS can be integrated with existing applications, and supports open-source libraries and SDKs for live video streaming hosting and server management.
- IVS supports live video streaming, playback, and interactivity with low latency and high scalability.
- The IVS SDK can be used with web, iOS, and Android platforms, and supports interactive features like chat rooms and playback.
- IVS provides a CLI for creating channels and managing stream keys, and supports SDKs for creating interactive chat rooms and managing user authorization.
- IVS supports interactive features like chat rooms, live video streaming, and adaptive playback quality.
- IVS can be integrated with other services like AWS Lambda and Amazon S3 for enhanced functionality.
- IVS offers a free tier with limited usage, and a cost estimator tool for estimating costs based on usage.


## Running a real world, mission-critical system on Azure - Loek Duys - NDC London 2023

URL: [https://www.youtube.com/watch?v=NWBAS6YhAKc](https://www.youtube.com/watch?v=NWBAS6YhAKc)

 - Running a real-world, mission-critical system on Azure:
  - Gas storage domain
  - Azure services: Storage, Queuing, Compute, Data Storage, DNS, Traffic Manager, Application Gateway, Firewall, SQL Server
  - Dapper for abstracting implementation
  - Infrastructure as Code (IaC) for deploying and managing infrastructure
  - Blazor WebAssembly for the frontend
  - Docker and Kubernetes for containerization and orchestration
  - Telemetry and logging for monitoring
  - Redis and Pop Shop messaging for queuing and messaging
  - Cost optimization, budget alerts, and cost monitoring
  - Disaster recovery and failover using secondary Azure regions
  - Redundancy, geo-replication, and data consistency
  - Design patterns: Retry, Circuit Breaker, Outbox, Caching
  - Dependency management and cutting dependencies
  - Autoscaling and load balancing
  - Resilient system design and fault tolerance
  - Dealing with outages and reducing coupling
  - Using Azure services and features to handle various aspects of the system, such as storage, compute, queuing, and messaging.

Key takeaways:
- Azure provides a wide range of services that can be used to build and manage mission-critical systems.
- Infrastructure as Code, containerization, and orchestration help manage and deploy infrastructure efficiently.
- Design patterns and resilience are essential for building a reliable and fault-tolerant system.
- Cost optimization and monitoring are crucial for managing cloud resources effectively.
- Redundancy, geo-replication, and data consistency ensure high availability and minimize downtime.


## How Kubernetes optimisation can combat climate change - Annie Talvasto - NDC London 2023

URL: [https://www.youtube.com/watch?v=kVAYsq9uv7I](https://www.youtube.com/watch?v=kVAYsq9uv7I)

 - Annie Talvasto's presentation focuses on sustainability in the context of Kubernetes optimization and green software.
- Green software is a relatively new field that aims to reduce the environmental impact of software development.
- Kubernetes can help combat climate change by reducing resource consumption, improving efficiency, and promoting sustainability.
- Principle Green Software:
  1. Microservices context: building applications in small, independent pieces to optimize resource usage.
  2. Green Software Practice: applying best practices like optimizing resource usage, reducing distance traveled across networks, and shaping demand.
  3. Energy efficiency: considering hardware efficiency, carbon awareness, and green energy sources.
- Key areas to improve sustainability in Kubernetes:
  1. Optimize resource usage: use best practices to reduce CPU, memory, and storage usage.
  2. Location shifting: choose data centers with renewable energy sources.
  3. Time shifting: schedule workloads during low-energy consumption periods.
  4. Demand shaping: adjust application workloads based on energy consumption patterns.
  5. Traffic optimization: reduce traffic by caching, compression, and optimizing delivery.
  6. Green Software Foundation: a non-profit organization focused on promoting sustainable software development.
- Tools and projects for green software development:
  1. Green Software Foundation: promoting best practices, measuring carbon emissions, and providing resources.
  2. Kubernetes-based sustainability projects: focusing on energy efficiency, carbon awareness, and green energy sources.
  3. Green Cube: a tool that calculates CO2 emissions produced by Kubernetes pods.
  4. GitHub repositories: finding projects and tools related to green software development.
  5. Argo CD: a cloud-native continuous delivery tool that can help optimize Kubernetes usage.
- Annie encourages getting involved in the green software community, contributing to projects, and following sustainability tags on GitHub and Slack channels.
- Key takeaways:
  1. Optimize everything towards a better goal.
  2. Consider the environmental impact of your software development.
  3. Get involved in the green software community and contribute to open-source projects.


## Comprehensive testing strategies for modern microservice architectures - Adelina Simion

URL: [https://www.youtube.com/watch?v=t2M_G0LxFvk](https://www.youtube.com/watch?v=t2M_G0LxFvk)

 - Comprehensive testing strategies for modern microservice architectures.
- Importance of unit, integration, and end-to-end tests.
- Fuzz testing as an alternative to traditional testing.
- Test-driven development (TDD) and its benefits.
- Contract testing for better communication between consumers and providers.
- Utilizing Go language for writing tests and code.
- Go testing tooling and libraries.
- Testing pyramid and its significance.
- Testing tools and practices in CI/CD pipelines, such as GitHub Actions and Docker Hub.
- Test container concept for efficient testing.
- Importance of writing tests at different stages of development.
- Balancing code coverage and robustness.
- Emphasizing test quality culture and collaboration.
- Fuzz testing for identifying edge cases and improving code robustness.
- Utilizing Go's static checkers for better code quality.
- Using linters and formatters for maintaining code quality.
- Test-driven development and the agile process.
- Writing tests before writing code to avoid surprises.
- Writing unit tests to test individual components.
- Writing integration tests to test interactions between components.
- Writing end-to-end tests for complete workflows.
- Utilizing mocking to simulate dependencies.
- Contract testing for better communication between microservices.
- Using CI/CD pipelines to automate testing and deployment.
- Testing in production for real-world feedback.
- Importance of continuous feedback loops in development.
- Testing as a way to prove code correctness and functionality.
- Writing tests to cover edge cases and unexpected scenarios.
- Leveraging Docker and containerization for efficient testing.
- Emphasizing the importance of a testing culture and collaboration.
- Fuzz testing for identifying edge cases and improving code robustness.
- Utilizing Go's static checkers for better code quality.
- Using linters and formatters for maintaining code quality.
- Writing tests before writing code to avoid surprises.
- Writing unit tests to test individual components.
- Writing integration tests to test interactions between components.
- Writing end-to-end tests for complete workflows.
- Utilizing mocking to simulate dependencies.
- Contract testing for better communication between microservices.
- Using CI/CD pipelines to automate testing and deployment.
- Testing in production for real-world feedback.
- Importance of continuous feedback loops in development.
- Testing as a way to prove code correctness and functionality.
- Writing tests to cover edge cases and unexpected scenarios.
- Leveraging Docker and containerization for efficient testing.
- Emphasizing the importance of a testing culture and collaboration.


## IaC Forged in Code: ARM/Bicep vs Terraform vs Pulumi - Mike Benkovich - Copenhagen DevFest 2023

URL: [https://www.youtube.com/watch?v=FDq7Up16KSw](https://www.youtube.com/watch?v=FDq7Up16KSw)

 - Infrastructure code tools: ARM (Azure Resource Manager), Bicep, Pulumi, Terraform
- ARM: native to Azure, verbose, can create and manage resources directly, can integrate with Azure DevOps
- Bicep: simplifies ARM templates, cleaner and easier to read, transpiler converts Bicep to ARM templates
- Pulumi: cross-platform, supports different languages (C#, JavaScript, Python, etc.), supports multiple cloud providers
- Terraform: popular cross-cloud tool, HCL language similar to Bicep, supports different cloud providers
- Tools like Pulumi and Terraform are good for organizations with multiple environments and different cloud providers
- Arm templates, Bicep, and Terraform can be used to create, manage, and deploy infrastructure code
- DevOps tools like Azure DevOps can be integrated with these infrastructure code tools for version control, CI/CD pipelines, etc.
- Infrastructure code tools help in provisioning, managing, and automating infrastructure, improving efficiency, and reducing errors
- Each tool has its own strengths and use cases, and the choice depends on the specific requirements and context of the project

Overall, the discussion focused on the different infrastructure code tools available and how they can be used to manage and automate infrastructure in various scenarios and contexts. Each tool has its own pros and cons, and the choice depends on the specific requirements, preferences, and goals of a project or organization.


## Becoming API- & Cloud-First at the LEGO Group - Rasmus Hald - Copenhagen DevFest 2023

URL: [https://www.youtube.com/watch?v=vRJV0Q9wm_g](https://www.youtube.com/watch?v=vRJV0Q9wm_g)

 Summary of the LEGO Group's Becoming API & Cloud-First at Copenhagen DevFest 2023:

- The LEGO Group invested heavily in digital transformation, tripling their digital organization in the last two years, and expanding their digital team and locations.
- The goal is to create a Loosely Coupled architecture that enables API-driven, agile, and business-aligned development.
- The LEGO Group aims to balance newness, speed, and quality, while maintaining agility and responding to market demands.
- The LEGO Builder app showcases their focus on digital experimentation and a new digital experience for customers.
- LEGO has adopted a Cloud-First, API-First strategy, leveraging cloud services, APIs, generative AI, and blockchain for digital innovation and improved business agility.
- Key focus areas include API management, API discoverability, and investing in message brokers to support APIs and cloud services.
- The LEGO Group embraces an iterative approach to building and refining their digital architecture, learning from their customers and continuously improving their offerings.


## Your code is just a detail - Damian Brady - Copenhagen DevFest 2023

URL: [https://www.youtube.com/watch?v=sp4YnZvXjXU](https://www.youtube.com/watch?v=sp4YnZvXjXU)

 In the video transcript, Damian Brady discusses the following points:

- Code quality and efficient algorithms
- The impact of new language features
- The importance of writing clean, elegant code
- The evolution of coding techniques and tools
- The role of DevOps in improving delivery value
- The challenges of optimizing context switching for developers
- The advantages of using ephemeral Dev environments
- The value of feature flags and dead code removal
- The importance of maintainable, readable, and efficient code
- The impact of optimizing developer experience
- The benefits of focusing on flow and joy in development
- The influence of the Phoenix Project and The Unicorn Project books
- The challenges of implementing continuous delivery processes
- The potential risks and benefits of introducing new processes
- The importance of measuring and quantifying happiness in the development team and customer satisfaction
- The correlation between developer happiness and company performance
- The impact of AI tools like GitHub Copilot on developer happiness and productivity

Overall, Damian Brady emphasizes the importance of writing clean, elegant code and optimizing the developer experience to improve the overall delivery of software and the happiness of the development team and customers.


## How to Build a Quokkabot - Amy Kapernick - NDC London 2023

URL: [https://www.youtube.com/watch?v=4WbOt5IKiUU](https://www.youtube.com/watch?v=4WbOt5IKiUU)

 - Amy Kapernick, front-end developer from Perth, Australia, gave a talk at NDC London 2023 about building a QuokkaBot using different technologies and APIs.
- The QuokkaBot was initially built as a WhatsApp bot, using the Twilio Communications API for sending a picture of a quokka to the user.
- The bot was later improved with an image recognition feature using Azure Cognitive Services' Custom Vision API.
- The bot has evolved into a multi-functional tool, integrating WhatsApp, email (using SendGrid), and Twitter communication methods.
- Custom Vision API has been used for training a model to recognize quokkas and other animals in images.
- The bot has a database of 545 photos of quokkas and 286 photos of Quakers (non-quokkas) for training.
- The bot can send images of quokkas or other animals based on the user's input.
- The bot can also identify if an image is a quokka, with an accuracy of around 99%.
- The bot's source code is available on GitHub, and Amy encourages people to contribute and improve the project.
- Amy mentioned the challenges of building the bot, such as the heavily biased training dataset, and how the bot can sometimes be tricked by user inputs.
- The bot has received a shiny Quokka sticker as a reward for its performance.
- Amy recommends exploring Azure Booth for easy training of custom vision service models and using Azure functions for serverless computing.
- The bot's website, aquokers.amyscapersdev.com, showcases the bot's functionality and features.


## From Domain Boundaries to Software Architecture -  Maxime Sanglan-Charlier & Nick Tune

URL: [https://www.youtube.com/watch?v=rU-F8op6f-I](https://www.youtube.com/watch?v=rU-F8op6f-I)

 - Maxime Sanglan-Charlier and Nick Tune discuss the importance of Loosely Coupled Architecture and Domain-Driven Design (DDD) in improving software development efficiency and teamwork.
- They emphasize the role of Event Storming, a workshop technique that helps visualize the flow of events and interactions within a system, in identifying domain boundaries and refining software architecture.
- Event Storming helps teams understand the system's behavior, dependencies, and interactions, enabling them to make informed design decisions and identify potential subdomains.
- By identifying clear domain boundaries, teams can refine and optimize their work, reducing dependencies and improving communication.
- DDD and Event Storming techniques facilitate collaboration across different roles and expertise within a team, leading to better-designed systems and more efficient development processes.
- The presenters discuss their experiences and challenges in applying these techniques in real-world scenarios, highlighting the importance of openness, collaboration, and visualization in the process.


## You Keep Using That Word: Asynchronous And Interprocess Comms - Sam Newman - NDC London 2023

URL: [https://www.youtube.com/watch?v=6FOCNf06lqY](https://www.youtube.com/watch?v=6FOCNf06lqY)

 In this transcript, Sam Newman discusses the terms asynchronous and request/response in the context of interprocess communication and microservices architecture. Key points include:

- Asynchronous vs. Synchronous: Asynchronous communication does not require immediate response, allowing other tasks to run concurrently.
- Synchronous request/response: Each request/response pair is processed in a linear, sequential manner.
- Asynchronous request/response: Multiple requests can be processed in parallel, improving efficiency.
- Interprocess communication: The exchange of information between different processes, such as microservices.
- Event-driven communication: Processes communicate by emitting and consuming events.
- Request/response style: More traditional interaction model, where one service sends a request and waits for a response.
- Event-driven style: More loosely coupled, where services emit events without blocking.
- Eventual consistency: Achieving consistency over time, rather than instantaneously.
- Reactive Manifesto: A set of principles advocating for reactive, event-driven, and non-blocking systems.
- Context: The surrounding circumstances or facts that influence meaning or interpretation.

Sam Newman discusses the importance of understanding terminology, context, and the semantic meaning of words in the field of software development and asynchronous communication.


## Tales from the trenches; Building Denmark’s biggest EV charging network-Emil Okkels & Lars Skovslund

URL: [https://www.youtube.com/watch?v=kFA1SSxTkbI](https://www.youtube.com/watch?v=kFA1SSxTkbI)

 Key takeaways from the transcript:

1. Denmark's largest EV charging operator needed a scalable solution to handle rapid growth and new protocols due to the increasing demand for electric vehicles.
2. The team faced challenges such as integrating new technology, managing increasing numbers of charging points, and ensuring reliability and stability.
3. The existing system, based on Azure Cloud and Service Fabric, had limitations and required refactoring to accommodate new requirements.
4. The team had to make difficult decisions, such as choosing between web sockets and a new protocol, and dealing with compatibility issues between old and new systems.
5. The new system needed to handle the growing demand for electric vehicle charging while maintaining high performance and low latency.
6. The team implemented a new, scalable system with an emphasis on automation, high standards, and accountability.
7. Key components of the new system included event-driven architecture, message bus, and a focus on monitoring, observability, and data storage for analytics.
8. The new system utilized Azure Cosmos DB for fast data ingestion and querying across partitions, providing valuable insights into the charging network's performance.
9. The team adopted a "society of developers" approach, with full autonomy and trust placed in each developer, fostering a culture of ownership and collaboration.
10. The project's success was attributed to the team's ability to learn from mistakes, adapt, and continuously improve while maintaining a positive mindset and embracing new technologies.


## Extend Your Kubernetes With the Power of Open Source - Annie Talvasto - Copenhagen DevFest 2023

URL: [https://www.youtube.com/watch?v=PWWutTE0R0g](https://www.youtube.com/watch?v=PWWutTE0R0g)

 - Extended Kubernetes with Open Source: Annie Talvasto discussed the power of open source in the Kubernetes ecosystem.
- She highlighted the importance of community engagement and collaboration, and mentioned key open source projects such as Helm, Prometheus, Linkerd, MySQL, gRPC, and Kada.
- Annie emphasized the value of open source tooling in simplifying deployment, scaling, and managing complex workloads.
- She also covered open source projects in various stages, from new and emerging (like Kada and Cubeflow) to mature and graduated projects (like Helm and Prometheus).
- Annie encouraged attendees to explore and engage with open source projects, as they can provide significant benefits, customizability, and innovation in the Kubernetes environment.

Key takeaways:
- Open source projects play a crucial role in the Kubernetes ecosystem, offering innovation, customizability, and community engagement.
- Mature projects like Helm and Prometheus are well-established, while newer projects like Kada and Cubeflow are emerging and offer potential for growth.
- Engaging with open source projects can lead to better tooling, simplified workflows, and improved resource management in Kubernetes environments.


## “Wouldn’t it be cool…” and other bad design approaches - Billy Hollis - NDC London 2023

URL: [https://www.youtube.com/watch?v=GGUqyb6mzDw](https://www.youtube.com/watch?v=GGUqyb6mzDw)

 Here are the key points summarized from the transcript:

- Importance of understanding user needs and avoiding assumptions in design.
- The risks of focusing on "cool" design elements without considering functionality.
- The dangers of complacency and comfort zones in design, leading to stagnation.
- The value of getting user feedback and learning from mistakes.
- The need for empathy and understanding user perspectives in design decisions.
- The importance of questioning assumptions and constantly evaluating design choices.
- The potential drawbacks of designing for convenience and laziness.
- The importance of iterative design and learning from user interactions.
- The need for a balance between Aesthetics and functionality in design.
- The significance of considering the entire user experience, including workflow.
- The benefits of using design thinking and collaboration across organizations.
- The impact of investing in good design and the potential return on investment.
- The importance of focusing on user needs in design, especially when working with legacy applications.
- The value of design systems in maintaining consistency and efficiency in design.
- The importance of understanding both the visual and interaction aspects of design.
- The need to balance design aesthetics with the user's ability to accomplish their tasks.
- The significance of empathy and understanding user perspectives in design decisions.

These points highlight the importance of user-centered design, iterative improvement, and considering the broader context of user needs and experiences when creating products and applications.


## What Anime Taught Me About K8s & Tech Careers - Annie Talvasto - Copenhagen DevFest 2023

URL: [https://www.youtube.com/watch?v=-33iJxlvYOY](https://www.youtube.com/watch?v=-33iJxlvYOY)

 - The speaker discusses the similarities between anime and Kubernetes, sharing 10 life lessons learned from anime that apply to tech careers.
- They highlight the importance of teamwork, self-development, and learning from hardships in both anime and tech careers.
- Some key points include:
  1. The importance of a strong team: In anime, characters become stronger together, and the same applies in tech careers.
  2. Overcoming hardships and learning from them: Characters in anime often have tragic pasts, and in the tech world, developers face challenges and must learn from them.
  3. The significance of relaxation and taking breaks: Anime often features beach episodes where characters relax, and taking breaks is essential in the tech world to maintain productivity.
  4. The importance of good monitoring and visibility: Characters in anime often need to know what's happening, and in tech, monitoring systems and alerts are crucial.
  5. The value of documentation: In anime, characters need to communicate effectively, and in tech, clear documentation is essential for team collaboration.
  6. The unifying aspect of complexity: Both anime and tech worlds involve dealing with complex situations and systems.
  7. The concept of ridiculous power scaling: In anime, characters can become incredibly powerful, and in tech, projects can grow and evolve rapidly.
  8. The importance of snacks and energy: In anime, characters often rely on snacks for energy, and in tech, maintaining energy levels is crucial for productivity.
  9. Believing in oneself and perseverance: Anime characters often face failures but believe in their abilities, and the same applies to tech careers.
  10. Embracing change and evolving: Characters in anime often transform, and in tech, embracing change and adapting is vital for success.

The speaker emphasizes that anime can provide valuable lessons and insights for those working in tech careers, and these parallels can help improve one's professional development.


## Celebrity Deathmatch: Akka.NET vs Orleans - Hannes Lowette - NDC London 2023

URL: [https://www.youtube.com/watch?v=g3zmvdD8E38](https://www.youtube.com/watch?v=g3zmvdD8E38)

 Summary:

- Hannes Lowette discussed the differences between AKA (Akka) and Orleans, two actor-based frameworks for building scalable distributed systems.
- Akka focuses on actor-based concurrency and fault tolerance, and is popular in the Java ecosystem.
- Orleans is designed for high-scale scenarios, with a focus on resilience, performance, and easy-to-understand code.
- Key differences include:
  - Akka uses a single-threaded, event-driven execution model with message passing between actors, while Orleans uses a multi-threaded model.
  - Orleans has a different approach to fault tolerance, using a silo concept and a resilience-based design.
  - Orleans has built-in support for persistence and state management, making it suitable for building stateful, concurrent systems.
- Both frameworks have their own strengths and weaknesses, and the choice between them depends on the specific use case and developer preference.
- Orleans is backed by Microsoft and has been promoted as a distributed programming model since the release of .NET 7, indicating continued investment and support.

Key takeaways:

- AKA and Orleans are both powerful actor-based frameworks for building scalable distributed systems.
- The choice between the two depends on the specific requirements of the project and the developer's familiarity with the framework.
- Both frameworks have evolved over time and continue to be supported by their respective communities.


## GitHub + Azure: Better Together! - April Edwards - Copenhagen DevFest 2023

URL: [https://www.youtube.com/watch?v=4quTYW4yoM4](https://www.youtube.com/watch?v=4quTYW4yoM4)

 - GitHub and Azure integration: The integration of GitHub and Azure allows for a unified and efficient development process, with GitHub as the platform for collaboration and Azure as the cloud platform for deployment.
- DevOps and GitHub Actions: GitHub Actions enable automation in the development cycle, from code review to deployment, by leveraging workflows and actions that are triggered by specific events.
- Code scanning and security: GitHub code scanning helps identify vulnerabilities in the code, and the integration with Azure Key Vault provides secure storage for secrets and credentials.
- Continuous Integration (CI) and Continuous Deployment (CD): CI/CD pipelines ensure that the code is tested and deployed in an automated and reliable manner, reducing the risk of human error and accelerating the development process.
- Infrastructure as Code (IaC): IaC allows the definition and provisioning of infrastructure using code, enabling consistency, repeatability, and version control.
- Environment protection: The use of environment tags and GitHub Actions workflows ensures that different environments (e.g., production, staging, testing) are protected and managed appropriately.
- GitHub Copilot: GitHub Copilot is an AI-powered code assistant that helps developers write code faster and more accurately by suggesting code snippets and completing lines of code.
- Secrets management: GitHub Secrets and Azure Key Vault help manage sensitive information such as credentials, keys, and tokens securely.
- Traceability and visibility: GitHub Actions workflows provide traceability and visibility into the development process, enabling developers to track changes, identify issues, and debug problems.
- GitHub Advanced Security: GitHub Advanced Security provides additional features such as secret scanning, code scanning, and dependency scanning to help identify and mitigate vulnerabilities in the codebase.
- Collaboration and communication: GitHub facilitates collaboration and communication among developers through features like issues, pull requests, code reviews, and comments.
- Learning and improvement: GitHub's resources, blog, and community help developers stay up-to-date with best practices, new features, and industry trends.

In summary, GitHub and Azure, along with GitHub Actions and GitHub Copilot, provide a comprehensive platform for developers to collaborate, develop, test, and deploy applications securely and efficiently. The platform encourages best practices and continuous improvement while offering a range of features to support modern development workflows.


## Where we're going... we don't need batch jobs - Adam Ralph - NDC London 2023

URL: [https://www.youtube.com/watch?v=9ix0vVL34-M](https://www.youtube.com/watch?v=9ix0vVL34-M)

 - Adam Ralph presents a video transcript discussing the challenges of implementing business logic in a system, using a discounting example to illustrate common problems.
- Key takeaways:
  - Traditional batch job approach leads to issues like race conditions and inconsistency.
  - Use of Domain-Driven Design (DDD) and object-oriented programming can help manage complexity.
  - Event-driven architecture with Sagas can address issues with concurrency and state consistency.
  - Sagas are a series of events or actions that occur in response to a business event, encapsulating business logic and state.
  - Using Sagas can help manage business logic in a consistent and predictable way.
  - Saga implementation may involve message handlers, state management, and timeouts.
  - Challenges include handling timeouts, state consistency, and ensuring business requirements are met.
  - Considerations for scaling Sagas include database design, message queuing, and transaction management.
  - Monthly invoicing could be another example of a process that may benefit from Saga implementation.

Adam Ralph suggests that developers should focus on understanding the business requirements and consider the use of Sagas or similar patterns to manage complex business logic in a consistent and predictable way.


## Message processing failed! But what's the root cause? - Laila Bougria - NDC London 2023

URL: [https://www.youtube.com/watch?v=zvqJmhJkPfE](https://www.youtube.com/watch?v=zvqJmhJkPfE)

 - Importance of observability in distributed systems
- Need for tracing, logging, and metrics in observability
- Overview of open Telemetry project: standard for observability
- Benefits of using open Telemetry: cross-platform, cross-runtime, and standardized
- Key concepts: Traces, spans, and activity sources
- Autoinstrumentation and tracing tools
- Role of open Telemetry in modern distributed systems
- Balancing performance optimization and observability
- Considerations for costs and benefits of observability tools
- Importance of feature flags and chaos engineering
- Recap: Observability is crucial in distributed systems and open Telemetry provides a standard for collecting and analyzing data across different components and languages.

Key takeaways:
- Observability is vital in understanding and troubleshooting distributed systems.
- Open Telemetry provides a standardized approach for collecting tracing, logging, and metrics data across different components and languages.
- Tools like open Telemetry, Honeycomb, and Azure Monitor offer value-added features to enhance observability.
- Balancing costs and benefits of observability tools is crucial for optimizing system performance and maintenance.


## Architecture Modernization: Aligning Software, Strategy, and Structure - Nick Tune

URL: [https://www.youtube.com/watch?v=Ls3XnV3BIyU](https://www.youtube.com/watch?v=Ls3XnV3BIyU)

 Key Points from Nick Tune's talk on Architecture Modernization:

- Architecture Modernization is crucial for successful companies to innovate and stay competitive in the fast-paced tech industry.
- Legacy systems can make change difficult and slow, but they also have the advantage of established brand loyalty and market share.
- Successful companies often need to modernize their systems to adapt to changing market conditions and keep up with industry trends.
- Modernization typically involves a shift from a monolithic architecture to a microservices-based architecture, enabling faster deployment and scalability.
- Tools such as event storming, architecture mapping, and modernization strategy selectors can help guide the process of modernizing systems.
- A Kickstarter workshop can be a useful way to get a company started on the modernization journey, bringing together stakeholders and enabling collaboration.
- The goal of modernization is to create a fast-flowing system that can adapt to changing business requirements and deliver value quickly.
- Modernization is not a one-time event but an ongoing process, and it requires a shift in mindset and organization culture.

Tips for successful modernization:
- Focus on both the technology (lift and shift) and business side (value stream, domain-driven design) of modernization.
- Start with small, manageable tasks and gradually build momentum.
- Use modernization tools and techniques to help visualize and understand the system architecture.
- Engage stakeholders and team members in the modernization process to ensure buy-in and support.
- Continuously assess the effort and value of modernization to ensure it remains a worthwhile investment.


## Variables of the Veracious Variety: How to Better Name your Variables - Adrienne Braganza Tacke

URL: [https://www.youtube.com/watch?v=nmphThdMxdM](https://www.youtube.com/watch?v=nmphThdMxdM)

 In this transcript, Adrienne Braganza Tekke discusses the importance of choosing meaningful, concise, and consistent variable names in programming. She emphasizes the following points:

- Variable naming is a critical aspect of programming, and choosing the right name can greatly impact code readability and maintainability.
- Good variable names should be:
  - Explicit
  - Consistent
  - Concise
- Variable names should avoid ambiguity and convey the exact purpose or concept they represent.
- Synonyms, homonyms, and homographs can lead to confusion and misinterpretation in variable naming.
- Keeping variable names short and specific helps prevent misunderstandings and reduces cognitive load.
- The use of meaningful and descriptive names reduces the need for lengthy comments in the code.
- Variable names should be self-explanatory to the point that they hardly require additional explanation.
- The choice of variable names can greatly affect the success of code reviews and collaboration.
- Names should be chosen carefully, considering the context and potential implications of their meaning.
- Consistency in naming conventions across a project or team is crucial for better understanding and collaboration.

In summary, choosing appropriate variable names is essential for code readability, maintainability, and collaboration. Explicit, consistent, and concise names help reduce cognitive load and improve the overall quality of the code.


## Modelling vs Reality - Einar Høst - Copenhagen DevFest 2023

URL: [https://www.youtube.com/watch?v=qpoRnBZlh7Y](https://www.youtube.com/watch?v=qpoRnBZlh7Y)

 In this talk, the speaker explores the concept of categorization and its role in shaping our perception of reality. The speaker discusses various examples, such as categorizing apes and animals, classifying everyday objects, and the challenges of categorizing complex concepts like humor, human emotions, and social behavior. The speaker also touches on the importance of language and naming in establishing categories, the role of taxonomy in organizing knowledge, and the challenges of defining categories in ambiguous contexts. They also discuss the implications of categorization in decision-making, problem-solving, and sense-making processes. The speaker highlights the importance of recognizing the limitations of categorization and the potential for miscommunication and misunderstanding when categorizing complex realities. They also suggest that modeling and technification can help refine categories and make them more precise and useful.


## Automating the component maintenance - Fleet management at Spotify - Niklas Gustavsson

URL: [https://www.youtube.com/watch?v=PFkvKSXkW3Y](https://www.youtube.com/watch?v=PFkvKSXkW3Y)

 Key takeaways from Niklas Gustavsson's presentation at Spotify:

- Fleet Management:
  - Aiming to automate component maintenance, including software maintenance and component operations.
  - Aiming to automate software ecosystem maintenance.
  - Fleet Management helps maintain security patches, component operations, and applying changes.
  - Fleet Management is a balance between automation and manual intervention.
  - Fleet Management is a culture of ownership and accountability.

- Microservices Architecture:
  - Spotify has around 4000 backend services.
  - The architecture has evolved over time and is based on small, independent components.
  - Components are owned by single teams, called squads, with strong ownership and responsibility.
  - Fleet Management allows for safe, controlled, and orchestrated rollouts.

- Fleet Management Challenges:
  - Maintaining ownership and responsibility in a large-scale, complex system.
  - Balancing automation and manual intervention for safe, controlled changes.
  - Dealing with breaking changes and dependency graph complexity.
  - Managing multiple, rapidly evolving components.
  - Instrumentation and observability for understanding dependencies within the ecosystem.

- Best Practices:
  - Use Git to track changes and coordinate controlled rollouts.
  - Use CI/CD pipelines and automated testing for component validation.
  - Use tooling to enforce conventions and standards.
  - Encourage strong ownership and responsibility for components and changes.
  - Continuously improve testing, monitoring, and observability.

- Fleet Management Success Stories:
  - Spotify's adoption of Fleet Management reduced the time and effort required for component maintenance and scaling.
  - The shift to Fleet Management allowed Spotify to handle large-scale, complex changes with increased safety and confidence.

In summary, Fleet Management at Spotify has been successful in automating component maintenance and improving the software ecosystem. It emphasizes ownership, accountability, and a balance between automation and manual intervention. The architecture is built on microservices, with small, independent components owned by single teams. The presentation highlighted the challenges and best practices for managing this complex system, as well as some success stories from Spotify's adoption of Fleet Management.


## Carbon-Aware Computing: Measuring and Reducing the Carbon Intensity of Software - Anders Lybecker

URL: [https://www.youtube.com/watch?v=mPqgXPqToak](https://www.youtube.com/watch?v=mPqgXPqToak)

 - Speaker's childhood memory of building snow huts and connection to environment
- Measurement of carbon intensity and focus on reducing carbon emissions
- Vestas V236 turbine: largest offshore wind turbine with a diameter of 236 meters and a capacity of 50 ton material saved
- Carbon awareness in hardware efficiency and energy-efficient software performance
- Carbon Software Foundation and carbon intensity specification for measuring carbon emitted per kilowatt-hour
- Challenges in optimizing energy usage and reducing carbon emissions in data centers
- Time shifting and location shifting to reduce carbon intensity in computing
- Carbon Decay, an open-source tool from Green Software Foundation for measuring carbon emission
- Emphasis on reducing carbon emissions in software development and infrastructure
- Creating a sustainable company through reduced energy consumption and carbon emissions

Overall, the speaker discussed their journey in reducing carbon emissions in software development and infrastructure, focusing on hardware efficiency, energy-efficient software performance, and utilizing tools like Carbon Decay from Green Software Foundation. They also emphasized the importance of working together as an industry to reduce the carbon footprint of technology.


## Build it fast, make it quick - Cory Gideon - NDC London 2023

URL: [https://www.youtube.com/watch?v=3F3o6RbDfzs](https://www.youtube.com/watch?v=3F3o6RbDfzs)

 - Corey Gideon emphasized the importance of understanding the problem and focusing on what truly matters in software development.
- Premature optimization often leads to wasted resources and can hinder the development of a product.
- Developers should focus on delivering a product that meets the needs of the end-user and business requirements.
- It's essential to avoid getting caught up in optimizing for performance or scalability prematurely.
- Understanding the cost and impact of optimization decisions is crucial for making informed choices.
- Developers should prioritize learning and improving their skills, rather than focusing on optimization.
- Balancing feature development with technical debt management is key to creating a successful product.
- Adopting a growth mindset and being open to learning from mistakes is essential for continuous improvement.
- The speaker highlighted the importance of considering the bigger picture and not getting lost in the details.
- Identifying the right balance between delivering a product and optimizing for performance or scalability is critical for success.

In summary, Corey Gideon's talk emphasized the importance of focusing on delivering value to the end-user and business requirements, avoiding premature optimization, managing technical debt, and striking a balance between feature development and optimization.


## Discussing Backend For Front-end - Nicolas Fränkel - NDC London 2023

URL: [https://www.youtube.com/watch?v=oKLVzK_pqgg](https://www.youtube.com/watch?v=oKLVzK_pqgg)

 In this talk, Nicolas Fränkel discussed various aspects of backend for frontend development, focusing on microservices and their impact on organizational structures. Here are the key points from the transcript:

- Nicolas Fränkel has been a developer for two decades, working primarily with Java and Spring. He's also an advocate for Kotlin and Flask applications.
- Fränkel believes that microservices are often over-hyped, and that the industry has not yet solved the problem of how to manage and organize them effectively.
- He sees microservices as a symptom of organizational problems, such as the tendency for organizations to focus on short-term goals rather than long-term sustainability.
- Fränkel suggests that developers should focus on creating modular, reusable components and using APIs to facilitate communication between them.
- He also recommends using API gateways to handle routing, authentication, and other cross-cutting concerns, which can simplify the architecture and reduce the complexity of the backend.
- Fränkel believes that developers should be aware of the limitations and trade-offs of their chosen technologies, and should strive to create solutions that are maintainable and adaptable to changing requirements.
- The talk also touched on the importance of collaboration between frontend and backend developers, and the role of tools like GraphQL in facilitating communication between different teams within an organization.
- Fränkel emphasized the need for a balanced approach to problem-solving, taking into account both technical and organizational aspects, and the importance of continuous learning and adaptation in the rapidly-changing technology landscape.


## Top 5 techniques for building the worst microservice system ever - William Brander - NDC London 2023

URL: [https://www.youtube.com/watch?v=88_LUw1Wwe4](https://www.youtube.com/watch?v=88_LUw1Wwe4)

 In his talk at NDC London, William Brander discussed the top five techniques to build the worst microservice system ever. These techniques aimed at intentionally making the system as bad as possible:

1. **Creating a Monolith**: Brander suggested starting with a monolithic system and slowly transitioning to microservices using a Strangler Fig pattern. This technique is based on incrementally isolating functionality and moving it to separate services.

2. **Introducing Network Hops**: Introducing network hops and distributed systems would likely increase latency and complexity, as communication between services can become expensive and time-consuming.

3. **Service-Oriented Architecture (SOA)**: Brander criticized SOA as a way to make systems worse, since it often leads to increased coupling and communication overhead between services.

4. **Distributed Monitoring**: Brander argued that distributed monitoring systems could potentially make the overall throughput of the system worse. In a distributed system, network operations and resource contention could lead to performance degradation.

5. **Service Boundary Definition**: Defining service boundaries based on nouns rather than verbs, and ensuring maximum chance for cross-service communication can lead to a more interconnected system, which may be harder to maintain and manage.

Overall, Brander's talk highlighted the challenges and pitfalls of building distributed microservice systems, and the potential for these systems to become complex, hard to maintain, and perform poorly if not designed carefully.


## From Domain Boundaries to Software Architecture - Maxime Sanglan-Charlier

URL: [https://www.youtube.com/watch?v=Yo6N66lud50](https://www.youtube.com/watch?v=Yo6N66lud50)

 - The speakers discuss the process of domain-driven design (DDD), software architecture, and using event storming and bounded context canvas as tools to identify and refine domain boundaries.
- Event storming helps visualize the flow of events in a system and is useful for identifying potential domain boundaries.
- Bounded context canvas is a tool for representing the boundaries and interactions between different domains within a system.
- DDD helps align software with business requirements, reduce coupling between different domains, and improve team communication and collaboration.
- When refining domain boundaries, it's important to consider the strength of coupling and cohesion, as well as the potential impact of changes in the domain.
- DDD techniques can be used in both monolithic and microservices architectures.
- Event storming, bounded context canvas, and DDD are iterative processes that require continuous refinement and collaboration between team members.
- The speakers emphasize the importance of learning from mistakes and experimenting with different techniques to find the best approach for a given situation.
- DDD techniques can be applied in distributed systems and help in shaping the architecture and organization of a team.


## Intentional Code - Minimalism in a World of Dogmatic Design - David Whitney - NDC London 2023

URL: [https://www.youtube.com/watch?v=vw2XffPmlYo](https://www.youtube.com/watch?v=vw2XffPmlYo)

 - Software design should be focused on clear communication and intentionality.
- Good design requires an understanding of the larger context and history of software design patterns.
- Design patterns should be used with respect for their origins and purpose, rather than simply following a trend.
- Software design can be influenced by literary techniques, such as using form and flow to communicate intent.
- Design patterns should evolve with the changing needs of the software industry.
- Good design requires a balance between abstracting complexity and providing clarity.
- Abstraction should be used thoughtfully to avoid adding unnecessary complexity.
- Design decisions should be made with an understanding of the trade-offs involved.
- The design process should be iterative, learning from the software and its users over time.
- The goal of good design is to create software that is easy to understand, maintain, and modify.
- Good design should be timeless and adaptable to new contexts and requirements.
- Designers should be aware of the impact of their design choices on the long-term maintainability of the software.
- Design should focus on creating a cohesive and organized code base, with clear communication of intent.
- Good design should be able to adapt to changing requirements and maintain its clarity and effectiveness over time.

The speaker emphasizes the importance of understanding the context and history of design patterns, as well as the need for an iterative and thoughtful design process. Good design should be adaptable, clear, and maintainable, with a focus on intentionality and communication.


## One Serverless Principle to Rule Them All: Idempotency - Adrienne Braganza Tacke - NDC London 2023

URL: [https://www.youtube.com/watch?v=96rkexbc2Ag](https://www.youtube.com/watch?v=96rkexbc2Ag)

 Key points from the transcript:

1. Identity is crucial in serverless architecture and applications.
2. Idempotency is an important principle in designing serverless systems.
3. Idempotency ensures that an operation's outcome remains the same even if it's performed multiple times.
4. The term "idempotent" comes from the fields of mathematics and computing.
5. In the context of serverless applications, idempotent operations help prevent side effects and duplicate requests.
6. To implement idempotency, use libraries and APIs that support identity checks.
7. Examples of such libraries and APIs include Stripe API, Nugget package, and Cisco APIs.
8. Idempotency can be achieved through persistent storage, such as DynamoDB, and using decorators to manage request identifiers.
9. Durable functions (e.g., Azure Orchestrator) can help manage retries and ensure idempotency in asynchronous transactions.
10. Identity maps, such as the one provided by Stripe API, can help manage and store keys and results, making it easier to scale applications.
11. The three key elements of implementing idempotency in serverless systems include identity, persistence, and retry mechanisms.


## Azure Static Web Apps with Blazor and .NET - Melissa Houghton - NDC London 2023

URL: [https://www.youtube.com/watch?v=REKbn6Bc36M](https://www.youtube.com/watch?v=REKbn6Bc36M)

 In this talk, Melissa Houghton discussed Azure Static Web Apps, focusing on the following key points:

- Azure Static Web Apps is a new service that helps deploy static web apps quickly and easily.
- It integrates with Blazor WebAssembly and .NET for building full-stack serverless applications.
- Key features include:
  - Globally distributed hosting with fast load times.
  - Integration with Azure CDN, Azure Front Door, and custom domains.
  - Built-in authentication using Azure Active Directory, GitHub, and Twitter providers.
  - Support for advanced features like static content, APIs, and serverless functions.
  - Flexible pricing options with a free plan and a standard plan with additional features.
- The talk demonstrated a live demo of building a weather forecast app using Blazor WebAssembly, Entity Framework, and Azure functions.
- The speaker also discussed debugging and local development using Visual Studio and CLI tools.
- Additional features discussed included diagnostics, error handling, and custom provider support.
- Azure Static Web Apps is designed to help developers build powerful, full-stack serverless applications quickly and efficiently.


## Keynote: Why are you making a new platform? - Christin Gorman - Copenhagen DevFest 2023

URL: [https://www.youtube.com/watch?v=-18bBu7lxP0](https://www.youtube.com/watch?v=-18bBu7lxP0)

 - Christine Gorman, a developer with 20 years of experience, discusses the need for a good platform
- Importance of focusing on delivering value, not just making a platform
- Platforms often fail because they don't address user needs, focusing instead on features and technology
- Platforms should be simple, adaptable, and solve real problems
- The importance of user-centered design and understanding user needs
- Platforms should be about enabling users to do their jobs better, not just delivering features
- Gorman discusses multiple failed examples of platforms that tried to do too much and failed to address user needs
- The challenge of making platforms that cater to many different user groups with various needs
- The importance of focusing on usability and user experience when developing a platform
- Gorman suggests that platforms should be like trains, efficiently transporting people where they want to go
- Platforms should have a clear purpose, like a compass pointing to a specific goal
- Gorman mentions that she doesn't believe in the concept of "platform-as-a-service" (PaaS), as she thinks it often leads to unnecessary complexity
- Gorman emphasizes the need for a platform that can adapt and evolve over time, rather than one that tries to do everything at once
- She highlights the importance of collaboration between developers, IT departments, and other stakeholders
- Gorman advocates for simplicity and adaptability in platform design, and for focusing on delivering value to users
- She criticizes the culture of constantly chasing new technologies and trends, suggesting that focus should be on delivering actual value to users
- Gorman emphasizes the need for understanding the whole picture, rather than focusing on specifics or details
- She suggests that platforms should be more like tools or components that can be assembled and reused, rather than monolithic solutions
- Gorman ends by discussing the importance of considering user needs, understanding their pain points, and designing platforms that genuinely help them in their work

Overall, Christine Gorman emphasizes the importance of focusing on delivering value through platforms, understanding user needs, and prioritizing simplicity and adaptability in platform design.


## Exciting new features in SQL Server for developers - Hasan Savran - NDC London 2023

URL: [https://www.youtube.com/watch?v=pFwb4gqC5CQ](https://www.youtube.com/watch?v=pFwb4gqC5CQ)

 - Exciting new features in SQL Server:
  - JSON functions and data types
  - Memory optimized tables
  - Graph database tables
  - JSON query functions
  - Memory table and Azure SQL integration
- Hassan Savran's session highlights:
  - Introduction of the JSON data type in SQL Server 2017
  - New JSON functions in 2019, like JSON_VALUE, JSON_QUERY, JSON_MODIFIED
  - SQL Server's JSON query functions enable complex JSON document manipulation
  - JSON value function allows querying JSON documents and returning default values
  - JSON functions are useful for creating and manipulating JSON data
  - Memory optimized tables in SQL Server 2017 provide faster performance
  - Graph database tables in SQL Server 2019 allow creating and managing graph data structures
  - Graph database tables integrate with relational data, providing flexibility
  - Hash indexes can be created for memory tables for faster querying
  - Native stored procedures can be used to create redis-like environments in SQL Server
  - The use of memory tables and stored procedures can significantly reduce storage requirements and improve performance
  - Querying JSON data is possible using the JSON query functions, which return JSON documents and JSON objects
  - The JSON value function returns the default value for a JSON document
  - The JSON_MODIFIED function can be used to detect changes in a JSON document
  - Using shortest path functions can help find the most efficient route between entities in a graph database
  - The CCI column store index can be used to efficiently analyze large JSON documents
  - The use of memory tables and stored procedures can significantly improve query performance and reduce storage requirements
  - The JSON functions and features in SQL Server enable better handling of JSON data and provide new possibilities for data manipulation and analysis.


## Don’t Build a Distributed Monolith - Jonathan "J." Tower - NDC London 2023

URL: [https://www.youtube.com/watch?v=p2GlRToY5HI](https://www.youtube.com/watch?v=p2GlRToY5HI)

 Summary:

- Monolithic architecture vs Microservices architecture
- Modular monolith: modular architecture within monolithic application
- Ball mud monolith: poorly designed monolithic application
- Microservices: scalable, maintainable, loosely coupled services
- Microservices architecture: building microservices with inter-service communication
- Distributed monolith: monolithic architecture with distributed architecture
- Distributed monolith vs distributed microservices
- Pitfalls:
  - Assuming microservices are always better
  - Shared data and data model issues
  - Domain-driven design challenges
  - Big monolith vs. microservices decision
  - Unencapsulated services
  - Mismatched team issues
  - Distributed monolith pitfalls
- Tips and best practices for avoiding pitfalls
- Transitioning from monolith to microservices
- Books and resources for learning microservices
- Importance of considering team structure and domain expertise

Key Takeaways:
- Understand the differences between monolithic architecture, microservices, and distributed monoliths
- Be aware of common pitfalls when transitioning to microservices
- Consider team structure and domain expertise when building microservices
- Know when to use monoliths or microservices based on the project requirements
- Educate yourself on best practices and resources to make the transition smoother.


## Navigating the Cloud Native Security Landscape - Pablo Musa - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=oNcJ8HnVzl0](https://www.youtube.com/watch?v=oNcJ8HnVzl0)

 Here's a concise summary of the provided transcript:

- Cloud Native Security Landscape:
  - Focus on software supply chain attack vectors
  - Supply chain attacks have become a common attack method
  - Cloud security misconfiguration is a major issue
  - Cloud security posture management (CSPM) is crucial for identifying misconfigurations
  - Identity and access management (IAM) is important in Cloud security
  - Container images and registries are common attack vectors
  - Runtime security is essential in Cloud environments
  - Cloud workload protection platform (CWPP) is necessary for vulnerability management
  - Cloud detection and response (CDR) is a last line of defense
  - Minimal images and trusted sources are key for reducing attack surface
  - Container images should be signed and scanned for vulnerabilities
  - Continuous integration and continuous delivery (CI/CD) pipelines need to be secure
  - Cloud inventory and compliance management are vital for security
  - Cloud security is becoming more important due to the shift to Cloud and containerization

Key takeaways:

- Focus on securing the software supply chain
- Implement strong identity and access management
- Regularly scan container images for vulnerabilities
- Use Cloud workload protection platforms
- Emphasize runtime security
- Adopt a secure CI/CD pipeline
- Minimize attack surface by using minimal images and trusted sources
- Practice the principle of least privilege
- Stay updated on Cloud security best practices and emerging threats


## AWS Security Reference Architecture: Visualize your security - Mohamed Wali - NDC Security 2024

URL: [https://www.youtube.com/watch?v=jGxS-7s-0sE](https://www.youtube.com/watch?v=jGxS-7s-0sE)

 - AWS Security Reference Architecture (SRA): A comprehensive guide for designing secure AWS environments based on AWS best practices
- AWS SRA provides a foundational blueprint for deploying different AWS Security Services across multiple accounts
- Key components include:
  - AWS Identity and Access Management (IAM)
  - AWS Config
  - Amazon GuardDuty
  - AWS Security Hub
  - AWS Inspector
  - AWS Firewall Manager
  - Amazon VPC
  - AWS Shield
  - AWS Key Management Service (KMS)
  - AWS Directory Service
  - AWS Resource Access Manager (RAM)
- SRA helps with:
  - Designing a secure AWS environment
  - Deploying and managing different AWS Security Services
  - Implementing AWS security best practices
- SRA uses a holistic approach to cover security needs
- AWS SRA is useful for organizations of various sizes
- SRA is updated regularly with new features and best practices
- AWS Code Repository: Contains code examples for different Solutions to deploy, configure, and manage different AWS Security Services
- AWS SRA can help prevent security breaches, ensure data protection, and maintain compliance
- SRA is a part of AWS Well Architected Framework
- SRA supports the five pillars of the Well Architected Framework: Operational Excellence, Security, Reliability, Performance Efficiency, and Cost Optimization
- AWS SRA can be used for both new and existing environments
- SRA can be customized to meet specific organizational requirements
- AWS SRA provides guidance on how to set up and configure AWS Security Services
- AWS SRA can help you create a secure AWS environment and maintain compliance with regulatory requirements
- AWS SRA can help you build a secure multi-account environment using AWS Organizations
- AWS SRA provides guidance on how to design and implement security features for AWS Security Services
- AWS SRA helps you understand how different AWS Security Services work together
- AWS SRA can help you identify security responsibilities across your organization
- AWS SRA can help you create and implement security policies for your organization
- AWS SRA helps you understand the relationship between AWS Security Services
- AWS SRA helps you understand how AWS Security Services align with AWS best practices
- AWS SRA provides guidance on how to use AWS Security Services to help detect, analyze, and respond to security threats
- AWS SRA provides guidance on how to use AWS Security Services to help automate security processes
- AWS SRA provides guidance on how to use AWS Security Services to help protect sensitive data and applications
- AWS SRA provides guidance on how to use AWS Security Services to help manage and monitor access to AWS resources
- AWS SRA provides guidance on how to use AWS Security Services to help ensure AWS infrastructure is secure
- AWS SRA provides guidance on how to use AWS Security Services to help enforce organizational policies and requirements
- AWS SRA provides guidance on how to use AWS Security Services to help maintain compliance with industry standards and regulations
- AWS SRA provides guidance on how to use AWS Security Services to help manage and monitor security findings
- AWS SRA provides guidance on how to use AWS Security Services to help automate security processes and responses
- AWS SRA provides guidance on how to use AWS Security Services to help protect sensitive data and applications
- AWS SRA provides guidance on how to use AWS Security Services to help manage and monitor access to AWS resources
- AWS SRA provides guidance on how to use AWS Security Services to help ensure AWS infrastructure is secure
- AWS SRA provides guidance on how to use AWS Security Services to help enforce organizational policies and requirements
- AWS SRA provides guidance on how to use AWS Security Services to help maintain compliance with industry standards and regulations
- AWS SRA provides guidance on how to use AWS Security Services to help manage and monitor security findings
- AWS SRA provides guidance on how to use AWS Security Services to help automate security processes and responses
- AWS SRA provides guidance on how to use AWS Security Services to help protect sensitive data and applications
- AWS SRA provides guidance on how to use AWS Security Services to help manage and monitor access to AWS resources
- AWS SRA provides guidance on how to use AWS Security Services to help ensure AWS infrastructure is secure
- AWS SRA provides guidance on how to use AWS Security Services to help enforce organizational policies and requirements
- AWS SRA provides guidance on how to use AWS Security Services to help maintain compliance with industry standards and regulations
- AWS SRA provides guidance on how to use AWS Security Services to help manage and monitor security findings
- AWS SRA provides guidance on how to use AWS Security Services to help automate security processes and responses
- AWS SRA provides guidance on how to use AWS Security Services to help protect sensitive data and applications
- AWS SRA provides guidance on how to use AWS Security Services to help manage and monitor access to AWS resources
- AWS SRA provides guidance on how to use AWS Security Services to help ensure AWS infrastructure is secure
- AWS SRA provides guidance on how to use AWS Security Services to help enforce organizational policies and requirements
- AWS SRA provides guidance on how to use AWS Security Services to help maintain compliance with industry standards and regulations
- AWS SRA provides guidance on how to use AWS Security Services to help manage and monitor security findings
- AWS SRA provides guidance on how to use AWS Security Services to help automate security processes and responses
- AWS SRA provides guidance on how to use AWS Security Services to help protect sensitive data and applications
- AWS SRA provides guidance on how to use AWS Security Services to help manage and monitor access to AWS resources
- AWS SRA provides guidance on how to use AWS Security Services to help ensure AWS infrastructure is secure
- AWS SRA provides guidance on how to use AWS Security Services to help enforce organizational policies and requirements
- AWS SRA provides guidance on how to use AWS Security Services to help maintain compliance with industry standards and regulations
- AWS SRA provides guidance on how to use AWS Security Services to help manage and monitor security findings
- AWS SRA provides guidance on how to use AWS Security Services to help automate security processes and responses
- AWS SRA provides guidance on how to use AWS Security Services to help protect sensitive data and applications
- AWS SRA provides guidance on how to use AWS Security Services to help manage and monitor access to AWS resources
- AWS SRA provides guidance on how to use AWS Security Services to help ensure AWS infrastructure is secure
- AWS SRA provides guidance on how to use AWS Security Services to help enforce organizational policies and requirements
- AWS SRA provides guidance on how to use AWS Security Services to help maintain compliance with industry standards and regulations
- AWS SRA provides guidance on how to use AWS Security Services to help manage and monitor security findings
- AWS SRA provides guidance on how to use AWS Security Services to help automate security processes and responses
- AWS SRA provides guidance on how to use AWS Security Services to help protect sensitive data and applications
- AWS SRA provides guidance on how to use AWS Security Services to help manage and monitor access to AWS resources
- AWS SRA provides guidance on how to use AWS Security Services to help ensure AWS infrastructure is secure
- AWS SRA provides guidance on how to use AWS Security Services to help enforce organizational policies and requirements

In summary, AWS Security Reference Architecture (SRA) is a comprehensive guide for designing secure AWS environments based on AWS best practices. It provides a foundational blueprint for deploying different AWS Security Services across multiple accounts, and covers the five pillars of the Well Architected Framework. SRA helps organizations build a secure multi-account environment using AWS Organizations, configure and implement security features for AWS Security Services, and understand how AWS Security Services work together.


## Tools and practices to help you deal with legacy code - Dennis Doomen - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=K8mWGiaEbq8](https://www.youtube.com/watch?v=K8mWGiaEbq8)

 Here is a condensed summary of the transcript:

- The speaker, a consultant, shares insights from working on a legacy codebase.
- They highlight the importance of understanding the environment, tools, practices, and principles to improve the codebase.
- Key points include:
  - Tools and practices:
    - Version control
    - Code review
    - Code analysis
    - Testing (unit, integration, and end-to-end)
    - Deployment pipeline
    - Refactoring
    - Dependency management
  - Techniques:
    - Source code exploration
    - Log file analysis
    - Codebase understanding
    - Identifying dependencies and decoupling
  - Architecture styles:
    - Onion architecture
    - Clean architecture
    - Event sourcing
  - Principles:
    - Single Responsibility Principle (SRP)
    - Open/Closed Principle
    - Liskov Substitution Principle
    - Interface Segregation Principle
    - Dependency Inversion Principle
    - SOLID principles
  - Strategies:
    - Gradual refactoring
    - Introducing tests
    - Simplifying and breaking down complex code
  - Challenges and considerations:
    - Balancing between improving code and delivering new features
    - Reducing technical debt
    - Understanding and dealing with legacy architecture
    - Maintaining a safe environment (safety net)
    - Working with a team and sharing knowledge

The speaker emphasizes the importance of understanding the codebase, tools, practices, principles, and architecture styles to improve a legacy codebase. They suggest gradual refactoring and introducing tests as key strategies for improvement.


## Apache Kafka in 1 hour for C# Developers - Gui Ferreira - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=H_dDTMz_x4E](https://www.youtube.com/watch?v=H_dDTMz_x4E)

 In his talk, Gui Ferreira discusses the following key points about Apache Kafka and its use in modern applications:

- Apache Kafka is a powerful and scalable open-source streaming platform that is widely used for building real-time data pipelines and streaming applications.
- Kafka is well-suited for handling high-throughput, low-latency, and fault-tolerant applications.
- Kafka's design principles focus on reliability, scalability, and strong durability.
- Kafka topics are distributed across multiple brokers, providing fault tolerance and high availability.
- Partitioning in Kafka allows for horizontal scalability and efficient load distribution.
- Kafka's consumer groups enable processing of messages across multiple instances, providing fault tolerance and dynamic scalability.
- Kafka connectors, Kafka Streams, and Kafka clients are powerful tools for building streaming applications and integrating with various data sources and sinks.
- Kafka's flexibility makes it a suitable choice for evolving architectures and handling complex data processing scenarios.
- Kafka's compact strategy enables efficient storage and retrieval of message history, which is useful for maintaining the state of applications and handling event sourcing patterns.
- Kafka's message keys play a crucial role in maintaining message order, ensuring that related messages are processed together.
- Kafka's topic partitions can be assigned different retention policies, allowing for fine-grained control over data retention.
- Kafka's schema registry, Kafka streams, and Kafka connect are essential components of the Kafka ecosystem, providing additional functionality and integration options.
- Kafka is used in various industries, including e-commerce, IoT, and real-time analytics, due to its robustness and scalability.

Overall, Gui Ferreira's talk provides a comprehensive overview of Apache Kafka, its principles, features, and use cases, emphasizing its role in modern, high-performance, and fault-tolerant applications.


## Just in Time Architecture - Macklin Hartley - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=REbnvajTlko](https://www.youtube.com/watch?v=REbnvajTlko)

 Summary of Macklin Hartley's talk:

- Initially worked on a product for online gaming company with monolithic architecture.
- The team faced issues with scalability and coordination overhead in monolithic architecture.
- Transitioned to microservices architecture to enable individual teams to work independently and deploy services separately.
- Faced new challenges with microservices: balance deductions without Avatar assignment, and handling multiple requests concurrently.
- Introduced event-driven architecture with event notification and event carried State transfer to help manage the workflow more efficiently.
- Explored transaction log transactional outbox pattern and event sourcing as potential solutions.
- Mentioned trade-offs with various architectural choices and the importance of choosing an architecture that fits the problem at hand.
- Emphasized the importance of aligning team goals towards meeting customer needs.

Key takeaways:
- Architects should find the best architecture that suits the problem at hand.
- Consider trade-offs with different architectural choices.
- Focus on meeting customer needs in the early stages of product development.


## C4 models as code - Simon Brown - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=4HEd1EEQLR0](https://www.youtube.com/watch?v=4HEd1EEQLR0)

 - Independent Consultant: Simon Brown, an independent consultant, specializes in software architecture.
- C4 Model: A model for visualizing software architecture using a hierarchical set of abstraction layers, known as C4 diagrams.
- Tooling: The challenge is to find good tooling to support the creation and maintenance of C4 diagrams.
- Structurizer: A tool designed to generate C4 diagrams, using a DSL (Domain Specific Language) for defining models in a structured way.
- Structurizer Light: A light version of Structurizer, designed for running directly in a web browser.
- Structurizer DSL: An open source project, a Java Library for parsing DSL files and creating in-memory representations of the models defined by those files.
- Structurizer Site Generator: Another open source tool to generate HTML micro sites from markdown documents, using information from C4 diagrams.
- Structurizer4Net: A Java library for generating C4 diagrams, using the DSL for defining models.
- Structurizer CLI: A CLI tool for exporting C4 diagrams in various formats, including SVG, PNG, PDF, etc.
- AWS version icon diagram key: Another way to combine tooling with cloud architecture diagrams.
- Mermaid: An open-source JavaScript library for creating diagrams and flowcharts from textual description, using a simple markdown-like syntax.
- adrs: Architecture Decision Records, a way of capturing architecture decisions and decisions rationale.
- Confluence: A software collaboration tool, used for documenting architecture diagrams, decisions, and documentation.
- Structuralize.com: A website providing Structurizer tooling, documentation, and examples.

Simon Brown discussed the challenges of tooling for creating and maintaining C4 diagrams. He introduced Structurizer, a tool that generates C4 diagrams using a Domain Specific Language (DSL) and Structurizer Light, a web-based, browser-friendly version of Structurizer. He also talked about Structurizer DSL, Structurizer4Net, Structurizer CLI, and mermaid as alternatives and tooling options. Additionally, he mentioned AWS version icon diagram key and Confluence for combining tooling with cloud architecture diagrams.


## Intentional Code - Minimalism in a World of Dogmatic Design - David Whitney  - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=KJxsro_V0kY](https://www.youtube.com/watch?v=KJxsro_V0kY)

 - David Whitney's talk on intentional code and minimalism in a world of dogmatic design.
- Key points:
  - Importance of code as literature and software as a communication medium.
  - Value of intentionality, syntax, and form in code.
  - Design patterns as potentially over-emphasized and sometimes outdated.
  - The importance of function and flow in code.
  - The role of cohesion and form in minimizing complexity.
  - The need for a balance between intent, form, and complexity in design.
  - The influence of design on cognitive load and maintainability.
  - The impact of design on the perception of software quality.
  - The importance of form and function in conveying intent and meaning in code.
  - The role of constraints in shaping design and reducing complexity.
  - The relationship between design and the context in which software is used.
  - The need for iterative design and continuous improvement in software development.

Overall, Whitney emphasizes the importance of intentionality, form, and function in software design, and suggests that many design patterns are over-emphasized and can contribute to increased complexity. He advocates for a more holistic and contextual approach to design, focusing on cohesion, flow, and cognitive load.


## High Optionality Programming: Architectural Choices That Mitigate Technical Debt - Aaron Stannard

URL: [https://www.youtube.com/watch?v=yV97QwC5gnE](https://www.youtube.com/watch?v=yV97QwC5gnE)

 Summary:
- High Optionality Architecture is essential for mitigating technical debt and anticipating changes in the future.
- The architecture includes:
  - Event-driven programming
  - Event sourcing and CQRS (Command Query Responsibility Segregation)
  - Actor model
  - Immutable data
  - Extend Design
- Benefits of High Optionality Architecture:
  - Flexibility to change
  - Maintain optionality by refactoring and redesigning software
  - Dynamic routing
  - Low risk deployment
- Key points from the transcript:
  - Anticipate changes in the business landscape
  - Plan for the future
  - Consider scalability and flexibility
  - Decouple different parts of the application
  - Use Extend Design to lower risk per deployment
  - Use CI/CD pipelines
  - Use tools like akkanet, Orleans, and Kafka
  - Use event-driven architecture with actors
  - Use event sourcing and CQRS for better read/write separation
  - Use immutable data for safety and reduced risk
  - Use a well-planned architecture to lower risk of technical debt accumulation

In the transcript, the speaker discusses High Optionality Architecture as a way to mitigate technical debt and prepare for future changes. The architecture includes event-driven programming, event sourcing and CQRS, actor model, immutability, and extend design. The benefits of High Optionality Architecture include flexibility, low risk deployment, and dynamic routing. The speaker emphasizes planning for the future, scalability, and flexibility. They also discuss the importance of decoupling different parts of the application, using CI/CD pipelines, and using tools like akkanet, Orleans, and Kafka. The speaker also highlights event-driven architecture with actors, event sourcing, CQRS, immutable data, and well-planned architecture as ways to lower the risk of technical debt accumulation.


## A Contrarian View of Software Architecture - Jeremy Miller - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=ttYQzHPe5s4](https://www.youtube.com/watch?v=ttYQzHPe5s4)

 - The speaker, Jeremy Miller, presents an alternative view on software architecture, focusing on problems in large, long-running systems.
- Key issues highlighted include:
  - Large, tightly coupled systems can become difficult to manage and maintain.
  - Database abstraction can lead to over-engineered, heavy solutions.
  - Prescriptive architectural patterns like Clean Architecture, Onion Architecture, and Hexagonal Architecture can be overly restrictive and sometimes unnecessary.
  - Developers should avoid excessive abstraction, which can add complexity and reduce clarity.
  - Using a microservices approach can help isolate problems, but might introduce its own challenges such as inter-service communication and coordination.
  - Infrastructure and business logic should be separated to promote maintainability and predictability.
  - Testing is crucial, but overemphasis on integration testing can sometimes lead to over-engineered solutions.
  - Adopting an Aframe architecture, which separates business logic from data handling and coordination, can help manage complexity in large systems.
- The speaker suggests that developers should:
  - Be careful when following prescriptive architectural patterns.
  - Focus on keeping code maintainable and understandable.
  - Use appropriate abstraction levels to solve real problems.
  - Continuously evaluate and adapt their approach based on the specific needs of the system they are working on.


## Practical Event Sourcing with Marten and .NET - Oskar Dudycz - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=jnDchr5eabI](https://www.youtube.com/watch?v=jnDchr5eabI)

 Here are the key points from the transcript:

- Event Sourcing: 
  - Focuses on capturing events as the primary source of information rather than the current state.
  - Events are immutable and contain specific information, such as logging incident details.
  - Event Sourcing allows for precise diagnostics, better understanding of the system, and easier debugging.

- Even Sourcing vs Classical Approach:
  - Even Sourcing simplifies the architecture by separating read and write operations.
  - It provides strong consistency and a history of the entire system.
  - Even Sourcing is more connected and relies on DDD principles.

- Benefits of Event Sourcing:
  - Simplified testing, as events are predictable and maintainable.
  - Powerful granular information for precise diagnostics.
  - Easier handling of incidents and better incident categorization.
  - Strong consistency and better handling of optimistic concurrency.

- Challenges:
  - Even Sourcing can be perceived as hard to implement.
  - Requires careful handling of data and version control.
  - Optimistic concurrency can be complex to manage.

- Event Sourcing Implementation:
  - Martin Fowler's approach involves using a postgres event store, which allows for a simple storage pattern.
  - The event store is a key-value database that stores events and their metadata.
  - Even Sourcing can be used with document databases, such as MongoDB, but it is essential to understand the database's characteristics.

- Projection:
  - Projections are read models that serve specific use cases and queries.
  - They can be built using various technologies and can be optimized for performance and readability.
  - Projections can help reduce cognitive load by providing a clear and concise view of specific data.

- Event Sourcing Patterns:
  - Event Sourcing can be combined with CQRS (Command Query Responsibility Segregation) to optimize read and write operations.
  - Even Sourcing can be used with messaging systems, such as Kafka or RabbitMQ, to handle events and stream data.
  - Even Sourcing can also be used with SignalR for real-time notifications and communication between the server and clients.

In summary, Event Sourcing is a powerful pattern that captures events as the primary source of information, enabling precise diagnostics, strong consistency, and simplified testing. It requires careful handling of data and version control but can be combined with various technologies and patterns, such as CQRS, messaging systems, and SignalR, to optimize the overall system's performance and maintainability.


## Balancing Coupling in Software Design - Vlad Khononov - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=eQOM-UrNTS4](https://www.youtube.com/watch?v=eQOM-UrNTS4)

 - Coupling is a crucial aspect of software design, affecting maintainability, flexibility, and changeability.
- Two types of coupling: structural and behavioral. Structural coupling refers to the interdependence of components through code, while behavioral coupling relates to how components interact with each other.
- Balancing coupling means designing systems with manageable interdependencies, minimizing the impact of changes in one part of the system on others.
- Designing modular systems involves understanding the nature of coupling, its dimensions, and how it affects system design.
- The three dimensions of coupling are:
  - Strength: The degree of interdependence between components.
  - Volatility: The rate of change of components.
  - Distance: The physical separation between components.
- High coupling strength increases the risk of cascading changes and high maintenance effort, while high volatility requires more communication and coordination.
- Decoupling components can reduce the risk of cascading changes, but extreme decoupling can lead to a loss of coherence and increased complexity.
- Design patterns like Domain-Driven Design (DDD) and Bounded Context can help manage coupling by defining clear boundaries and organizing components around business concepts.
- Using microservices and serverless functions can also help balance coupling by isolating parts of the system and reducing interdependencies.
- Anticorruption layers and event-sourcing can help manage interactions between bounded contexts and minimize the impact of changes in one context on another.


## Succeeding at Reactive Architecture - Ian Cooper - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=xvuPoN3cJXk](https://www.youtube.com/watch?v=xvuPoN3cJXk)

 In the talk, Ian Cooper discussed reactive architecture and various concepts related to it. Here are the key points summarized:

- Reactive Manifesto: Emphasizes the creation of a reactive system that is responsive, resilient, and elastic.
- Reactive programming: Aims to create better architecture by focusing on the flow of data and active models.
- Flow-based programming: Focuses on data flow, with nodes representing actions and transformations.
- Data flow programming: Emphasizes data flow instead of object-oriented design, leading to a more scalable and resilient system.
- Actor model: An alternative to data flow programming, where actors communicate through messages and asynchronous events.
- Flow-based programming vs. Actor model: Both models aim to create a responsive and elastic system by handling synchronous and asynchronous events.
- Load shedding: A technique to prevent system failures by limiting the load during high demand.
- Backpressure: A mechanism to prevent overloading a system by controlling the rate of incoming data.
- Event sourcing: A technique that uses events and a store to maintain the state of a system.
- Reactive trade-offs: Discussing the balance between consistency, availability, and partition tolerance.
- Crash pattern: A design pattern that handles failures gracefully and recovers from them.
- Reactive system model: A model that uses message-driven architecture, elasticity, resilience, and responsiveness to build systems.
- Reactive voltage system: A system that uses message passing, elasticity, resilience, and responsiveness.
- Message-driven architecture: A design pattern that uses messaging middleware to process and route messages between components.
- Circuit breaker pattern: A pattern to prevent system failures by limiting the number of retries when an error occurs.
- Observable systems: Systems that provide visibility and insights into the system's behavior, using logs, metrics, and traces.

In summary, the talk focused on reactive architecture, reactive programming concepts, and various models and patterns used to build scalable, resilient, and responsive systems.


## An Introduction to Residuality Theory - Barry O'Reilly - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=0wcUG2EV-7E](https://www.youtube.com/watch?v=0wcUG2EV-7E)

 - Introduction to Residuality Theory: A new way of thinking about software systems, focusing on understanding the interaction between ordered and disordered systems.
- Traditional software architecture has treated disordered systems like ordered systems, which often leads to failure.
- The key is to identify and manage residues, which are the leftover components in a system that are still functioning and connected.
- Residuality Theory offers a way to understand complex systems and their behavior by identifying residues and their interactions.
- The theory uses random simulation, network analysis, and Kaufman Networks to model and analyze software systems.
- Kaufman Networks can help identify attractors, which are patterns of behavior in a system, and understand how they interact with residues.
- By understanding the interactions between ordered and disordered systems, software architects can design more resilient systems that can survive unpredictable stressors.
- Residuality Theory can help software engineers manage complexity, reduce risk, and improve the reliability of software systems.


## CDNs 101: An Introduction to Content Delivery Networks - Jake Ginnivan - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=djyt_mG3S60](https://www.youtube.com/watch?v=djyt_mG3S60)

 - CDNs (Content Delivery Networks) are essential for streaming video, handling 1995% of internet traffic, and are crucial for web infrastructure.
- CDNs use edge computing to distribute content closer to end users, reducing latency and improving performance.
- Caching is the primary reason for using CDNs; it serves static assets and allows stale content to be served when the origin server is unavailable.
- CDNs intelligently route traffic, avoiding congestion and optimizing for performance.
- CDNs provide DDoS protection, saving money by mitigating attacks.
- CDNs offer flexibility, allowing custom routing rules, multiple origins, and adaptive strategies.
- CDN logs and analytics provide valuable insights into website performance, user behavior, and traffic patterns.
- Edge computing allows running compute resources at the edge of the network, near end users, reducing latency and improving responsiveness.
- CDNs can serve a wide range of web assets, including images, JavaScript files, and even collaborative applications.
- Caching strategies include private cache (browser), shared cache (reverse proxy), and CDN caching.
- CDN cache control headers allow fine-tuned control over caching behavior and cacheability of content.
- Cache-busted URLs are used to bypass cache and ensure the latest content is served.
- CDN misconfigurations, such as cache poisoning, can lead to security vulnerabilities.
- CDNs are particularly useful for single-page applications, allowing for flexible routing and API call handling.

In summary, CDNs play a critical role in modern web infrastructure by delivering content quickly and efficiently, reducing latency, and improving user experience. They provide a range of features, such as caching, DDoS protection, and flexible routing, making them an essential tool for web developers and businesses.


## Calculating the Value of Pie: Real-Time Survey Analysis With Apache Kafka® - Danica Fine - NDC Oslo

URL: [https://www.youtube.com/watch?v=V2S5odp-co4](https://www.youtube.com/watch?v=V2S5odp-co4)

 Summary:

- Apache Kafka: A distributed event streaming platform used for high-volume real-time data processing.
- Danica Fine: A developer advocate for Confluent, a company focused on Apache Kafka.
- Recipe for building a real-time survey analysis application using Kafka:
  - Define the survey.
  - Create a Kafka cluster.
  - Use Kafka topic to store survey responses.
  - Use Kafka consumer to read the data.
  - Use Kafka producer to send survey data.
  - Use Kafka connect to move data from other data sources to Kafka.
  - Use KSQL DB, a stream processing database, for real-time stream processing and analytics.
  - Use Avro schema registry to maintain the schema of survey responses.
  - Use Kafka connect to move data from other data sources to Kafka.
  - Use Kafka connect to move data from Kafka to other data sources.
  - Use Kafka Stream API, consumer producer API, for stream processing.
  - Use Kafka consumer to process data in real-time.
  - Use Kafka producer to send data to Kafka.

Notes:

- The speaker believes Kafka is a versatile and powerful tool for real-time data processing and stream processing.
- Apache Kafka has various use cases across different industries.
- The speaker appreciates the flexibility and ease of use of Kafka and its ecosystem.


## Real-time applications with Azure Web PubSub - Poornima Nayar - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=rL8ckLV_oRQ](https://www.youtube.com/watch?v=rL8ckLV_oRQ)

 - Azure Web PubSub: A service for building real-time applications using websockets and the publish-subscribe pattern.
- Key Concepts: Publisher, Subscriber, Channel (Topic), Connection, Hub (Organizational level).
- Publisher: Standalone entity, sends messages, loosely coupled with Subscribers.
- Subscriber: Receives messages, can belong to multiple groups (Hubs).
- Channel: Topic or stream of information, used to categorize messages.
- Connection: Unique connection ID, represents a user entity.
- Hub: Organizational level entity, can have multiple groups and subgroups.
- Billing Model: Units (1000 concurrent connections per Hub, free plan; premium plan for additional features).
- Key Features: Low barrier entry, supports multiple platforms and programming languages, built-in support for large real-time applications.
- Use Cases: Chat apps, IoT live dashboards, gaming, collaborative whiteboard apps.
- Azure Web PubSub vs SignalR: Web PubSub is simpler, offloads websocket connections to another service (Azure web app), suitable for different messaging patterns.
- Reliability: Does not store customer data, message queuing and reliable JSON protocol available through other services (e.g., Azure Service Bus).
- General Availability: Azure Web PubSub has been generally available for two years, with a rich set of documentation and examples available.


## Infrastructure as Code on Azure: Bicep vs Terraform vs Pulumi - Erwin Staal - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=tkEokNZEceE](https://www.youtube.com/watch?v=tkEokNZEceE)

 - Comparison of Infrastructure as Code tools: Bicep, Terraform, and Plumy
  - Bicep:
    - Created by Microsoft for Azure infrastructure management
    - DSL (Domain Specific Language) tailored for Azure
    - Easy to get started with, built-in CLI extension for Visual Studio Code
    - Uses output and input parameters, modular infrastructure
    - Supports Azure-specific tags and resources
  - Terraform:
    - Created by HashiCorp in 2014, DSL called HCL (HashiCorp Configuration Language)
    - Supports multiple cloud providers including Azure
    - Uses outputs, input variables, and templates
    - Can be transpiled to deploy using different languages (Go, C#, Python, etc.)
    - Supports state management, locking, and encryption
  - Plumy:
    - Created in 2018, supports multiple languages (C#, F#, VB.NET)
    - Polymorphic, allowing use of multiple languages and APIs
    - Supports Azure and other cloud providers
    - Can manage Active Directory, SQL Server, and more
    - Deployment process uses Azure CLI, PowerShell, or Plumy CLI

Key takeaways:
- Bicep is specifically designed for Azure and might be the easiest and quickest to learn.
- Terraform is more versatile and supports multiple cloud providers, but has more features and complexities.
- Plumy is a newer tool that supports multiple languages and APIs, but may lack the extensive tooling and community support of Terraform.


## Have I Been Pwned: Serving billions of requests and terabytes of data without going broke! - Stefán

URL: [https://www.youtube.com/watch?v=yQHoUDdoZe0](https://www.youtube.com/watch?v=yQHoUDdoZe0)

 - Have I Been Pwned (HIBP) serves billions of requests and terabytes of data without going broke.
- Troy Hunt launched HIBP in December 2013, allowing users to check if their data was leaked in data breaches.
- The service currently contains data from 600+ breaches and over 12 billion email addresses.
- HIBP uses an ingestion pipeline that allows law enforcement and government agencies to access the data.
- The Pawned API has been used by the FBI, NCAA, and other organizations to access password data.
- HIBP has over 34 government entities signed up, including the USA, UK, Germany, and Iceland.
- The platform uses Azure blob storage and Azure table storage for efficient data management.
- Cloudflare CDN is used to cache static data and reduce server load.
- Cloudflare Workers were initially developed to handle URL case sensitivity issues.
- The platform uses a dynamic compression strategy to handle large response sizes efficiently.
- Azure Function and Cloudflare Workers help manage cache hit ratios and minimize costs.
- Content-Length headers are removed to avoid conflicts with compression and response size management.
- Cloudflare's CDN and caching capabilities enable efficient handling of large volumes of traffic.
- Static data caching is essential for handling a large number of requests with minimal server load.
- Dynamic data handling and caching strategies are crucial for serving various types of content efficiently.
- The use of case-insensitive URLs helps prevent caching issues and improves user experience.


## Rules to better ChatGPT and using the best API ever ⭐ - Adam Cogan - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=3JARMLD2gus](https://www.youtube.com/watch?v=3JARMLD2gus)

 Here is a summary of the provided transcript:
- The speaker, Adam Kogan, shares his experience and insights about using the GPT API, specifically GPT-35 and GPT-4, in various contexts.
- He discusses the transformative capabilities of the API for tasks such as code generation, answering questions, and data analysis.
- Kogan emphasizes the importance of the GPT API in a wide range of applications, from coding and software development to content creation and marketing.
- He highlights the growing popularity and adoption of the GPT API in different industries and companies.
- Kogan mentions the potential of the GPT API to revolutionize the way people work and interact with technology.
- He also touches on the current limitations and potential future developments in the GPT API, such as integrating with more advanced AI models and features.
- The speaker shares his thoughts on the future of AI technology, predicting that it will have a significant impact on various aspects of our lives and work.


## Kubernetes Resiliency - Chris Ayers - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=2UHYr1tuBFs](https://www.youtube.com/watch?v=2UHYr1tuBFs)

 In this video, Chris Ayers discusses Kubernetes resiliency, focusing on various aspects such as:

- Infrastructure components: compute nodes, networking, storage, persistent volume claims, container registries, load balancers, and networking
- Single point of failure: leveraging availability zones, node pools, and regional awareness to mitigate risks
- High-level availability and low overhead: managed Kubernetes for multinode deployment, multi-region, multi-availability zone features, and Kubernetes topology spread constraints
- Node pools: customizing and controlling node configurations, isolating workloads, and scaling independently
- Azure-specific features: managed AKS, Azure Arc, Azure Monitor, Azure Insights, Prometheus, Grafana, and custom metrics
- Kubernetes best practices: monitoring, scaling, resource requests, limits, and liveness/readiness probes
- Pod disruption budgets: ensuring application availability and handling traffic efficiently with best-effort and best-effort budgets
- Application design: building resilient applications, leveraging horizontal pod autoscalers, and considering resource limits
- Dealing with CPU limits: understanding request limits, request/limit debate, and handling request limits efficiently
- Resource requests: setting appropriate values to avoid eviction and resource contention
- Kubernetes production best practices: recommendations for deploying applications and managing resources in production environments
- Azure Arc: enabling control plane policy enforcement, administration, and monitoring for on-premises clusters

In summary, resiliency in Kubernetes involves designing applications and infrastructure components, leveraging high-level availability and low overhead, managing node pools and availability zones, and following best practices for monitoring, scaling, and resource management.


## Sessionize: From idea to 100.000 speakers, and beyond! - Domagoj Pavlešić - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=NuzCoPxedMU](https://www.youtube.com/watch?v=NuzCoPxedMU)

 - Sessionize founder, Domagoj Pavlešić, shares his journey from a Croatian tourist to creating a successful event management software.
- Pavlešić started with developing a content management system, eventually leading to the creation of Sessionize, a system for organizing events and managing speakers.
- Sessionize's early success was driven by the founder's experience as an MVP and a conference organizer.
- The product development process included building a content management system, a speaker management system, and a registration system, all integrated into a single platform.
- Pavlešić emphasizes the importance of automation, laziness, and finding shortcuts in product development.
- Sessionize's scalable cloud-based system allows it to handle events globally, supporting seven continents and more than 106 countries.
- The company's growth was accelerated by a significant increase in sales during the pandemic, as events moved online.
- Sessionize's support system focuses on fast response times, which are achieved by automating routine tasks and providing human support when needed.
- Pavlešić shares his tips for building a successful software service: love the product, focus on a niche, listen to customers, and maintain simplicity.
- The founder also highlights the importance of automation, transparency, and customer support in creating a successful software service business.


## Common Query Patterns in MQL (Mongo Query Language) - Nuri Halperin - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=dzte-Q0NTdg](https://www.youtube.com/watch?v=dzte-Q0NTdg)

 - MQL (Mongo Query Language) query patterns and optimization techniques
  - Flat vs. nested document structure: Flat documents are easier to query and index
  - Major MQL operator categories: match, sort, group
  - Pattern recognition: Understand document structure and use it to your advantage in queries
  - Performance consideration: Optimize query execution by reducing memory footprint and disk usage
  - Using indexes: Index-assisted queries can improve performance by avoiding full collection scans
  - Pipeline stages: Gather (filtering and grouping), Assemble (optimizing and reshaping), and Present (displaying results)
  - Pipeline stages optimization: Reduce the amount of work per stage, avoid unnecessary operations, and use indexes
  - Efficient data manipulation: Use project (field selection) and unwind (decompose array elements) operators
  - Handling large datasets: Use skip and limit operators to manage result sets and reduce memory usage
  - Pattern matching: Use regular expressions and dollar match operators for complex pattern matching
  - Aggregation framework: Use it for advanced data processing and reporting
  - Pipeline stages: Gather (filtering and grouping), Assemble (optimizing and reshaping), and Present (displaying results)
  - Grouping: Use group operators to categorize and summarize data
  - Sorting: Use sort operators to organize data in a specific order
  - Matching: Use match operators to filter and select data based on criteria
  - Pipeline stages: Gather (filtering and grouping), Assemble (optimizing and reshaping), and Present (displaying results)
  - Performance optimization: Use indexes, pipeline stages, and efficient data manipulation techniques to improve query performance
  - Pattern recognition: Recognize common document structures and use them to your advantage in queries
  - Graph lookups: Use graph lookups for advanced data manipulation and querying in graph databases
  - Syntactic sugar: Use MongoDB-specific shortcuts and operators to simplify queries and improve readability
  - Facet: Use facet (bucket) operator to group and summarize data at different levels of granularity
  - Window functions: Use window functions for advanced calculations and aggregations
  - Pipeline operators: Learn the major operators in MQL and how to use them effectively
  - Query patterns: Familiarize yourself with common query patterns and their applications
  - Pipeline stages: Gather (filtering and grouping), Assemble (optimizing and reshaping), and Present (displaying results)
  - Dollar operators: Understand the use of dollar operators and their role in MQL
  - Whitelist and blacklist: Use whitelist and blacklist operators to control data flow in pipelines
  - Pattern matching: Utilize pattern matching operators and techniques for advanced querying
  - Data types: Understand how different data types interact with MQL operators and functions
  - Document structure: Recognize different document structures and their implications for querying
  - Optimization techniques: Apply various techniques to optimize MQL queries and improve performance
  - Pipeline stages: Gather (filtering and grouping), Assemble (optimizing and reshaping), and Present (displaying results)
  - Pipeline operators: Learn the major operators in MQL and how to use them effectively
  - Query patterns: Familiarize yourself with common query patterns and their applications
  - Pipeline stages: Gather (filtering and grouping), Assemble (optimizing and reshaping), and Present (displaying results)
  - Query optimization: Apply various techniques to optimize MQL queries and improve performance
  - Pipeline stages: Gather (filtering and grouping), Assemble (optimizing and reshaping), and Present (displaying results)
  - Pipeline operators: Learn the major operators in MQL and how to use them effectively
  - Query patterns: Familiarize yourself with common query patterns and their applications
  - Pipeline stages: Gather (filtering and grouping), Assemble (optimizing and reshaping), and Present (displaying results)
  - Query optimization: Apply various techniques to optimize MQL queries and improve performance
  - Pipeline stages: Gather (filtering and grouping), Assemble (optimizing and reshaping), and Present (displaying results)
  - Pipeline operators: Learn the major operators in MQL and how to use them effectively
  - Query patterns: Familiarize yourself with common query patterns and their applications
  - Pipeline stages: Gather (filtering and grouping), Assemble (optimizing and reshaping), and Present (displaying results)
  - Query optimization: Apply various techniques to optimize MQL queries and improve performance
  - Pipeline stages: Gather (filtering and grouping), Assemble (optimizing and reshaping), and Present (displaying results)
  - Pipeline operators: Learn the major operators in MQL and how to use them effectively
  - Query patterns: Familiarize yourself with common query patterns and their applications
  - Pipeline stages: Gather (filtering and grouping), Assemble (optimizing and reshaping), and Present (displaying results)
  - Query optimization: Apply various techniques to optimize MQL queries and improve performance


## How to choose the right database for your application - Zoe Steinkamp - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=hj2yFugmpz8](https://www.youtube.com/watch?v=hj2yFugmpz8)

 - Influx Data is a company that provides time series database solutions.
- Time series databases are specialized for handling time-stamped data with high cardinality, such as IoT devices or server infrastructure logs.
- The history of database management systems dates back to the 1970s, with the development of SQL and the relational database model.
- NoSQL databases, such as MongoDB, emerged in the early 2000s to address specific use cases that relational databases couldn't handle efficiently.
- Popular database types include relational databases, document databases, graph databases, time series databases, and vector databases.
- Time series databases are particularly useful for IoT devices, robotics, data shipping tracking, inventory level tracking, and monitoring applications.
- Graph databases are designed for complex relationships and are often used in social networks, recommender systems, and fraud detection.
- Vector databases are used for visual search, natural language processing, and AI chatbots.
- When choosing a database, it is crucial to consider the specific use case and requirements of the application.
- Influx Data's Vector database is an example of a powerful, open-source solution for handling unstructured data.
- The database landscape is constantly evolving, with new technologies emerging and existing databases being refined.
- It is important for developers to stay informed about new database technologies and consider different options when choosing the right database for their applications.


## You Keep Using That Word: Asynchronous And Interprocess Comms - Sam Newman - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=2LMEJ-WGFTk](https://www.youtube.com/watch?v=2LMEJ-WGFTk)

 - Asynchronous communication vs. synchronous communication in microservices architecture
- Asynchronous communication allows services to operate independently without waiting for a response, which can lead to better performance and scalability
- Synchronous communication involves waiting for a response before proceeding, which can lead to bottlenecks and decreased performance
- Asynchronous communication can be achieved through intermediaries like message brokers, which help decouple services and handle communication
- Message brokers are useful for temporal decoupling, ensuring service availability, and providing guaranteed delivery
- Asynchronous communication can help manage complex systems and handle issues like latency, throughput, and consistency
- It's important to understand the context in which the terms are used, as their meanings can vary
- Asynchronous communication is not always the best solution, and there are trade-offs to consider
- The distinction between synchronous and asynchronous communication is an ongoing conversation and debate in the field of software architecture and distributed systems

In summary, the debate between synchronous and asynchronous communication in microservices architecture revolves around performance, scalability, and system complexity. Asynchronous communication, when implemented with intermediaries like message brokers, can improve system resilience and decouple services, but it's important to understand the context in which the terms are used and consider the trade-offs involved.


## Developing microservices like a boss with Dapr and Azure Container Apps - Jakob Ehn - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=npVfxDiEyeg](https://www.youtube.com/watch?v=npVfxDiEyeg)

 - Dapper: A framework for building distributed applications with a developer-centric focus, simplifying complexities around service discovery, pub-sub messaging, state management, and more.
- Container apps: A cloud native service on Azure that makes it easy to build, deploy, and manage containerized applications.
- Revision: The idea of immutable, versioned snapshots of applications, allowing for easy rollbacks and traffic shifting without downtime.
- Dapper and container apps: A powerful combination for building microservices, implementing best practices, and handling resiliency, retries, exponential backoffs, circuit breakers, and handling timeouts.
- Dapper sidecar: A design pattern where an application consists of two services (A and B) that don't directly talk to each other, instead, Dapper handles the complexity of the interaction.
- Container apps features: Scaling (up and down), zero instance, automatic health checks, and self-healing capabilities.
- Scaling options: Choose between zero, one, or multiple instances, and use Azure service bus or event-driven scaling based on metrics like number of HTTP requests, message queue size, etc.
- Container apps pricing: Consumption plan allows for pay-as-you-go, based on resource usage (CPU and memory) and number of requests.
- Preview features: Container app copilot (AI-based assistance for application management) and other upcoming features.

Q&A Session:

Q: What are the primary benefits of using Dapper and container apps together for microservices?

A: The primary benefits of using Dapper and container apps together for microservices include:
- Simplified development: Dapper abstracts away complexities around service discovery, pub-sub messaging, state management, and more, making it easier for developers to build microservices.
- Improved resiliency: Dapper handles resiliency features like retries, exponential backoffs, circuit breakers, and handling timeouts, ensuring the microservices are robust and fault-tolerant.
- Enhanced scalability: Container apps support easy scaling up and down, as well as zero instance mode, allowing for efficient resource usage.
- Better cost management: Consumption plan allows for pay-as-you-go pricing based on resource usage and number of requests, enabling cost optimization.
- Easier maintenance and updates: Dapper's sidecar pattern and revision concept make it simple to rollback and switch between different versions of a microservice without downtime.
- Accelerated deployment: Container apps support quick and easy deployment using CLI or bicep, allowing developers to focus on application logic rather than infrastructure management.

Q: Can Dapper be used with other cloud platforms or is it specific to Azure?

A: Dapper is an open-source framework that can be used with any cloud platform or even on-premises environments. Although the presentation focuses on using Dapper with Azure container apps, it is not limited to Azure and can be adapted to other cloud platforms or even custom environments.

Q: How does Dapper handle service discovery?

A: Dapper abstracts away the complexity of service discovery by providing a built-in service registry. The registry keeps track of the available services and their current states (running, stopping, or failed). Dapper automatically updates the registry as services start, stop, or fail, and services can query the registry to discover other services and their endpoints. This simplifies service discovery and makes it more reliable, as Dapper handles the registry updates and caching automatically.

Q: Can you give an example of how Dapper handles resiliency?

A: Certainly! Dapper provides built-in resiliency features to handle failures and retries gracefully. Here are some examples:
- Retries: Dapper automatically retries failed requests with exponential backoffs, reducing the impact of transient failures.
- Circuit breaker: Dapper implements circuit breaker patterns to prevent cascading failures by detecting and handling failures in dependent services.
- Timeouts: Dapper supports timeouts for requests and retries, ensuring that a single failure does not block the entire system indefinitely.
- Health checks: Dapper performs health checks on services to determine their availability and routing traffic accordingly.

These resiliency features help maintain the reliability and robustness of microservices built with Dapper.


## Message processing failed! But what's the root cause? - Laila Bougria - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=U8Aame0akl4](https://www.youtube.com/watch?v=U8Aame0akl4)

 - Importance of observability in distributed systems:
  - Observability allows developers to understand and debug issues in a distributed system.
  - Observability helps identify the root cause of a problem and optimize system performance.
- Challenges in debugging distributed systems:
  - Difficulty in tracing through multiple components.
  - Debugging time can be long and complex.
  - Identifying the root cause can be hard.
- Benefits of using Open Telemetry and distributed tracing:
  - Standardized approach for collecting and analyzing telemetry data.
  - Supports multiple languages, platforms, and tools.
  - Enhances observability by capturing trace, log, and metric data.
- Example of adding observability to an application:
  - Use activity source and span to represent components and interactions.
  - Collect and send telemetry data to observability backends.
  - Use exporters, tags, and attributes to enrich data and improve visibility.
- Trade-offs when implementing observability systems:
  - Additional cost, latency, and bandwidth can be introduced.
  - Benefits include faster debugging, better performance optimization, and proactive monitoring.
- Feature flagging and chaos engineering:
  - Feature flagging allows controlled release of new features to user segments.
  - Chaos engineering introduces controlled disruptions to test system resilience and robustness.
- Importance of observability for debugging and performance optimization:
  - Observability aids in faster failure investigation and solving issues in production environments.
  - Observability helps prevent and mitigate risks in a distributed system.

In summary, observability is crucial for debugging and optimizing distributed systems. Open Telemetry, distributed tracing, feature flagging, and chaos engineering can enhance observability and improve system resilience, debugging efficiency, and performance optimization.


## Falling off the Edge: Practical Uses for Edge Computing - Alexander Karan - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=GrcanQ-nT_s](https://www.youtube.com/watch?v=GrcanQ-nT_s)

 - Edge Computing:
  - Importance of Edge Computing for fast response times and reduced latency
  - Edge Computing is not specific to any technology stack
  - Focus on architecture, not specific technologies
- Edge Functions:
  - Edge Functions = Serverless Functions + Edge Network
  - Edge Functions are growing in popularity due to benefits like:
    - Faster response times
    - Dynamic content rendering
    - Reduced load on central servers
- Edge Runtime:
  - Edge Runtime refers to the process of instant compute
  - It's about the location, not the technology
- Edge Computing in the future:
  - Growing data sets
  - Continued improvements in network technology
  - Distributed data management becoming more important
- Edge Functions vs Serverless Functions:
  - Pros and cons of each approach
  - Edge Functions may have an advantage due to reduced latency and faster response times
- Edge Computing and Web Development:
  - Edge Computing can improve web applications by:
    - Reducing latency
    - Enabling faster, dynamic content rendering
    - Allowing for global distribution of code
- Tools and Platforms:
  - Edge computing tools and platforms like Dino, Dino KV, Planet Scale, Neon, and cloudflare worker
  - Advantages and disadvantages of each
- Edge Computing and Distributed Data:
  - The challenge of managing distributed data across Edge Networks
  - The importance of understanding data location and sensitivity
- Building APIs with Edge Computing:
  - Edge Computing can make API building and distribution faster and more efficient
  - Examples include using Dino, Dino deploy, AWS Lambda, and cloudflare worker
- Edge Computing in the Real World:
  - Real-world examples of Edge Computing applications, such as IoT devices, smart speakers, and other connected devices
- Edge Computing and Legal Requirements:
  - Legal and regulatory considerations for Edge Computing, such as data privacy and security laws
- Future of Edge Computing:
  - Continued growth and innovation in Edge Computing technologies
  - The potential for Edge Computing to become a standard tool in the developer tool belt
- Final Thoughts:
  - Edge Computing offers significant benefits for developers and users alike
  - It's important to keep an open mind and stay informed about new developments in the field


## Introduction to Serverless with .NET + AWS Lambda - Brandon Minnick - NDC Oslo 2023

URL: [https://www.youtube.com/watch?v=9AanhFsM0cE](https://www.youtube.com/watch?v=9AanhFsM0cE)

 - Introduction to Serverless with AWS Lambda and .NET
- Serverless definition: serverless is about not worrying about servers, operating systems, or redundancy
- History of serverless: on-premise data centers, cloud infrastructure, and serverless
- Advantages of serverless: scalability, less maintenance, reduced overhead, and cost-effective
- AWS Lambda and .NET: using AWS Lambda for serverless applications and .NET for coding
- Cold start time: time taken to start executing a serverless function for the first time
- AWS cold start improvements: AWS continuously working on reducing cold start times
- Serverless benefits: no server management, automatic scaling, and pay-per-use
- Serverless use cases: mobile apps, web apps, API backends, and more
- Sample application: generating thumbnail images from uploaded photos using S3 and AWS Lambda
- Tools and resources: AWS Toolkit for Visual Studio, AWS Explorer extension, AWS Lambda, S3, and API Gateway
- Serverless challenges: cold start times, latency, and learning curve
- Serverless best practices: keep code simple, use stateless functions, and optimize performance
- Serverless future: ongoing improvements, cost reduction, and wider adoption

In summary, the presentation discussed the concept of serverless computing with AWS Lambda and .NET, highlighting the benefits and use cases. It provided a practical example of a serverless application and touched upon the tools and resources needed to build serverless applications. The presentation also covered some challenges and best practices for working with serverless technologies.


## Offline first computing in an online world - Ørnulf Staff  - NDC TechTown 2023

URL: [https://www.youtube.com/watch?v=Pa_zuzhRZ6A](https://www.youtube.com/watch?v=Pa_zuzhRZ6A)

 - Offline-first computing in an online world
- Distributed Systems
- Offline-first vs Local-first computing
- Git for offline-first
- Conflict-Free Replicated Data Types (CRDT)
- Data Distribution Service (DDS)
- Publish/Subscribe model
- DDS in Offline-first applications
- Local-first architecture
- Consistency vs Availability
- Automerge Library
- DDS vs CRDT in conflict resolution
- Local-first software and libraries
- Chat application example

Key points:
- Offline-first computing is crucial for applications in disconnected environments.
- Distributed Systems like DDS can help manage data synchronization in offline-first applications.
- CRDTs are useful for handling conflicts in distributed systems.
- Automerge Library is an open-source project that can help with conflict resolution in distributed systems.
- Local-first software and libraries can assist in building robust offline-first applications.


## Pub crawling in Orleans: exploring the Actor model - Sander Molenkamp - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=VCTjc56tr2o](https://www.youtube.com/watch?v=VCTjc56tr2o)

 - Orleans is a distributed computing framework designed by Microsoft for building scalable, distributed applications with ease.
- Orleans uses the Actor model, a concurrency model focused on asynchronous message passing between actors, which are lightweight objects that execute tasks in a parallel fashion.
- Orleans has built-in support for fault tolerance, load balancing, and automatic scaling.
- Orleans allows developers to build applications that work across multiple nodes, making the system more resilient to failures and better equipped to handle high loads.
- Orleans simplifies building distributed systems by abstracting away many complexities and providing a high-level API for developers to interact with.
- Orleans comes with a dashboard for monitoring the health and performance of the cluster, as well as individual actors.
- The Orleans framework can be used in various scenarios, including distributed event-driven applications, microservices, and IoT applications.
- In the example given, Orleans was used to build a beer crawler application that keeps track of visited venues and their beer ratings.
- The Orleans framework allows developers to build applications that are highly scalable and fault-tolerant, making it suitable for a wide range of applications in various industries.


## Faster, Cheaper, Greener: Pick Three - Lea Mladineo - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=52Pjo6NCiJs](https://www.youtube.com/watch?v=52Pjo6NCiJs)

 - The speaker shared their journey of transforming the fund UPS's software engineering from a monolithic architecture to a cloud-native distributed architecture using AWS services.
- They highlighted the challenges of scaling and maintaining an over-provisioned infrastructure, the growing storage demands, and the need for a more efficient and sustainable solution.
- The new architecture leverages AWS components such as Lambda, Fargate, EC2, S3, DynamoDB, and Aurora Serverless V2 to distribute computation, store data efficiently, and optimize resource usage.
- The result is a more stable, faster, and cost-effective solution that requires less maintenance and supports business growth without negatively impacting the environment.
- The speaker emphasized the importance of sustainability in software engineering, using tools and platforms that reduce carbon footprint, and educating clients on making greener choices.
- They also discussed the company's commitment to becoming carbon neutral by 2027 and the role of software engineers in creating a sustainable future.

Key Takeaways:
- Transitioning from a monolithic architecture to a cloud-native distributed architecture can improve efficiency, stability, and cost-effectiveness.
- AWS services can be effectively used to address challenges related to scaling, storage, and resource optimization.
- Sustainability should be a consideration in software engineering, with a focus on reducing carbon footprint and using green technologies.


## Toward a carbon-aware Cloud - Olivier Bierlaire - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=l9WQFcgaJ_I](https://www.youtube.com/watch?v=l9WQFcgaJ_I)

 - Toward a carbon-aware Cloud: Focusing on reducing carbon emissions from cloud infrastructure.
  - Measuring and reducing carbon emissions in cloud infrastructure is a growing concern.
  - Cloud data centers are responsible for 20% of the world's digital sector electricity usage.
  - Estimating the carbon footprint of software is essential to understand the environmental impact of digital technologies.
  - Emissions come from direct (scope one), indirect (scope two), and value chain (scope three) activities.
- Green software foundation and carbon intensity formulas help calculate the carbon footprint of software.
  - Software Carbon Intensity (SCI) is expressed in grams of CO2 per hour.
  - SCI considers energy consumed by software and the carbon intensity of the electricity grid in the country where the data center is located.
- Embodied energy (emission) and unembodied emission are important factors to consider in the carbon footprint calculation.
  - Embodied energy considers the energy used during the manufacturing and disposal of server hardware.
- Carbon-aware decisions in cloud infrastructure, such as choosing data centers with clean energy sources and optimizing hardware usage, can significantly reduce emissions.
  - Carbon-aware scalers, demand shifting, and load balancing across regions with lower carbon intensity grids can help reduce emissions.
- Cloud infrastructure can be made more sustainable by adopting green software practices, using energy-efficient hardware, and following best practices in software development and operations (DevOps).
  - Green coding, efficient code, and efficient hardware usage are key to reducing cloud emissions.
- Tools like Bavista API, bav Vista, and ClimateCare can help measure and manage the carbon footprint of cloud infrastructure and software.
- Companies are increasingly required to report their carbon emissions, with regulations like the Corporate Sustainability Reporting Directive (CSRD) becoming more stringent in the future.
- Many companies are committing to become carbon neutral or achieve net zero emissions, often by investing in renewable energy sources and offsetting their carbon footprint.


## Messaging: The fine line between awesome and awful - Laila Bougria

URL: [https://www.youtube.com/watch?v=pImwOEsQkQo](https://www.youtube.com/watch?v=pImwOEsQkQo)

 In this transcript, the speaker discusses the challenges and benefits of using a message-based system in software development. Here are the key points:

- Message-based systems can reduce coupling and improve scalability, reliability, and performance.
- Asynchronous communication patterns (request-response, publish-subscribe, and one-way communication) have different advantages and use cases.
- Asynchronous communication can lead to UI inconsistency and data becoming outdated.
- Messaging systems can help mitigate these issues and improve resilience and fault tolerance.
- The SLA (Service Level Agreement) and SLI (Service Level Indicator) need to be enforced for message-based systems.
- Latency, distributed computing, and processing time are factors to consider in message-based systems.
- Orchestration patterns and choreography can help manage complex workflows in distributed systems.
- Testing and observability are critical for success in message-based systems.
- Item potency, a measure of how many instances of a message can be processed simultaneously, is an important concept in message-based systems.
- Patterns like Saga can be used to handle item potency and mimic distributed transactions.
- Messaging systems can introduce complexity, but they can also be powerful tools for building scalable and reliable systems.


## The Modern Trolley Problem - Responsible AI Principles - Michelle Sandford

URL: [https://www.youtube.com/watch?v=54_c2o--O6M](https://www.youtube.com/watch?v=54_c2o--O6M)

 - Modern Trolley Problem: Discussion on AI ethics, responsibilities, and decision-making processes in autonomous vehicles
- Importance of considering greatest happiness, deontological ethics, and responsibility in AI decision-making
- Examples of AI applications in autonomous vehicles and the need for responsible design and deployment
- Brief history and evolution of AI, including facial recognition, object detection, and custom vision applications
- The role of AI in various industries, including finance and healthcare, and the importance of fairness and transparency
- The challenge of balancing innovation and safety in autonomous vehicles, and the need for proper testing, training, and maintenance
- The importance of responsible AI in protecting privacy and security, and complying with regulations such as GDPR
- The potential for AI to improve accessibility and inclusivity in various aspects of society
- The need for explainable AI and human-understandable code to ensure transparency and accountability
- The potential for AI to make errors and the importance of addressing these errors and learning from them
- The role of manufacturers, designers, and end-users in ensuring responsible use of AI technology

In summary, the speaker discusses the ethical implications and challenges of AI in autonomous vehicles, highlighting the importance of responsible design, fairness, transparency, and human-understandable code. They also emphasize the need for accountability in AI decision-making and the potential for AI to improve society when used responsibly.


## Architecture Modernization: Aligning Software, Strategy, and Structure - Nick Tune

URL: [https://www.youtube.com/watch?v=75QmuivHE0Y](https://www.youtube.com/watch?v=75QmuivHE0Y)

 Summary:

- Modernization: Aligning software, strategy, and structure
- Established companies have advantages such as market share, brand reputation, and customer loyalty
- Challenges in modernizing: organizational side, organizational baggage, and Legacy software
- Modernization process: flow, architecture, work organization, and flow
- Importance of recognizing misassumptions and complexity
- Architects need a clear mindset, focusing on value streams and business outcomes
- Domain-driven design (DDD) and boundary identification
- Technology upgrade, infrastructure upgrade, and language runtime library
- Legacy encapsulation: rethinking the whole user experience and business workflow
- Tools for modernization: listening, impact mapping, event storming, and domain modeling
- Importance of enabling teams: coaching, team topology, and skill development
- Workshops and team alignment can help kickstart modernization efforts
- Modernization is not a one-time event but an ongoing process of continuous improvement and learning.


## Fine Grained Authorisation with Relationship-Based Access Control - Ben Dechrai - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=XTyEKCKcWYw](https://www.youtube.com/watch?v=XTyEKCKcWYw)

 Here is a concise summary of the transcript:

- Relationship-Based Access Control (RBAC) evolves with time and provides more granularity in managing access permissions.
- RBAC is based on relationships and roles, which can be defined by entities and attributes.
- The Entity-Attribute-Value (EAV) model is used to define entities and their attributes.
- In the EAV model, each attribute has a value that can be used to define permissions, such as access times or locations.
- An example of an RBAC model involves defining entities (e.g., user, article, comment) and their relationships (e.g., owner, editor, member).
- Relationship-Based Access Control can handle complex permission scenarios, such as accessing sensitive documents within specific timeframes or locations.
- RBAC is useful in a variety of contexts, such as document management systems, social media platforms, and corporate environments.
- Open source solutions, such as Oso and Sona, can be used to implement RBAC.
- The speaker recommends using RBAC for its ability to provide detailed information about permissions, access control, and reporting.

In summary, the speaker presents a comprehensive overview of relationship-based access control, its benefits, and how it can be implemented in various contexts.


## Effects Malware hunting in Cloud environment - Filipi Pires - NDC Security 2024

URL: [https://www.youtube.com/watch?v=akozHp2dh6g](https://www.youtube.com/watch?v=akozHp2dh6g)

 - Malware hunting in Cloud environments
- File manipulation techniques
- Investigating Cloud attack possibilities
- Analysis of PDF files
- Reverse engineering and attacker techniques
- Cloud provider security
- Importance of sharing knowledge and feedback
- Open-source tools for Cloud security
- Understanding Cloud provider responsibilities
- The role of attackers and defenders in Cloud environments

Key points:
- Malware hunters need to understand Cloud environments and file manipulation techniques
- Investigating Cloud attacks requires knowledge of attacker techniques and reverse engineering
- PDF files can be manipulated for malicious purposes
- Cloud providers have a shared responsibility for security
- Open-source tools can help improve Cloud security
- Feedback and knowledge sharing are crucial for improving Cloud security practices


## Asymmetric Encryption: A Deep Dive - Eli Holderness - NDC Security 2024

URL: [https://www.youtube.com/watch?v=cGu3g-iQWT4](https://www.youtube.com/watch?v=cGu3g-iQWT4)

 - History of cryptography:
  - Early methods: Caesar Cipher, substitution ciphers, transposition ciphers
  - Development: asymmetric cryptography, RSA, Diffie-Hellman, elliptic curve cryptography (ECC)
- RSA:
  - Developed 1977 by Rivest, Shamir, and Adleman
  - Asymmetric cryptographic system based on mathematical problems (factorization)
  - Secure: relies on the difficulty of factoring large numbers
- ECC:
  - Developed to provide the same security as RSA with smaller key sizes
  - Based on elliptic curve discrete logarithm problem
  - More efficient: smaller key sizes, faster computations, smaller payload sizes
- Quantum Computing:
  - Theoretical computing model using quantum bits (qubits)
  - Potentially break RSA and ECC: Shor's algorithm, quantum factoring
- Post-Quantum Cryptography:
  - Crypto systems resistant to quantum attacks
  - Examples: Lattice-based cryptography, code-based cryptography, hash-based cryptography
- Modern Cryptography:
  - Continuous development of new algorithms and systems
  - Incorporating quantum-resistant algorithms in browsers and systems
  - Focus on security, efficiency, and performance

The talk covers the history of cryptography, from early methods like the Caesar Cipher to modern asymmetric cryptography like RSA and ECC. It also discusses the potential impact of quantum computing on cryptography and the development of post-quantum cryptography to counteract the threat. The speaker emphasizes the importance of continued research and development in cryptography for maintaining secure communication systems.


## Optimizing Cloud Detection & Response With Security Chaos Engineering - Kennedy Torkura

URL: [https://www.youtube.com/watch?v=PgckGfVy4a0](https://www.youtube.com/watch?v=PgckGfVy4a0)

 - Kennedy Torkura discussed optimizing cloud detection and response using security chaos engineering.
- Security chaos engineering helps uncover vulnerabilities and improve resilience in cloud environments.
- Traditional detection methods like antivirus and intrusion detection systems have limitations.
- Security chaos engineering can be used to test cloud detection and response systems, identify weaknesses, and improve resilience.
- A simple example of testing cloud detection response was provided:
  - Create a user with elevated privileges in AWS.
  - Attach a policy that gives the user access to S3.
  - Create a login profile that allows the attacker to log in using the compromised credentials.
  - The attacker can then access the S3 bucket and perform actions.
- Data Dog can be used to detect cloud security incidents, analyze logs, and generate alerts.
- The process of emulating attacks helps understand the implications of detected findings and optimize detection rules.
- Continuous improvement is crucial in the ever-changing cybersecurity landscape.
- In a demo, Kennedy showed how an attacker could exploit misconfigured cloud resources like a public S3 bucket and Lambda function.
- Security chaos engineering can help identify vulnerabilities, prevent attacks, and improve the overall security posture of cloud environments.


## The Top 10 List of Istio Security Risks and Mitigation Strategies - José Carlos Chávez

URL: [https://www.youtube.com/watch?v=ktaSNg_fZ0Q](https://www.youtube.com/watch?v=ktaSNg_fZ0Q)

 - Top 10 List of Istio Security Risks and Mitigation Strategies:
  1. Secure communication: Encrypt traffic by default and use permissive security settings to reduce the attack surface.
  2. Unsafe authorization patterns: Use explicit deny policies to limit access and avoid giving excessive permissions.
  3. Service account and authorization: Least privilege principle, minimize permissions granted, and enforce role-based access control.
  4. Broken object-level authorization: Use Next-Generation Access Control (NGAC) to adapt policies based on real-world system states.
  5. Supply chain vulnerabilities: Regularly scan images for vulnerabilities, use signed images, and follow patch management processes.
  6. Web application firewall and patch network: Deploy WAFs to block attacks and monitor for anomalous behavior.
  7. Observability and monitoring: Collect and analyze observability data to understand incidents and improve incident response.
  8. Access control and incident response: Implement a recovery plan and proactively monitor for potential incidents.
  9. Zero trust model: Adopt a zero trust mindset and design policies based on the assumption that attackers are already inside the network.
  10. Dependency management: Regularly review and manage dependencies to ensure they are secure and up-to-date.

To mitigate security risks in Istio, focus on secure communication, authorization, observability, and dependency management. Implementing a zero trust model and proactively monitoring for potential incidents can also significantly improve security posture.


## Mirror, mirror on the wall... Am I a critical thinker after all? - Laila Bougria - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=fxB2aMKcZDw](https://www.youtube.com/watch?v=fxB2aMKcZDw)

 In the transcript, Leila Bougria shares her thoughts and experiences on critical thinking, decision making, and problem-solving in the software engineering industry. Here are the key points:

- Critical thinking is essential for understanding complex problems and making informed decisions.
- It's important to first clearly define the problem and identify the stakeholders affected.
- Critical thinking involves questioning assumptions, evaluating risks, and gathering supporting facts.
- Decision making should be structured, taking into consideration multiple alternatives, risks, and assumptions.
- Feedback and collaboration are crucial in the decision-making process, fostering an open and diverse environment.
- Continuous learning and self-reflection are key to improving critical thinking skills.
- The process of decision making should be adaptive and open to change as new information becomes available.
- Incorporating building blocks and structured feedback can help in refining the decision-making process.
- Embracing critical thinking can lead to better problem-solving, more effective decision making, and overall improvement in the software engineering industry.


## Top 5 techniques for building the worst microservice system ever - William Brander - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=gjqZLEyS5ww](https://www.youtube.com/watch?v=gjqZLEyS5ww)

 In this talk, William Brander discusses the top 5 techniques for building the worst microservice system ever, and he humorously highlights the challenges and pitfalls of microservices.

1. Strangler Fig Pattern: Start with a monolith and incrementally extract functionality into separate microservices. This often results in increasing system complexity and decreased performance.
2. Big Bang Rewrite: Attempt a complete rewrite of the system, which is likely to introduce new problems, especially network operations, tight coupling, and increased latency due to contention and garbage collection issues.
3. Use Off-the-Shelf Frameworks: Relying on popular frameworks may lead to tight coupling between services, making the system difficult to change or maintain.
4. Define Service Boundaries Based on Verbs: Instead of defining service boundaries based on nouns (entities), defining them based on verbs (actions) can lead to cross-service communication and tight coupling.
5. Encourage Cross-Service Communication: Although cross-service communication can be a good way to share data, it can also lead to tight coupling, conflation of logical and physical boundaries, and difficulty in scaling or changing individual services.

William Brander emphasizes that microservices are not always the best solution, and that developers should carefully consider the downsides and trade-offs of building microservices.


## C4 models as code - Simon Brown - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=OmAiWPLrwkw](https://www.youtube.com/watch?v=OmAiWPLrwkw)

 Summary of the Simon Brown C4 models and tooling talk:

- C4 Model:
  - Four levels of abstraction: Context, Container, Component, Code
  - Hierarchical abstraction of software systems
  - Focus on abstraction, not exact code
  - Use abstraction levels to describe software systems

- C4 Model Tools:
  - Structurize (structurized.com)
    - Open-source tooling for creating C4 diagrams
    - Exports in various formats (PlantUML, Mermaid, DSL, etc.)
    - Automatic layout and manual layout options
    - Customizable element styling
    - Export to diagram code
  - Structurer Light:
    - Lightweight, Java-based tool for creating C4 diagrams
    - Exports to various formats (PlantUML, Mermaid, etc.)
    - Integration with Docker and Spring Boot
    - Customizable styling and shapes
    - Supports exporting to different diagram formats
  - Structurize Site Generator:
    - Open-source tool for visualizing models as static websites
    - Creates interactive diagrams with hierarchical navigation
    - Export to various formats (PlantUML, Mermaid, etc.)
  - Arch Overarch:
    - Open-source tool for defining hierarchical C4 models
    - Extensible, data-notation (EDN) based
  - PlantUML Rendering Engine:
    - Automatic layout for C4 diagrams
    - Customizable with YAML syntax
    - Editor with autocomplete and help

- Myths and Critiques:
  - C4 model pigeonholed into four levels of abstraction
  - Some prefer more customizable abstraction levels (e.g., Igraph/ILPH)

- Key Takeaways:
  - C4 is a powerful tool for visualizing software systems
  - Structurize and other tools provide easy-to-use, open-source support for creating and exporting C4 diagrams
  - Customization and extension options allow for greater flexibility and adaptability in modeling


## Don’t Build a Distributed Monolith: How to Avoid Doing Microservices Wrong - Jonathan J. Tower

URL: [https://www.youtube.com/watch?v=9iW_2MMLd4w](https://www.youtube.com/watch?v=9iW_2MMLd4w)

 Summary:

- Microservices: A popular architectural pattern where applications are composed of small, independent services that communicate with each other using well-defined APIs.
- Monolith vs. Microservices: A monolith is a single application with all components integrated into a single unit. Microservices are smaller, independently deployable services that can scale and evolve independently.
- Distributed Monolith: A term used to describe a monolithic architecture that is built using microservices but ends up tightly coupled and difficult to scale and maintain.
- Modular Monolith: A well-designed monolith that is easy to change, with components that are loosely coupled and independently deployable.
- Ball Mud Monolith: A poorly designed monolith that is tightly coupled, difficult to change, and has high technical debt.
- Event-driven architecture: A design pattern where services communicate with each other by publishing and subscribing to events.
- Cross-cutting concerns: Aspects of a system that cut across multiple modules or components, such as logging, authentication, and configuration.
- Synchronous communication: A communication pattern where a service sends a request to another service and waits for a response before continuing.
- Asynchronous communication: A communication pattern where a service sends a request to another service without waiting for a response.
- Microservices anti-patterns: Common mistakes made when implementing microservices, such as overusing microservices, not enforcing consistency, or creating distributed monoliths.
- Team organization: A critical aspect of microservices architecture, where teams are organized around individual microservices.
- Automation: Automating the build and release process is essential for microservices success, as it allows for frequent, independent updates and minimizes errors.

Key takeaways:

- Understand the pros and cons of monolithic vs. microservices architecture before deciding which to use.
- Design your system with modularity and loose coupling in mind, even if starting with a monolithic architecture.
- Pay attention to team organization and communication patterns, as these are critical for successful microservices implementation.
- Automate your build and release process to ensure fast, efficient updates and minimize human error.


## Understanding Microservices: a guide for the monolithic developer - Layla Porter - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=X332xtfQs5M](https://www.youtube.com/watch?v=X332xtfQs5M)

 - Understanding Microservices: guide for monolithic developers
- Microservices vs. Monolithic architecture
- Benefits of Microservices: modular, easier scaling, easier to maintain, fault isolation
- Challenges: complexity, learning curve, distributed systems, communication, data consistency
- Approach: incremental change, decompose monolith into microservices, loose coupling, high cohesion
- Patterns: DDD (Domain-Driven Design), Kit Kat pattern (modular monolith), Citadel pattern (satellite pattern)
- Communication: synchronous, asynchronous (event-driven, message-driven)
- Services: order service, payment service, notification service
- Distributed System Concepts: RabbitMQ, service discovery, API Gateway, load balancing, circuit breaker, configuration management
- Security: API Gateway, identity management, authentication, authorization
- Monitoring and Observability: health checks, availability, scalability, load balancing
- Tools: Eureka (service discovery), Netflix OSS, Spring Cloud, Kubernetes, Dapper, API Gateway, Docker
- Tips: plan right solution, manage complexity, learn incrementally, embrace distributed systems, keep learning

Overall, the talk covers the journey of understanding and adopting microservices architecture, from the benefits and challenges, patterns and practices, to tools and considerations for successful implementation and management.


## Access control in message-driven systems - Marc Klefter - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=eTfCLCFDHkU](https://www.youtube.com/watch?v=eTfCLCFDHkU)

 In the talk, Marc Klefter discusses the implementation of Access Control in message-driven systems using a zero-trust approach. Key points include:

- Emphasis on attribute-based access control for flexibility in decision-making.
- Importance of verified identity and tokens, such as JSON Web Tokens (JWTs), for enforcement.
- The role of microservices and distributed systems, where verified identity claims are propagated along synchronous flows.
- The use of asynchronous communication for zero-trust messaging, allowing for different enforcement points and reducing trust.
- Different types of messages: command, query, and event-driven.
- The concept of event sourcing, where events are stored long-term and used to reconstruct state, allowing for replayability.
- The importance of identity context and how it can be handled in different scenarios.
- The use of Open Policy Agent (OPA) and policy definition languages like OPA, Rego, for implementing access control.
- The challenges of implementing zero-trust access control in message-driven systems, such as token expiration and user removal.
- The need for collaboration between application developers and network security personnel in implementing access control.

Overall, the talk highlights the complexities of implementing access control in message-driven systems and the importance of a zero-trust approach for ensuring security and proper enforcement.


## Building that glorious monolith. And carving it too. - Glenn F. Henriksen - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=knJ5fq2p6mk](https://www.youtube.com/watch?v=knJ5fq2p6mk)

 - Glenn F. Henriksen discusses the challenges and benefits of building glorious monoliths and microservices.
- Monoliths can be easier to maintain and understand due to their single-codebase nature, while microservices offer scalability and flexibility through distributed systems.
- Glenn talks about his experience at a startup, Justified, where they initially chose microservices for their application but later decided to build a monolith.
- Advantages of microservices:
  - Loosely coupled and independently deployable services
  - Easier to scale and maintain individual services
  - Better fault isolation and resilience
- Disadvantages of microservices:
  - Increased complexity due to distributed systems
  - Network latency and communication overhead
  - Difficulty managing configuration and state across services
- Glenn discusses the importance of domain-driven design, which helps in understanding the domain and managing complexity by breaking down the application into smaller, more manageable parts (bounded contexts).
- He also talks about the role of events and event-driven architectures in microservices, where events represent changes in the domain state and can trigger changes in other services.
- Glenn emphasizes the importance of discipline, test-driven development, and continuous integration/continuous deployment (CI/CD) practices in managing complexity and ensuring quality in both monoliths and microservices.
- He concludes by stating that building a glorious monolith or distributed system depends on the specific needs of the project and the team's expertise, and both approaches have their advantages and disadvantages.


## Autonomous microservices don't share data. Period. - Dennis van der Stelt - NDC Porto 2023

URL: [https://www.youtube.com/watch?v=_UN50hNZlx4](https://www.youtube.com/watch?v=_UN50hNZlx4)

 Here are the key points from the transcript:

- Dennis van der Stelt discusses the evolution of software architecture and development, from microservices to serverless architecture and event-driven systems.
- He highlights the challenges of maintaining large systems with microservices, such as high maintenance costs and difficulty in scaling.
- He explores different approaches, including containerization (Docker), serverless functions (AWS Lambda, Azure Functions), and event-driven architecture.
- He emphasizes the importance of understanding boundaries and high cohesion in designing systems, and the need for proper encapsulation to avoid dependencies and maintain autonomy.
- He discusses the role of event sourcing and event-driven architecture in building a more scalable and resilient system.
- He mentions the concept of eventual consistency, which allows systems to work with more loosely coupled components, improving fault tolerance and scalability.
- He encourages developers to continuously learn and adapt to new technologies and architectures, such as messaging systems and event-driven architecture.
- He shares insights from his experiences and lessons learned over the years, emphasizing the need to re-evaluate and question established practices.

In summary, the talk covers the journey of software architecture from monolithic systems to microservices and serverless architecture, discussing the challenges and benefits of each approach, and the importance of understanding boundaries, high cohesion, and event-driven architecture for building scalable and maintainable systems.


## Automating security monitoring - Truls Dahlsveen - NDC Security 2024

URL: [https://www.youtube.com/watch?v=RDYtEZltIY4](https://www.youtube.com/watch?v=RDYtEZltIY4)

 - Security monitoring is evolving due to increased complexity and alert fatigue
- Automation is key to addressing alert fatigue and handling security incidents efficiently
- Automation helps in reducing the time spent on repetitive tasks, freeing up analysts to focus on more complex issues
- Incorporating machine learning and artificial intelligence (AI) can help improve detection and response
- Security orchestration and automation response (SOAR) tools can help streamline incident handling
- Enrichment is important for better visibility and understanding of incidents
- Automation can help detect anomalies and malicious activities more effectively
- Threat intelligence sharing platforms can help in identifying known malicious indicators
- Automation can help handle duplicate incidents and reduce the need for manual intervention
- AI-powered tools can help generate concise summaries and reports, improving communication with stakeholders
- Automation can also improve overall efficiency and save time in handling incidents
- Teaching analysts to automate tasks can empower them and make their work more effective.

Overall, the speaker emphasizes the importance of automation in security monitoring to combat alert fatigue, improve efficiency, and enhance incident handling. Automation, along with machine learning and AI, can significantly reduce the time spent on repetitive tasks and help security analysts focus on more complex issues.


## Tune your Toolbox for Velocity and Value - Josh Grossman - NDC Security 2024

URL: [https://www.youtube.com/watch?v=RWoOGC68Bg0](https://www.youtube.com/watch?v=RWoOGC68Bg0)

 Here is a concise summary of the transcript:

- Importance of choosing the right application security testing tool
- Balance between SAST and DAST tools
- Understanding different tools and their capabilities
- SAST tools for finding vulnerabilities in code
- DAST tools for testing running applications
- Need for combining SAST and DAST tools
- Importance of tool proficiency and configuration
- Understanding software composition analysis
- Focus on tool integration, utility, and methodology
- Importance of tool customization and process understanding
- SCA and tool integration for dependency management
- Balancing tool coverage and accuracy
- Importance of false positive management
- Integration of SCA with ticketing systems and user interfaces
- Continuous improvement and learning from false positives
- Importance of measuring tool performance with metrics
- Balance between tool accuracy and false positives
- Addressing vulnerabilities in libraries and dependencies
- Importance of tool configuration for different scenarios
- SCA for identifying vulnerabilities in dependencies
- Importance of process understanding and tool proficiency
- Importance of tool customization and process understanding

The speaker emphasizes the importance of understanding and choosing the right application security testing tools, balancing SAST and DAST, and focusing on tool proficiency and configuration. They discuss the need for tool integration, customization, and understanding the process behind the tools. The speaker also touches on false positives, measuring tool performance, tool accuracy, and the importance of tool customization and process understanding.


## Sandboxing in Linux with zero lines of code - Ignat Korchagin - NDC Security 2024

URL: [https://www.youtube.com/watch?v=4LNpCsw5Pkw](https://www.youtube.com/watch?v=4LNpCsw5Pkw)

 - Ignat Korchagin discussed sandboxing in Linux without writing any code, focusing on the Linux kernel and its security features.
- He emphasized the importance of understanding system calls and their role in separating user space (application logic) from kernel space (system calls and kernel functions).
- Korchagin introduced seccomp, a Linux security feature that allows enforcing system call policies to restrict what a process can do.
- He demonstrated how to use setcom to create policies and deny lists, which can be applied to processes using the systemd unit file or by preloading libraries with LD_PRELOAD.
- The sandboxing toolkit consists of two parts: a shared library for dynamic linking and a special launcher for statically linked applications.
- Korchagin suggested using the lip sandboxing approach, which provides tighter access control and a smaller allow list, reducing the attack surface for applications.
- He recommended using the lipac comp library, contributing to its GitHub project, and referring to the documentation for seccomp, systemd, and filter directives for further understanding and implementation.


## Distribu-ready with the Modular Monolith - Layla Porter - NDC London 2024

URL: [https://www.youtube.com/watch?v=P7gJ9Lo0VrE](https://www.youtube.com/watch?v=P7gJ9Lo0VrE)

 In the transcript, Laia Porter discusses the concept of a Modular Monolith in distributed systems and software architecture. Here are the key points:

- The Modular Monolith is an architecture pattern that balances the benefits of both monolithic and microservices architectures.
- Modular Monolith allows the encapsulation of individual modules within a single executable, making it easier to deploy, scale, and maintain.
- The Kit Kat pattern is introduced, where each module is like a chocolate biscuit that can be swapped or updated independently.
- Loose coupling and high cohesion are important in this architecture to maintain a clear separation between modules.
- The Satellite Pattern is suggested for incrementally moving towards a distributed system, where each module is a self-contained application.
- The speaker discusses the importance of pragmatism in software architecture, balancing the need for flexibility with technical debt and complexity.
- They also mention the need for discipline, such as code reviews, pair programming, automated testing, and adhering to best practices.
- The transcript ends with a recommendation to explore the topics further through reading, video tutorials, and attending conferences.

In summary, Laia Porter's talk explores the concept of Modular Monolith architecture, discussing its benefits and how it can be incrementally adopted in a distributed system. The speaker emphasizes the importance of pragmatism, discipline, and encapsulation in this architecture pattern.


## So You Want to Build An Event Driven System? - James Eastham - NDC London 2024

URL: [https://www.youtube.com/watch?v=qcJASFx-F5g](https://www.youtube.com/watch?v=qcJASFx-F5g)

 - Event-driven architecture: A design pattern that uses events as the primary mechanism for communication between microservices.
- Core idea: Instead of using request/response communication, components in an event-driven system communicate by publishing and subscribing to events.
- Benefits: Enables loose coupling, increased scalability, and faster event processing.
- Challenges: Eventual consistency, handling events that fail to be published, and managing a large number of events.
- EDA (Event-Driven Architecture): A system design that focuses on event communication between components.
- CQRS: A pattern for splitting a system into two completely independent services - one for command processing (CRUD operations) and another for querying (read-only operations).
- Outbox pattern: A pattern to handle failed transactions by storing uncommitted data in a separate table (outbox) and using CQRS to deal with the eventual consistency.
- Change Data Capture (CDC): A technique that captures changes in the database and publishes them as events. This allows for real-time read operations and optimizing data structure for specific use cases.
- Observability: Important in event-driven systems to understand the cause and effect of events.
- Event storming: An event-driven design workshop where participants collaboratively model the system using events.
- Event Ren: Refers to the event architecture, a communication pattern way of modeling integration in a system.
- Event architecture: A way of thinking about integration, decoupling, and communication in a system.
- Event-driven communication: A communication model where components communicate by publishing and reacting to events.
- Event storming workshop: A collaborative workshop to model an event-driven system.
- EDA vs. Traditional monolithic architecture: EDA can be used in both monolithic systems and microservices systems.
- Event-driven microservices: A microservices architecture where services communicate using event-driven communication.
- Event bus: A component in an event-driven system that facilitates event routing.
- Metadata and data schema: In an event-driven system, events often contain metadata and data payload.
- Eventual consistency: A consistency model where eventually all replicas will have the same state, but at any given time, different replicas may have different states.
- Synchronous and asynchronous communication: Asynchronous communication (like events) allows components to operate independently without waiting for a response.

Key Takeaways:
- Event-driven architecture is a design pattern that promotes loose coupling and increased scalability.
- Event storming is a collaborative workshop for modeling an event-driven system.
- Event-driven microservices communication model helps create systems that can evolve and scale rapidly.
- Eventual consistency is a consistency model where eventually all replicas will have the same state.
- CQRS and outbox pattern are techniques to handle events and data storage in an event-driven system.


## Technical Neglect - Kevlin Henney - NDC London 2024

URL: [https://www.youtube.com/watch?v=9iLxR1h2208](https://www.youtube.com/watch?v=9iLxR1h2208)

 Here are the main points from the transcript:

1. Technical debt is often misinterpreted and misused.
2. Technical debt is not always a negative concept.
3. Technical debt can be a metaphor for other things in the real world.
4. The term "technical debt" has evolved over time.
5. Technical debt has changed meaning, and the term is often used to describe things in the software development process.
6. Technical debt is influenced by the audience, context, and the way we use language.
7. Technical debt is a way to describe a complex issue in an understandable way.
8. Technical debt is often used to describe a problem that might be a consequence of poor decisions.
9. Technical debt can be a consequence of neglect and a lack of understanding.
10. Technical debt is not always a bad thing, and it can be a result of taking shortcuts.
11. Technical debt can be an artifact of poor communication and misunderstanding.
12. Technical debt can be a consequence of not addressing a problem.
13. Technical debt can be a consequence of not having a good understanding of the codebase.
14. Technical debt can be an artifact of the way we use metaphors.
15. Technical debt can be a reflection of the way we think and how we communicate.
16. Technical debt can be influenced by the environment, tools, and the way we work.
17. Technical debt can be influenced by the way we prioritize requirements.
18. Technical debt is a way to describe the accumulation of problems that tend to get worse over time.
19. Technical debt can be influenced by the way we prioritize work.
20. Technical debt is a way to describe the accumulation of problems that tend to get worse over time.
21. Technical debt can be influenced by the way we prioritize work.
22. Technical debt can be influenced by the way we prioritize work.
23. Technical debt can be a consequence of not addressing an issue, and often becomes worse with time.
24. Technical debt can be influenced by the way we prioritize work.
25. Technical debt can be influenced by the way we prioritize work.
26. Technical debt can be influenced by the way we prioritize work.
27. Technical debt can be influenced by the way we prioritize work.
28. Technical debt can be influenced by the way we prioritize work.
29. Technical debt can be influenced by the way we prioritize work.
30. Technical debt can be influenced by the way we prioritize work.
31. Technical debt can be influenced by the way we prioritize work.
32. Technical debt can be influenced by the way we prioritize work.
33. Technical debt can be influenced by the way we prioritize work.
34. Technical debt can be influenced by the way we prioritize work.
35. Technical debt can be influenced by the way we prioritize work.
36. Technical debt can be influenced by the way we prioritize work.
37. Technical debt can be influenced by the way we prioritize work.
38. Technical debt can be influenced by the way we prioritize work.
39. Technical debt can be influenced by the way we prioritize work.
40. Technical debt can be influenced by the way we prioritize work.
41. Technical debt can be influenced by the way we prioritize work.
42. Technical debt can be influenced by the way we prioritize work.
43. Technical debt can be influenced by the way we prioritize work.
44. Technical debt can be influenced by the way we prioritize work.
45. Technical debt can be influenced by the way we prioritize work.
46. Technical debt can be influenced by the way we prioritize work.
47. Technical debt can be influenced by the way we prioritize work.
48. Technical debt can be influenced by the way we prioritize work.
49. Technical debt can be influenced by the way we prioritize work.
50. Technical debt can be influenced by the way we prioritize work.
51. Technical debt can be influenced by the way we prioritize work.
52. Technical debt can be influenced by the way we prioritize work.
53. Technical debt can be influenced by the way we prioritize work.
54. Technical debt can be influenced by the way we prioritize work.
55. Technical debt can be influenced by the way we prioritize work.
56. Technical debt can be influenced by the way we prioritize work.
57. Technical debt can be influenced by the way we prioritize work.
58. Technical debt can be influenced by the way we prioritize work.
59. Technical debt can be influenced by the way we prioritize work.
60. Technical debt can be influenced by the way we prioritize work.
61. Technical debt can be influenced by the way we prioritize work.
62. Technical debt can be influenced by the way we prioritize work.
63. Technical debt can be influenced by the way we prioritize work.
64. Technical debt can be influenced by the way we prioritize work.
65. Technical debt can be influenced by the way we prioritize work.
66. Technical debt can be influenced by the way we prioritize work.
67. Technical debt can be influenced by the way we prioritize work.
68. Technical debt can be influenced by the way we prioritize work.
69. Technical debt can be influenced by the way we prioritize work.
70. Technical debt can be influenced by the way we prioritize work.
71. Technical debt can be influenced by the way we prioritize work.
72. Technical debt can be influenced by the way we prioritize work.
73. Technical debt can be influenced by the way we prioritize work.
74. Technical debt can be influenced by the way we prioritize work.
75. Technical debt can be influenced by the way we prioritize work.
76. Technical debt can be influenced by the way we prioritize work.
77. Technical debt can be influenced by the way we prioritize work.
78. Technical debt can be influenced by the way we prioritize work.
79. Technical debt can be influenced by the way we prioritize work.
80. Technical debt can be influenced by the way we prioritize work.
81. Technical debt can be influenced by the way we prioritize work.
82. Technical debt can be influenced by the way we prioritize work.
83. Technical debt can be influenced by the way we prioritize work.
84. Technical debt can be influenced by the way we prioritize work.
85. Technical debt can be influenced by the way we prioritize work.
86. Technical debt can be influenced by the way we prioritize work.
87. Technical debt can be influenced by the way we prioritize work.
88. Technical debt can be influenced by the way we prioritize work.
89. Technical debt can be influenced by the way we prioritize work.
90. Technical debt can be influenced by the way we prioritize work.
91. Technical debt can be influenced by the way we prioritize work.
92. Technical debt can be influenced by the way we prioritize work.
93. Technical debt can be influenced by the way we prioritize work.
94. Technical debt can be influenced by the way we prioritize work.
95. Technical debt can be influenced by the way we prioritize work.
96. Technical debt can be influenced by the way we prioritize work.
97. Technical debt can be influenced by the way we prioritize work.
98. Technical debt can be influenced by the way we prioritize work.
99. Technical debt can be influenced by the way we prioritize work.
100. Technical debt can be influenced by the way we prioritize work.
101. Technical debt can be influenced by the way we prioritize work.
102. Technical debt can be influenced by the way we prioritize work.
103. Technical debt can be influenced by the way we prioritize work.
104. Technical debt can be influenced by the way we prioritize work.
105. Technical debt can be influenced by the way we prioritize work.
106. Technical debt can be influenced by the way we prioritize work.
107. Technical debt can be influenced by the way we prioritize work.
108. Technical debt can be influenced by the way we prioritize work.
109. Technical debt can be influenced by the way we prioritize work.
110. Technical debt can be influenced by the way we prioritize work.
111. Technical debt can be influenced by the way we prioritize work.
112. Technical debt can be influenced by the way we prioritize work.
113. Technical debt can be influenced by the way we prioritize work.
114. Technical debt can be influenced by the way we prioritize work.
115. Technical debt can be influenced by the way we prioritize work.
116. Technical debt can be influenced by the way we prioritize work


## Kafka for .NET Developers - Ian Cooper - NDC London 2024

URL: [https://www.youtube.com/watch?v=DrocPyaJX7Q](https://www.youtube.com/watch?v=DrocPyaJX7Q)

 - Introduction to CFA (Confluent, Inc.)
- CFA's architecture, messaging, and features
- CFA in production vs local development environments
- CFA producers and consumers
- CFA's single-threaded reactive single-producer/single-consumer pattern
- Consistent Hashing and CFA's approach to load balancing
- CFA's Kafka producer and consumer APIs
- CFA's producer/consumer groups, topic partitioning, and leader election
- CFA's producer/consumer groups, topic partitioning, and leader election
- CFA's REST API for Kafka configuration
- CFA's Kafka UI and CFA-Docker UI
- CFA's schema registry, serializers, and deserializers
- CFA's Kafka Connect integration for data engineering
- CFA's use in real-time stream processing applications
- CFA's compatibility with Zookeeper (3.3+) and without Zookeeper (3.4+)
- CFA's role in data pipelines and messaging queues
- CFA's integration with popular open-source frameworks
- CFA's flexibility and scalability for various use cases
- Demo on CFA's producer/consumer interactions and configurations
- CFA's impact on messaging workflows and reliability
- CFA's role in building scalable, reliable systems

In summary, CFA (Confluent, Inc.) is a popular open-source framework for building scalable, reliable messaging systems. It provides a unified, high-level API for working with Kafka and other distributed systems. CFA's key features include its single-threaded, reactive producer/consumer pattern, consistent hashing for load balancing, and its schema registry for managing serializers and deserializers. CFA can be integrated with popular open-source frameworks like Flink and Kafka Connect for data engineering tasks, and it can be used with REST APIs for configuration. CFA's Kafka UI and CFA-Docker UI provide useful tools for working with Kafka topics and configurations. The talk also covers CFA's compatibility with Zookeeper, its role in data pipelines and messaging queues, and its use in real-time stream processing applications.


## Moving IO to the edges of your app: Functional Core, Imperative Shell - Scott Wlaschin

URL: [https://www.youtube.com/watch?v=P1vES9AgfC4](https://www.youtube.com/watch?v=P1vES9AgfC4)

 - Avoid IO core - Keep IO at the edges - Use pure code: deterministic, explicit input/output, no magic numbers coming from nowhere - Use deterministic side effects: functional core, imperative shell - Use special data types for decisions - Prefer domain-driven design - Use dependency rejection: keep IO and business logic separate - Prefer composition over inheritance - Avoid interface creep: keep interfaces small - Use parameterization: pass individual focused functions - Use interpreters: separate IO and business logic - Encourage domain-specific languages - Leverage microservices: isolate changes, make them independent - Use the interpreter pattern: write code that interprets instructions - Keep IO at the edge: use IO in the outermost layer - Use dependency injection: inject business logic into IO code - Use composition over inheritance: combine smaller pieces to create larger functionality - Avoid interface creep: keep interfaces small and focused - Use parameterization: pass individual focused functions - Use interpreters: separate IO and business logic - Encourage domain-specific languages - Leverage microservices: isolate changes, make them independent - Use the interpreter pattern: write code that interprets instructions - Keep IO at the edge: use IO in the outermost layer - Use dependency injection: inject business logic into IO code - Use composition over inheritance: combine smaller pieces to create larger functionality - Avoid interface creep: keep interfaces small and focused - Use parameterization: pass individual focused functions - Use interpreters: separate IO and business logic - Encourage domain-specific languages - Leverage microservices: isolate changes, make them independent - Use the interpreter pattern: write code that interprets instructions - Keep IO at the edge: use IO in the outermost layer - Use dependency injection: inject business logic into IO code - Use composition over inheritance: combine smaller pieces to create larger functionality

Note: The summary provided above is a concise, context-based summary of the provided transcript. However, it should be noted that the transcript itself is already quite concise and well-structured, making it a good standalone resource for understanding the concepts discussed.


## You Keep Using That Word: Asynchronous And Interprocess Comms - Sam Newman - NDC London 2024

URL: [https://www.youtube.com/watch?v=x-MOtcat1iE](https://www.youtube.com/watch?v=x-MOtcat1iE)

 In this talk, Sam Newman discusses the concepts of asynchronous communication, synchronous communication, and the terms' relationship to one another. He emphasizes the importance of understanding the context of these terms when discussing system architecture and interprocess communication. Key points covered include:

- Asynchronous communication: A method where messages are sent without requiring an immediate response. It helps in decoupling the sender and receiver and allows for better handling of latency and parallel processing.
- Synchronous communication: A method where messages are sent and received in a sequence, often requiring an immediate response. It can lead to tight coupling between the sender and receiver.
- Event-driven communication: A type of asynchronous communication where the sender sends data, and the receiver processes it at a later time, often in response to an event.
- Nonblocking and blocking calls: These terms refer to the behavior of a call in a system, where blocking calls are those that wait for a response before continuing, and nonblocking calls proceed without waiting for a response.
- Intermediaries: These are elements that help manage the communication between services, often used to handle temporal decoupling and improve system resilience.
- The Reactive Manifesto: A document that outlines principles for creating reactive systems, which often rely on asynchronous communication.
- Terminology: Sam argues that the terms "asynchronous" and "synchronous" have caused confusion, and that better understanding of context can help clarify discussions around system architecture and interprocess communication.

In summary, Sam Newman discusses the nuances of asynchronous and synchronous communication, emphasizing the importance of context and clear definitions when designing and discussing systems. The talk highlights the role of intermediaries, event-driven communication, and the Reactive Manifesto in building resilient and scalable systems.


