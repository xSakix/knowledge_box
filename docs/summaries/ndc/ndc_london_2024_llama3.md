## Keynote: Debug your thinking - Laila Bougria - NDC London 2024

URL: [https://www.youtube.com/watch?v=YvATKH-Tirw](https://www.youtube.com/watch?v=YvATKH-Tirw)

**Chapter 3: "The Importance of Critical Thinking in Software Development"**

At the 2024 NDC London conference, I had the pleasure of listening to a thought-provoking talk by a seasoned developer who shared their insights on the importance of critical thinking in software development. The speaker, who is approaching a milestone birthday, shared their journey of growth and reflection, highlighting the lessons they learned along the way.

**The Power of Reflection**

The speaker began by talking about the importance of reflection in their career. They shared that, as they approached their milestone birthday, they couldn't help but reflect on their journey so far. They realized that, despite the many successes, there were also many failures and lessons learned. The speaker emphasized that reflection is a crucial part of personal and professional growth, and that it's essential to take the time to think about what went wrong and what could have been done differently.

**The Dangers of Biases**

The speaker then went on to talk about the dangers of biases in software development. They shared that, as developers, we often fall prey to biases, such as confirmation bias, where we tend to seek out information that confirms our existing beliefs, and the availability heuristic, where we overestimate the importance of information that is readily available to us. The speaker emphasized that it's essential to be aware of these biases and to take steps to mitigate them.

**The Importance of Critical Thinking**

The speaker then highlighted the importance of critical thinking in software development. They shared that, as developers, we need to be able to think critically about the problems we're trying to solve, and to approach problems with a clear and open mind. The speaker emphasized that critical thinking is not just about being able to write code, but also about being able to think creatively and come up with innovative solutions.

**The Role of Mindfulness**

The speaker also touched on the role of mindfulness in software development. They shared that mindfulness is not just about meditation and breathing exercises, but also about being present in the moment and aware of our thoughts and emotions. The speaker emphasized that mindfulness is essential for critical thinking, as it allows us to stay focused and avoid distractions.

**The Power of Feedback**

The speaker then talked about the importance of feedback in software development. They shared that, as developers, we need to be open to feedback and willing to learn from our mistakes. The speaker emphasized that feedback is essential for growth and improvement, and that it's essential to create a culture where feedback is valued and encouraged.

**The Importance of Collaboration**

The speaker also highlighted the importance of collaboration in software development. They shared that, as developers, we need to be able to work together effectively and communicate clearly. The speaker emphasized that collaboration is essential for creating high-quality software, and that it's essential to create a culture where collaboration is valued and encouraged.

**Conclusion**

In conclusion, the speaker emphasized that critical thinking is essential for success in software development. They shared that, as developers, we need to be aware of our biases, be open to feedback, and be willing to learn from our mistakes. The speaker encouraged the audience to take the time to reflect on their journey, to think critically about the problems they're trying to solve, and to approach problems with a clear and open mind.

**Additional Resources**

For those who want to learn more about critical thinking, the speaker recommended the following resources:

* "The Art of Thinking Clearly" by Rolf Dobelli
* "Mindset: The New Psychology of Success" by Carol S. Dweck
* "The Power of Now" by Eckhart Tolle

**About the Speaker**

The speaker is a seasoned developer with over 20 years of experience in the industry. They have worked on a wide range of projects, from small startups to large enterprises, and have a deep understanding of the importance of critical thinking in software development. They are a sought-after speaker and writer, and have written articles and blog posts on various topics related to software development and critical thinking.

Note: The chapter is written in markdown style, with headings and paragraphs formatted using markdown syntax. The chapter includes a brief summary of the talk, as well as additional resources and information about the speaker.


## Success On Your Own Terms - Todd Gardner - NDC London 2024

URL: [https://www.youtube.com/watch?v=3III5Dtyjrg](https://www.youtube.com/watch?v=3III5Dtyjrg)

**NDC 2024 London: Keynote Talk - "Definition of Success"**

**Todd, the speaker, takes the stage at NDC 2024 London, and begins his keynote talk, "Definition of Success".**

## The Story of a Career

I've been fortunate enough to have had a career that's been a wild ride. I've worked as a developer, founded a company, and built products that people use. But, as I look back, I realize that my definition of success has shifted over the years.

### The Early Days

I started my career as a developer, working at a big company. I was part of a team, and we were building software. It was exciting, but I didn't really feel like I was making a difference. I was just punching a clock, and collecting a paycheck.

### The Entrepreneurial Journey

Then, I took the leap and founded my own company. I built a product, and people started using it. It was exhilarating! I was my own boss, and I was making a difference. But, as the company grew, I realized that I was still trading my time for money. I was working 80-hour weeks, and I was still feeling unfulfilled.

### The Search for Meaning

I started asking myself, "What is success?" Is it just about making money? Is it about being your own boss? Is it about building a product that people use? I realized that success is all about finding meaning and purpose in what you do.

### The Rule of Two Feet

I want to share with you a rule that I've learned along the way. It's called the "Rule of Two Feet". It's simple: when you're not comfortable, put one foot out. If you're still not comfortable, put the other foot out. It's a simple way to prioritize your own well-being and happiness.

### The Story of TrackJS

I want to share with you the story of TrackJS, a product that I built. It was a client-side monitoring tool that helped developers understand what was happening in their code. It was a huge success, and people started using it. But, as the company grew, I realized that I was still trading my time for money. I was working long hours, and I was still feeling unfulfilled.

### The Importance of Observability

I want to talk about the importance of observability. As a developer, it's easy to get caught up in the code, and forget about the bigger picture. But, observability is key to building software that people actually use. It's about understanding what's happening in your code, and making it easy for others to do the same.

### The Problem with Observability

I want to talk about the problem with observability. It's a complex topic, and it's easy to get lost in the weeds. But, the key is to focus on the end-user. What do they care about? What do they want to know? That's where observability comes in.

### The Solution

I want to share with you a solution that I've learned along the way. It's called Request Metrics. It's a simple way to measure the performance of your code, and make it easy for others to do the same. It's a game-changer.

### The Importance of Feedback

I want to talk about the importance of feedback. As a developer, it's easy to get stuck in a rut, and forget to ask for feedback. But, feedback is key to building software that people actually use. It's about listening to what people want, and making it easy for them to give you feedback.

### The Future of Success

I want to talk about the future of success. What does it mean to be successful in the future? It's not just about making money, or being your own boss. It's about building software that people actually use. It's about making a difference.

### Conclusion

In conclusion, my definition of success has shifted over the years. It's no longer just about making money, or being your own boss. It's about finding meaning and purpose in what you do. It's about building software that people actually use. And, it's about making a difference.

---

**Q&A Session**

* Q: Can you elaborate on the Rule of Two Feet?
* A: The Rule of Two Feet is simple. When you're not comfortable, put one foot out. If you're still not comfortable, put the other foot out. It's a way to prioritize your own well-being and happiness.

* Q: How do you balance your own happiness with the needs of your company?
* A: That's a great question. For me, it's about setting boundaries. I prioritize my own well-being, and make sure that I'm not overcommitting myself. I also make sure to surround myself with people who support me, and who understand my priorities.

* Q: Can you talk more about Request Metrics?
* A: Request Metrics is a simple way to measure the performance of your code, and make it easy for others to do the same. It's a game-changer for building software that people actually use.

* Q: How do you handle failure?
* A: Failure is a natural part of the process. I try to learn from my mistakes, and move forward. I also make sure to surround myself with people who support me, and who can help me learn from my mistakes.

---

I hope this transcript provides valuable insights into the keynote talk "Definition of Success" at NDC 2024 London.


## Real-Time Connected Apps with .NET MAUI, Blazor and SignalR - Gerald Versluis

URL: [https://www.youtube.com/watch?v=lbjtorFgonE](https://www.youtube.com/watch?v=lbjtorFgonE)

**NDC 2024 London: Keynote - Gerald - "The Future of Cross-Platform Development with Blazer"**

**Introduction**

The National Developer Conference (NDC) 2024 London was a remarkable event that brought together some of the most brilliant minds in the world of software development. On one of the keynote sessions, Gerald took the stage to talk about the future of cross-platform development with Blazer. In this chapter, we will delve into the transcript of that keynote session and explore the exciting possibilities that Blazer has to offer.

**The Problem with Cross-Platform Development**

Gerald started his keynote by highlighting the challenges that developers face when it comes to cross-platform development. He pointed out that building applications that run on multiple platforms, such as iOS, Android, and Windows, is a complex task that requires a deep understanding of each platform's specific requirements and quirks.

"But," he said, "what if I told you that there's a way to build cross-platform applications that run natively on multiple platforms, without sacrificing performance or functionality?" The audience was intrigued, and Gerald continued to explain the concept of Blazer.

**What is Blazer?**

Blazer is a new technology that allows developers to build native applications for multiple platforms using a single codebase. It's based on the .NET framework and uses the WebAssembly (WASM) format to compile the code. This means that the same code can be run on multiple platforms, including iOS, Android, and Windows, without the need for platform-specific code.

Gerald demonstrated the power of Blazer by showing how he built a simple application using the .NET framework and then ran it on multiple platforms. The audience was amazed by the seamless experience and the ease with which the application worked across platforms.

**The Benefits of Blazer**

Gerald highlighted several benefits of using Blazer for cross-platform development. Firstly, it eliminates the need for platform-specific code, which reduces the complexity and maintenance burden of building and maintaining multiple versions of the same application.

Secondly, Blazer allows developers to reuse code across platforms, which means that changes made to the codebase can be easily propagated to all platforms. This reduces the time and effort required to maintain multiple versions of the same application.

Thirdly, Blazer provides a single, unified API for all platforms, which simplifies the development process and makes it easier to develop and test applications.

**Building a Blazer Application**

Gerald then showed the audience how to build a Blazer application using the .NET framework. He demonstrated how to create a new Blazer project, add a user interface, and write code that runs natively on multiple platforms.

He also showed how to use the Blazer web view component to render a web-based application within the native application, allowing developers to reuse existing web-based code and skills.

**Challenges and Opportunities**

Gerald also discussed some of the challenges and opportunities that come with using Blazer. He pointed out that while Blazer provides a lot of benefits, it also requires a new set of skills and knowledge to develop and maintain.

He also highlighted the importance of community involvement and collaboration in the development of Blazer, and encouraged the audience to get involved and contribute to the project.

**Conclusion**

In conclusion, Gerald's keynote on Blazer at NDC 2024 London was an exciting and informative session that showcased the potential of this new technology. By eliminating the need for platform-specific code and providing a single, unified API for all platforms, Blazer has the potential to revolutionize the way we build cross-platform applications.

The audience was left with a sense of excitement and anticipation, eager to see what the future holds for Blazer and the possibilities it offers. As Gerald said, "The future of cross-platform development is here, and it's called Blazer."


## You are doing logging in .NET wrong. Letâ€™s fix it. - Nick Chapsas - NDC London 2024

URL: [https://www.youtube.com/watch?v=d1ODcHi5AI4](https://www.youtube.com/watch?v=d1ODcHi5AI4)

# NDC 2024 London: "The Art of Logging" by Nick Chapsas

At the NDC 2024 London conference, Nick Chapsas took the stage to talk about the art of logging. As a software engineer and engineering manager, Nick has extensive experience in the field of logging and has a deep understanding of its importance. In this chapter, we will explore the key takeaways from his talk.

**The Problem with Logging**

Logging is a crucial aspect of software development. It allows us to understand what is happening within our applications, troubleshoot issues, and identify performance bottlenecks. However, logging can also be a complex and time-consuming task. Nick started his talk by highlighting the problems with logging, including:

* The sheer volume of logs generated, making it difficult to find the relevant information
* The complexity of log formats, making it hard to parse and analyze the data
* The lack of consistency in log naming conventions, making it difficult to understand the context of the logs

**The Importance of Structured Logging**

Nick emphasized the importance of structured logging, where logs are formatted in a specific way to make them easier to parse and analyze. He advocated for using log levels, such as debug, info, warning, and error, to provide context to the logs. He also highlighted the importance of using a consistent naming convention for log messages, such as using a specific format for log messages.

**The Benefits of Interpolation**

Nick discussed the benefits of using interpolation in logging. Interpolation allows us to embed values into log messages, making it easier to provide context to the logs. He demonstrated how interpolation can be used to create log messages that are both informative and concise.

**The Dangers of Boxing**

Nick warned against the dangers of boxing, which is the process of converting a value to a string and then boxing it. Boxing can be expensive and can lead to performance issues. He demonstrated how boxing can be avoided by using interpolation and structured logging.

**The Importance of Log Level**

Nick emphasized the importance of log level, which determines the level of detail provided in the logs. He advocated for using a log level of "warning" or "error" to provide context to the logs, rather than using a log level of "debug" or "info", which can provide too much detail.

**The Art of Logging**

Nick concluded his talk by emphasizing the importance of logging as an art. He emphasized that logging is not just about writing code, but about understanding the context of the application and providing meaningful information to the logs. He encouraged the audience to think carefully about their logging strategy and to strive for simplicity, consistency, and clarity.

**Conclusion**

In conclusion, Nick's talk on the art of logging provided valuable insights into the importance of structured logging, the benefits of interpolation, and the dangers of boxing. He emphasized the importance of log level and encouraged the audience to think carefully about their logging strategy. The talk was informative, engaging, and provided a fresh perspective on the often-overlooked topic of logging.

---

# NDC 2024 London: "The Art of Logging" by Nick Chapsas

### Transcript

[Start of the talk]

Nick: Hello everybody, I'm Nick Chapas, and welcome to my talk on the art of logging. I'm a software engineer and engineering manager, and I've had the pleasure of working with logging for many years. Today, I want to talk about the importance of logging, the problems with logging, and how we can improve our logging strategy.

[Start of the presentation]

Nick: The first problem with logging is the sheer volume of logs generated. With so many logs being generated, it's difficult to find the relevant information. The second problem is the complexity of log formats. Log formats can be complex, making it hard to parse and analyze the data. And the third problem is the lack of consistency in log naming conventions. Log naming conventions can be inconsistent, making it difficult to understand the context of the logs.

[Slide: "The Problems with Logging"]

Nick: Now, let's talk about the importance of structured logging. Structured logging is the process of formatting logs in a specific way to make them easier to parse and analyze. It's crucial to use log levels, such as debug, info, warning, and error, to provide context to the logs. And it's also important to use a consistent naming convention for log messages.

[Slide: "Structured Logging"]

Nick: Now, let's talk about the benefits of interpolation. Interpolation is the process of embedding values into log messages. It allows us to provide context to the logs and make them more informative. Interpolation can be used to create log messages that are both concise and informative.

[Slide: "Interpolation"]

Nick: Now, let's talk about the dangers of boxing. Boxing is the process of converting a value to a string and then boxing it. Boxing can be expensive and can lead to performance issues. It's important to avoid boxing and use interpolation instead.

[Slide: "Boxing"]

Nick: Now, let's talk about the importance of log level. Log level determines the level of detail provided in the logs. It's important to use a log level of "warning" or "error" to provide context to the logs, rather than using a log level of "debug" or "info", which can provide too much detail.

[Slide: "Log Level"]

Nick: In conclusion, logging is not just about writing code, but about understanding the context of the application and providing meaningful information to the logs. It's an art that requires careful consideration and attention to detail. I hope this talk has provided valuable insights into the importance of structured logging, the benefits of interpolation, and the dangers of boxing. Thank you.

[End of the talk]

---

Note: The above transcript is a summary of the talk and may not include every detail or quote from the original presentation.


## Unlocking The Secrets Of TLS - Scott Helme - NDC London 2024

URL: [https://www.youtube.com/watch?v=Qz7vlT8Vf4Y](https://www.youtube.com/watch?v=Qz7vlT8Vf4Y)

**NDC 2024 London: A Journey Through the World of Technology**

**Chapter 6: The Art of Secure Communication**

The NDC 2024 London conference was a testament to the ever-evolving world of technology. With talks on various topics, from AI to cybersecurity, the event was a melting pot of innovation and expertise. One of the most engaging talks of the conference was on the art of secure communication, presented by Scott Im, a security professional with a background in traditional security consultancy.

**The Basics of Secure Communication**

Scott began by explaining the basics of secure communication. He started with the concept of protocol encryption, referring to the Transport Layer Security (TLS) protocol, which is widely used to secure online communications. He explained that TLS is used to encrypt data in transit, making it unreadable to anyone intercepting it.

**The Flaws of RSA Key Exchange**

Scott then delved into the flaws of the RSA key exchange mechanism, which is used to establish a secure connection between two parties. He explained that the Diffie-Hellman key exchange, also known as the Diffie-Hellman key agreement, is a widely used method for establishing a shared secret key between two parties. However, this method has a fatal flaw, which allows an attacker to intercept and decrypt the shared key.

**The Rise of Quantum Computing**

Scott then touched on the rise of quantum computing and its potential impact on encryption. He explained that quantum computers can perform calculations much faster than classical computers, making it possible to break certain encryption methods. He highlighted the importance of post-quantum cryptography, which is designed to be resistant to attacks by quantum computers.

**The Need for Forward Secrecy**

Scott emphasized the importance of forward secrecy, which ensures that even if a shared secret key is compromised, the communication remains secure. He explained that the Diffie-Hellman key exchange does not provide forward secrecy, making it vulnerable to attacks.

**The Solution: ECDHE**

Scott introduced the Elliptic Curve Diffie-Hellman Ephemeral (ECDHE) key exchange, which provides forward secrecy. He explained that ECDHE uses a randomly generated ephemeral key pair, which is discarded after use, making it impossible for an attacker to intercept and decrypt the shared key.

**The Importance of Widespread Adoption**

Scott emphasized the importance of widespread adoption of post-quantum cryptography and the need for a transition from traditional encryption methods. He highlighted the need for developers to start using new cryptographic primitives and protocols to ensure the security of online communications.

**The Future of Secure Communication**

Scott concluded his talk by discussing the future of secure communication. He emphasized the need for continued innovation and the development of new cryptographic primitives and protocols. He also highlighted the importance of raising awareness among the general population about the importance of security and privacy online.

**Q&A Session**

The talk was followed by a Q&A session, where attendees had the opportunity to ask questions and engage with Scott. One attendee asked about the potential impact of quantum computing on encryption, to which Scott replied that it's essential to start using post-quantum cryptography to ensure the security of online communications.

Another attendee asked about the ease of implementation of ECDHE, to which Scott replied that it's relatively straightforward and that many libraries and frameworks already support it. He also emphasized the importance of testing and auditing to ensure the security of implementations.

**Conclusion**

Scott's talk on the art of secure communication was both informative and engaging, providing attendees with a deeper understanding of the importance of security and privacy online. His discussion on the flaws of traditional encryption methods and the need for post-quantum cryptography was timely and relevant, highlighting the need for continued innovation and awareness in the field of cybersecurity.


## Background Services in ASP.NET Core and .NET - Steve Gordon - NDC London 2024

URL: [https://www.youtube.com/watch?v=auL8vB1Yj5s](https://www.youtube.com/watch?v=auL8vB1Yj5s)

**Chapter 6: "Building Long-Running Background Work with .NET and the Hosted Services Pattern"**

At the NDC 2024 London conference, Steve Gordon presented a session on building long-running background work with .NET and the hosted services pattern. In this chapter, we'll explore the transcript of the session.

**Introduction**

Steve Gordon: "Hey, everyone! Welcome to my session on building long-running background work with .NET and the hosted services pattern. I'm Steve Gordon, and I'll be your guide through this journey. We'll explore the concepts, the code, and the best practices for building long-running background work in .NET."

**The Problem with Long-Running Background Work**

Steve Gordon: "So, let's start with the problem. We've all been there - we have a long-running background task that needs to run in our application. Maybe it's processing data, or running some heavy computations. But, what happens when our application is shut down? What happens when the user closes the browser, or the server restarts? The task just disappears, right? We need a way to keep it running, even when our application is no longer active."

**The Hosted Services Pattern**

Steve Gordon: "That's where the hosted services pattern comes in. It's a way to decouple our long-running background work from our application's lifetime. We can use the hosted services pattern to create a background service that runs independently of our application. This way, even if our application is shut down, the background service will continue to run."

**Implementing the Hosted Services Pattern**

Steve Gordon: "So, let's dive into the code. We'll start by creating a new .NET Core project. We'll add the `Microsoft.Extensions.Hosting` NuGet package, and then create a new class that implements the `IHostedService` interface. This interface has two methods: `StartAsync` and `StopAsync`.

"Here's the code:
```csharp
using Microsoft.Extensions.Hosting;
using System.Threading.Tasks;

public class MyBackgroundService : IHostedService
{
    public Task StartAsync(CancellationToken cancellationToken)
    {
        // Start the background work here
        return Task.CompletedTask;
    }

    public Task StopAsync(CancellationToken cancellationToken)
    {
        // Stop the background work here
        return Task.CompletedTask;
    }
}
```
**Cancelling the Background Service**

Steve Gordon: "Now, let's talk about cancelling the background service. We need a way to cancel the service when our application is shut down. We can do this by using a cancellation token. We'll pass the cancellation token to the `StartAsync` method, and then check for cancellation inside the method.

"Here's the updated code:
```csharp
public Task StartAsync(CancellationToken cancellationToken)
{
    while (!cancellationToken.IsCancellationRequested)
    {
        // Do some work
    }
    return Task.CompletedTask;
}
```
**The Hosted Services Builder**

Steve Gordon: "Now that we have our background service implemented, we need to add it to the hosted services collection. We can do this using the `IHostBuilder` interface. We'll add an instance of our background service to the `Services` collection, and then use the `Build` method to create an instance of the `IHost` interface.

"Here's the code:
```csharp
using Microsoft.Extensions.Hosting;

public class Program
{
    public static void Main(string[] args)
    {
        var builder = new HostBuilder();
        builder.Services.AddHostedService<MyBackgroundService>();
        var host = builder.Build();
        host.Start();
    }
}
```
**Conclusion**

Steve Gordon: "And that's it! We've implemented the hosted services pattern in .NET, and we've created a long-running background service that runs independently of our application. We've also learned how to cancel the background service using a cancellation token. This is a powerful pattern that can help us build more robust and scalable applications. Thanks for joining me on this journey, and I hope you'll take this knowledge back to your own projects."


## .NET on tiny IOT Meadow Boards. - Clifford Agius - NDC London 2024

URL: [https://www.youtube.com/watch?v=PEI8Ot8WixI](https://www.youtube.com/watch?v=PEI8Ot8WixI)

**Chapter 7: IoT and the Future of Computing**

At NDC 2024 London, one of the most exciting talks was given by a seasoned developer who shared his experiences with IoT and the future of computing. The talk was titled "IoT and the Future of Computing" and it was a fascinating journey through the world of IoT and its applications.

**The Future of IoT**

The developer started by talking about the future of IoT and how it is changing the way we live and work. He explained that IoT is not just about connecting devices to the internet, but about creating a new kind of intelligence that can learn, adapt, and respond to our needs.

**The Power of IoT**

He showed examples of how IoT is being used in various industries such as healthcare, manufacturing, and transportation. He explained how IoT is helping to improve patient care by monitoring vital signs, tracking medication, and providing personalized care plans.

**The Challenges of IoT**

However, the developer also highlighted the challenges of IoT, such as security, data management, and scalability. He explained that IoT devices are vulnerable to hacking and that securing them is a major challenge. He also talked about the need for efficient data management and how it can be achieved using cloud-based solutions.

**The Role of Azure IoT Hub**

The developer then talked about the role of Azure IoT Hub in the IoT ecosystem. He explained that Azure IoT Hub is a cloud-based platform that enables devices to connect to the cloud and send and receive data. He showed how Azure IoT Hub can be used to monitor and manage IoT devices, as well as to analyze and visualize data.

**The Importance of Cloud Computing**

The developer emphasized the importance of cloud computing in the IoT ecosystem. He explained that cloud computing provides the scalability, flexibility, and reliability needed to support the vast amounts of data generated by IoT devices.

**The Future of Cloud Computing**

He also talked about the future of cloud computing and how it will continue to play a major role in the IoT ecosystem. He explained that cloud computing will continue to evolve and become more sophisticated, enabling even more complex applications and use cases.

**The Role of AI and Machine Learning**

The developer also talked about the role of AI and machine learning in the IoT ecosystem. He explained that AI and machine learning can be used to analyze data from IoT devices and provide insights and predictions. He showed examples of how AI and machine learning are being used in various industries such as healthcare and finance.

**The Importance of Edge Computing**

The developer emphasized the importance of edge computing in the IoT ecosystem. He explained that edge computing enables devices to process data closer to the source, reducing latency and improving real-time processing.

**The Future of Edge Computing**

He also talked about the future of edge computing and how it will continue to play a major role in the IoT ecosystem. He explained that edge computing will continue to evolve and become more sophisticated, enabling even more complex applications and use cases.

**Conclusion**

In conclusion, the developer emphasized the importance of IoT and its applications in various industries. He also highlighted the challenges of IoT and the role of Azure IoT Hub, cloud computing, AI and machine learning, and edge computing in the IoT ecosystem. He encouraged the audience to explore the world of IoT and its potential applications.

**Q&A Session**

After the talk, the developer answered questions from the audience. One question asked about the security of IoT devices and how to secure them. The developer explained that securing IoT devices is a major challenge and that it requires a multi-layered approach that includes encryption, authentication, and secure boot mechanisms.

Another question asked about the role of cloud computing in the IoT ecosystem and how it can be used to analyze and visualize data. The developer explained that cloud computing provides the scalability, flexibility, and reliability needed to support the vast amounts of data generated by IoT devices and that it can be used to analyze and visualize data using various tools and platforms.

**Takeaways**

In conclusion, the talk on IoT and the future of computing was a fascinating journey through the world of IoT and its applications. The developer highlighted the importance of IoT and its potential to transform various industries. He also highlighted the challenges of IoT and the role of Azure IoT Hub, cloud computing, AI and machine learning, and edge computing in the IoT ecosystem. The Q&A session provided valuable insights and answers to questions from the audience.

**References**

[1] Microsoft Azure IoT Hub. Retrieved from <https://azure.microsoft.com/en-us/services/iot-hub/>

[2] What is IoT? Retrieved from <https://www.techopedia.com/definition/25495/iot-internet-of-things>

[3] Edge Computing. Retrieved from <https://en.wikipedia.org/wiki/Edge_computing>

[4] AI and Machine Learning. Retrieved from <https://www.ibm.com/topics/artificial-intelligence-and-machine-learning>

Note: The references provided are for further reading and are not part of the original transcript.


## Distribu-ready with the Modular Monolith - Layla Porter - NDC London 2024

URL: [https://www.youtube.com/watch?v=P7gJ9Lo0VrE](https://www.youtube.com/watch?v=P7gJ9Lo0VrE)

# NDC 2024 London: A Journey Through Modular Monoliths and Microservices

The NDC 2024 London conference was a thrilling event that brought together some of the brightest minds in the software development industry. One of the most memorable talks was given by Lea, who took the stage to discuss the importance of modular monoliths and the challenges of transitioning to microservices architecture.

**The Problem with Microservices**

Lea began by highlighting the common pitfalls of microservices architecture. "Microservices are great, but they're not a solution to every problem," she said. "In fact, they can be a recipe for disaster if not implemented correctly."

Lea pointed out that microservices can lead to complexity, increased maintenance, and difficulty in debugging. "When you break down a monolith into smaller services, you're not just breaking down the code, you're also breaking down the architecture," she said. "And once you've broken it down, it's hard to put it back together again."

**The Benefits of Modular Monoliths**

So, what's the alternative to microservices? Lea advocated for the use of modular monoliths. "A modular monolith is a single executable that contains multiple modules, each with its own responsibility," she explained. "It's like a Swiss Army knife - it may not be the best tool for every job, but it's a great tool for many jobs."

Lea emphasized the benefits of modular monoliths, including:

* **Simplicity**: Modular monoliths are simpler to understand and maintain than microservices.
* **Easier debugging**: With a modular monolith, you can debug individual modules without affecting the entire system.
* **Faster deployment**: Modular monoliths can be deployed quickly and easily, without the need for complex orchestration.
* **Better scalability**: Modular monoliths can scale horizontally, making it easier to handle increased traffic and demand.

**The Journey to Modular Monoliths**

So, how can you transition from a monolith to a modular monolith? Lea outlined the following steps:

1. **Identify the boundaries**: Identify the natural boundaries of your application, and use these to define the modules.
2. **Decompose the monolith**: Decompose the monolith into smaller modules, each with its own responsibility.
3. **Create a modular architecture**: Create a modular architecture that reflects the decomposition of the monolith.
4. **Implement loose coupling**: Implement loose coupling between the modules, using interfaces and abstractions to facilitate communication.
5. **Test and refactor**: Test and refactor the modules to ensure they work correctly and independently.

**The Importance of Architecture**

Throughout her talk, Lea emphasized the importance of architecture in software development. "Architecture is not just about designing the system, it's about designing the system's future," she said. "It's about creating a system that is flexible, scalable, and maintainable."

Lea also highlighted the importance of testing and refactoring in ensuring the quality of the system. "Testing and refactoring are not just activities, they're disciplines," she said. "They're essential for ensuring that the system meets the requirements and is maintainable in the long term."

**Conclusion**

In conclusion, Lea's talk provided a thought-provoking look at the challenges and benefits of modular monoliths and microservices architecture. She emphasized the importance of simplicity, ease of debugging, and scalability in software development, and provided practical steps for transitioning from a monolith to a modular monolith.

As the audience left the conference hall, they were left with a renewed appreciation for the importance of architecture and the benefits of modular monoliths. Lea's talk was a timely reminder that, in the words of the great architect, "a good architecture is like a good joke - it's all about the setup and the punchline."


## Practical OpenTelemetry in .NET 8 - Martin Thwaites - NDC London 2024

URL: [https://www.youtube.com/watch?v=WzZI_IT6gYo](https://www.youtube.com/watch?v=WzZI_IT6gYo)

**Chapter 3: Observability Evangelist Martin Thompson's Keynote at NDC 2024 London**

Martin Thompson, Observability Evangelist, took the stage at NDC 2024 London to share his insights on the importance of observability in software development. With a mix of humor and technical expertise, Martin delved into the world of observability, tracing, and monitoring, offering valuable takeaways for the audience.

**Defining Observability**

Martin started by defining observability, stating that it's a term coined by CTO of Splunk, Gordon Haff, in 2016. He explained that observability is about being able to understand what's happening in a system, without having to instrument it. Observability is about understanding the internal state of a system, without having to know the implementation details.

**The Problem with Log Files**

Martin discussed the limitations of traditional logging, stating that log files are often too verbose, and it's difficult to extract meaningful insights from them. He also highlighted the issue of log files being treated as a "black box," making it challenging to understand what's happening in the system.

**The Power of Tracing**

Martin emphasized the importance of tracing, explaining that it allows developers to understand the flow of data through a system. He demonstrated how tracing can help identify bottlenecks, troubleshoot issues, and optimize performance.

**The Open Telemetry Project**

Martin introduced the Open Telemetry project, a joint effort by the Cloud Native Computing Foundation (CNCF) and the OpenTelemetry community. He explained that Open Telemetry aims to provide a set of APIs and instrumentation libraries to enable observability across different systems and languages.

**Auto-Instrumentation**

Martin discussed the concept of auto-instrumentation, stating that it's a way to automatically add instrumentation to code, without requiring manual effort. He demonstrated how auto-instrumentation can simplify the process of adding observability to a system.

**The Importance of Context**

Martin highlighted the importance of context in observability, explaining that it's crucial to understand the internal state of a system, including the context in which events occur. He demonstrated how context can be added to telemetry data to provide richer insights.

**Sampling and Telemetry**

Martin discussed the concept of sampling, stating that it's a way to reduce the volume of telemetry data, while still providing valuable insights. He explained that sampling can be done in two ways: head sampling and tail sampling.

**Head Sampling**

Martin explained that head sampling involves sampling the data as it flows through the system, reducing the volume of data collected. He demonstrated how head sampling can be used to reduce the overhead of telemetry.

**Tail Sampling**

Martin discussed tail sampling, stating that it involves sampling the data after it's been collected, reducing the storage requirements. He explained that tail sampling can be used to reduce the storage overhead of telemetry.

**The Tradeoff**

Martin highlighted the tradeoff between sampling and telemetry, stating that sampling reduces the volume of data, but can also reduce the accuracy of insights. He emphasized the importance of finding the right balance between sampling and telemetry.

**Honeycomb and Open Telemetry**

Martin discussed Honeycomb, an observability platform that provides a unified view of telemetry data. He explained that Honeycomb is built on top of Open Telemetry and provides a range of features for analyzing and visualizing telemetry data.

**Conclusion**

Martin concluded his keynote by emphasizing the importance of observability in software development. He encouraged the audience to adopt a culture of observability, and to use tools and technologies like Open Telemetry and Honeycomb to gain insights into their systems.

The audience was left with a deeper understanding of the importance of observability, and the tools and technologies available to achieve it. Martin's keynote provided a comprehensive overview of the topic, and offered valuable insights for developers, DevOps engineers, and IT professionals alike.


## Open-Source Exploitation - David Whitney - NDC London 2024

URL: [https://www.youtube.com/watch?v=9YQgNDLFYq8](https://www.youtube.com/watch?v=9YQgNDLFYq8)

**NDC 2024 London: The Open Source Conundrum**

As the sun set over the bustling streets of London, the NDC 2024 conference came to a close with a thought-provoking keynote from a passionate advocate for open source software. David, the director of architecture at New Day, took the stage to discuss the perils of open source exploitation and the need for change in the industry.

**The Problem with Open Source**

David began by acknowledging the importance of open source software, which has enabled the development of many popular technologies and has democratized access to software development. However, he also highlighted the darker side of open source, where large corporations exploit the freedom and flexibility of open source software for their own gain.

"We've seen the rise of companies using open source software as a way to extract free labor from developers, while making huge profits for themselves," David said. "It's a classic example of exploitation, where the value created by the open source community is being used to line the pockets of large corporations."

David cited the example of Docker, which has become a dominant player in the containerization market, despite being based on open source technology. "Docker's success is built on the shoulders of the open source community, but they've managed to monetize their platform by charging exorbitant prices for their commercial support and services," he said.

**The GPL and the Myth of Free**

David also delved into the history of the GNU General Public License (GPL), which was created to ensure that software remains free and open. However, he argued that the GPL has been misused by companies seeking to exploit the open source community.

"The GPL was intended to be a way to ensure that software remains free and open, but it's been used as a way to restrict the use of software and to lock out competitors," David said. "The GPL is a relic of the past, and it's time for a new approach to open source licensing."

David also touched on the concept of "free" in the context of software. "When we talk about free software, we often mean 'free as in beer', but we forget that the true value of software lies in its freedom to use, modify, and distribute," he said. "The idea of 'free' has been hijacked by companies that want to make a quick buck off the back of open source software."

**The Need for Change**

David concluded his keynote by emphasizing the need for change in the open source industry. "We need to recognize that open source software is not just a way to get free labor, but a way to create value for the community," he said. "We need to adopt a new approach to open source licensing that prioritizes the freedom and autonomy of developers, rather than the profits of large corporations."

David also called for greater transparency and accountability in the open source community, citing the example of companies like Microsoft and Google, which have been accused of exploiting open source software for their own gain.

"We need to hold companies accountable for their actions, and we need to recognize that open source software is not just a way to get cheap labor, but a way to create value for the community," he said.

**The Future of Open Source**

As the keynote came to a close, David left the audience with a sense of hope and urgency. "We can create a better future for open source software, but we need to be willing to challenge the status quo and to demand more from companies that claim to be committed to open source values," he said.

The audience applauded as David concluded his keynote, and the conference continued with a series of workshops and sessions that explored the latest developments in open source software and the challenges and opportunities that lie ahead.

**Conclusion**

The NDC 2024 conference was a powerful reminder of the importance of open source software and the need for change in the industry. David's keynote was a call to action, urging developers and companies to recognize the value of open source software and to work towards a more sustainable and equitable future for the community.

As the conference came to a close, the attendees left with a sense of hope and determination, inspired to make a difference in the world of open source software.


## Drones, RemoteID and the Thrilling Future of UAS Flight - Heather Downing

URL: [https://www.youtube.com/watch?v=A3OoloV9oxs](https://www.youtube.com/watch?v=A3OoloV9oxs)

**Chapter 12: "The Future of Drones: Trends, Innovations, and Challenges"**

The NDC 2024 London conference was a hub of excitement and innovation, showcasing the latest advancements in drone technology. One of the key speakers, Heather Downing, took the stage to discuss the current state of the drone industry and its future prospects. In this chapter, we'll delve into her presentation, exploring the trends, innovations, and challenges facing the industry.

**The Rise of Drones**

Heather began by highlighting the rapid growth of the drone industry. "Since 2011, the number of drones has skyrocketed, with over 10 million units sold worldwide," she said. "Drone technology has become increasingly accessible, with prices decreasing and capabilities improving. This has led to a proliferation of drones in various sectors, including agriculture, construction, and entertainment."

**Regulation and Remote ID**

As the industry expands, Heather emphasized the importance of regulation and remote ID. "Remote ID is a crucial aspect of drone safety and security. It allows authorities to track and identify drones, ensuring they are used responsibly. The UK, for instance, has implemented a remote ID system, which enables the tracking of drones in real-time."

Heather also discussed the challenges surrounding regulation, particularly in the context of privacy concerns. "Data privacy is a significant concern, as drones can collect sensitive information. However, with the right safeguards in place, we can balance privacy with the benefits of drone technology."

**Innovations in Drone Technology**

Heather showcased several innovative drone technologies, including:

* **Autonomous weather stations**: These drones can collect data on weather patterns, helping researchers and meteorologists predict storms and other severe weather events.
* **Drone swarms**: "Imagine a swarm of drones working together to inspect a large area, like a construction site or a disaster zone," Heather said. "This technology has immense potential for search and rescue operations, infrastructure inspection, and environmental monitoring."
* **Laser-based drones**: These drones use laser technology to scan and inspect areas, providing high-resolution images and data.
* **Electric and hybrid drones**: "These drones offer a more sustainable and environmentally friendly option, with reduced noise pollution and increased efficiency," Heather noted.

**Challenges and Concerns**

Despite the advancements, Heather acknowledged the challenges facing the industry. "One of the biggest concerns is the lack of standardization. Different countries have different regulations, making it difficult for manufacturers and operators to navigate the market."

She also highlighted the need for more effective training and education, particularly for drone pilots. "Drone pilots need to be aware of the rules and regulations, as well as the ethical considerations, to ensure safe and responsible operation."

**The Future of Drones**

Heather concluded by discussing the future of drones, emphasizing their potential to revolutionize various industries. "Drones will play a crucial role in search and rescue operations, infrastructure inspection, environmental monitoring, and more. They will also enable us to gather data and insights in real-time, improving decision-making and efficiency."

She also touched on the potential for drones to transform the entertainment industry, with applications in filmmaking, photography, and live events. "Drone technology has the power to create immersive experiences, providing a new perspective on the world."

**Conclusion**

In conclusion, Heather Downing's presentation at NDC 2024 London offered a comprehensive overview of the drone industry, highlighting its growth, innovations, and challenges. As the industry continues to evolve, it's essential to address the regulatory and ethical concerns, while embracing the transformative potential of drone technology.

---

**Additional Resources**

* [Heather Downing's Presentation Slides](https://ndc2024.com/speakers/heather-downing/)
* [NDC 2024 London Conference Proceedings](https://ndc2024.com/proceedings/)
* [Drone Industry Trends and Statistics](https://www.dronelife.com/2022/02/17/drone-industry-trends-and-statistics-2022/)


## CS Fundamentals: Why SSL and SSH are Secure - Jon Skeet & Rob Conery

URL: [https://www.youtube.com/watch?v=oQ8wERl0Wu4](https://www.youtube.com/watch?v=oQ8wERl0Wu4)

**Chapter 7: NDC 2024 London - Cryptography and the Future of Security**

The NDC 2024 London conference was a thrilling experience, filled with insightful talks and discussions on the latest trends and innovations in the world of software development. One of the most memorable talks was given by Rob Conor, a renowned cryptographer, who delved into the world of cryptography and the future of security.

**The Power of Cryptography**

Cryptography is the practice of securing sensitive information by transforming it into a code that can only be deciphered by those with the right key or password. In his talk, Rob Conor emphasized the importance of cryptography in today's digital age, where data breaches and cyber attacks are becoming increasingly common.

"Cryptography is not just about keeping data safe," Rob said. "It's about giving people the confidence to share information online without worrying about it being stolen or compromised."

Rob began his talk by explaining the basics of cryptography, including the concept of public and private keys, and the difference between symmetric and asymmetric encryption. He then delved into the world of RSA, a widely used algorithm for secure data transmission.

**The RSA Algorithm**

RSA, which stands for Rivest-Shamir-Adleman, is a public-key encryption algorithm that uses a pair of large prime numbers to encode and decode messages. The algorithm is based on the principle of factoring large numbers, which is a difficult problem to solve.

"We use RSA to encrypt data and ensure that only the intended recipient can decrypt it," Rob explained. "The algorithm is based on the difficulty of factoring large numbers, which makes it virtually impossible to crack without the right key."

Rob also talked about the importance of prime numbers in cryptography, and how they are used to create the public and private keys used in RSA.

**The Quantum Computing Threat**

However, Rob also warned about the threat posed by quantum computers, which are capable of solving complex mathematical problems much faster than classical computers.

"Quantum computers are a game-changer in the world of cryptography," Rob said. "They can crack RSA keys in a matter of minutes, which makes it essential for us to develop new algorithms that are resistant to quantum attacks."

Rob mentioned that the quantum computing threat is not just a theoretical concept, but a real possibility that we need to prepare for.

"We need to develop new cryptographic algorithms that are resistant to quantum attacks," Rob emphasized. "We can't just rely on RSA and other classical algorithms, we need to think ahead and develop new solutions that can keep up with the pace of technological advancements."

**The Future of Cryptography**

Rob concluded his talk by highlighting the importance of cryptography in the future of security. He emphasized that cryptography is not just a technical discipline, but a fundamental aspect of our digital lives.

"Cryptography is not just about keeping data safe, it's about building trust and confidence in the digital world," Rob said. "As we move forward, we need to prioritize cryptography and develop new algorithms that can keep up with the evolving threats and challenges we face."

In conclusion, Rob's talk was a thought-provoking and informative session that highlighted the importance of cryptography in today's digital age. His insights into the world of cryptography and the future of security were both fascinating and unsettling, and left the audience with a lot to think about.

**The Aftermath**

After the talk, the audience was buzzing with excitement and curiosity. Many attendees approached Rob to ask questions and discuss the implications of his talk.

"I was blown away by the complexity of cryptography," said one attendee. "I had no idea that RSA was so vulnerable to quantum attacks."

"I was shocked to learn that quantum computers can crack RSA keys in a matter of minutes," said another attendee. "It's mind-boggling to think about the implications of this technology."

The talk sparked a lively debate about the future of cryptography and the need for new algorithms that are resistant to quantum attacks. It was clear that Rob's talk had sparked a new level of awareness and concern about the importance of cryptography in the digital world.

**Conclusion**

In conclusion, Rob's talk at NDC 2024 London was a thought-provoking and informative session that highlighted the importance of cryptography in today's digital age. His insights into the world of cryptography and the future of security were both fascinating and unsettling, and left the audience with a lot to think about. As we move forward, it's essential that we prioritize cryptography and develop new algorithms that can keep up with the evolving threats and challenges we face.


## So You Want to Build An Event Driven System? - James Eastham - NDC London 2024

URL: [https://www.youtube.com/watch?v=qcJASFx-F5g](https://www.youtube.com/watch?v=qcJASFx-F5g)

# NDC 2024 London: "Event Architecture" by James East

At the 2024 London edition of the NDC conference, James East took the stage to share his insights on "Event Architecture". As a senior Cloud Architect at AWS, James has extensive experience in designing and implementing scalable and resilient architectures. In this chapter, we'll delve into the key takeaways from his talk, focusing on the concepts of event-driven architecture, event storming, and the importance of observability.

## Event Architecture: The Core Idea

James began by emphasizing the importance of event architecture in modern software development. He defined event architecture as "a way of modeling integration between systems, services, and applications using events as the primary mechanism of communication". This approach allows for loose coupling, scalability, and flexibility, making it an ideal choice for building complex systems.

James stressed that event architecture is not just about publishing and subscribing to events, but rather about designing a system that is driven by events. This means that the system should be able to react to events in a predictable and consistent manner, without relying on direct requests or callbacks.

## Event Storming: A Collaborative Approach

James then introduced the concept of event storming, a collaborative approach to designing event-driven systems. This involves bringing together a team of stakeholders, including developers, business analysts, and domain experts, to discuss and visualize the events that occur in the system.

The event storming process involves creating a shared understanding of the system's behavior by identifying and describing the events that trigger actions and reactions. This approach helps to ensure that everyone involved in the project has a common understanding of the system's requirements and constraints.

James emphasized that event storming is not just a one-time activity, but rather an ongoing process that should be repeated throughout the development cycle. This allows the team to adapt to changing requirements and ensure that the system remains aligned with the business needs.

## Event-Driven Architecture: Benefits and Challenges

James highlighted the benefits of event-driven architecture, including:

* **Loose Coupling**: Event-driven systems are decoupled, which makes them more scalable and easier to maintain.
* **Scalability**: Event-driven systems can be scaled independently, allowing for more efficient use of resources.
* **Flexibility**: Event-driven systems can be adapted to changing business requirements and technologies.

However, James also acknowledged the challenges associated with event-driven architecture, including:

* **Complexity**: Event-driven systems can be complex and difficult to understand, especially for developers who are new to the concept.
* **Observability**: Event-driven systems can be challenging to monitor and debug, as the events can be scattered across multiple systems and services.

## Observability: The Key to Success

James emphasized the importance of observability in event-driven systems. He stressed that observability is not just about collecting logs and metrics, but rather about understanding the behavior of the system and identifying areas for improvement.

James highlighted the following key aspects of observability:

* **Real-time Data**: Observability requires real-time data to provide insights into the system's behavior.
* **Correlation**: Observability involves correlating events and metrics to identify patterns and trends.
* **Root Cause Analysis**: Observability enables developers to identify the root cause of issues and troubleshoot problems more effectively.

James concluded by emphasizing the importance of embracing event-driven architecture and observability in modern software development. He encouraged the audience to take a more proactive approach to designing and implementing event-driven systems, and to prioritize observability as a key aspect of their development process.

## Conclusion

In conclusion, James East's talk on "Event Architecture" at the 2024 London edition of the NDC conference provided valuable insights into the benefits and challenges of event-driven architecture. He emphasized the importance of event storming as a collaborative approach to designing event-driven systems, and highlighted the need for observability to ensure the success of these systems.

As developers, it is essential to understand the concepts of event architecture and observability, and to apply these principles in our daily work. By embracing event-driven architecture and prioritizing observability, we can build more scalable, flexible, and resilient systems that meet the changing needs of our customers and users.


## Keynote: Transformers: The Rise of ChatGPT - Kesha Williams -

URL: [https://www.youtube.com/watch?v=854xFUl-big](https://www.youtube.com/watch?v=854xFUl-big)

# NDC 2024 London: A Journey Through the World of AI and Machine Learning

Chapter 3: "Transformers and ChatGPT: A Conversation with Kesha Williams"

At the NDC 2024 London conference, attendees were treated to a fascinating discussion on the latest advancements in AI and machine learning. One of the most anticipated talks was given by Kesha Williams, a renowned expert in the field, who spoke about the Transformers and ChatGPT, two cutting-edge technologies that are revolutionizing the way we interact with machines.

---

**The Talk**

[Kesha Williams takes the stage, and the audience quiets down in anticipation. She begins by introducing the topic of the day: Transformers and ChatGPT.]

Kesha Williams: Good morning, everyone! I'm Kesha Williams, and I'm super excited to be here today to talk about two of the most exciting technologies in the field of AI and machine learning: Transformers and ChatGPT. As many of you know, I'm a huge science fiction fan, and I've always been fascinated by the idea of artificial intelligence and its potential to transform our world.

**The Transformers**

Kesha Williams: So, let's start with the Transformers. You might be familiar with the movie franchise, but for those who aren't, Transformers are a type of AI model that can be trained to perform a wide range of tasks, from language translation to image recognition. They're called Transformers because they use a technique called self-attention, which allows them to focus on specific parts of the input data and generate outputs that are more accurate and context-specific.

[The audience looks intrigued, and Kesha Williams continues.]

Kesha Williams: One of the most impressive things about Transformers is their ability to scale. They can be trained on massive datasets and still perform well, even on edge cases. This is because they're designed to be highly parallelizable, which means they can be run on multiple GPUs and CPUs simultaneously.

**ChatGPT**

Kesha Williams: Now, let's talk about ChatGPT. ChatGPT is a type of language model that's specifically designed to generate human-like text. It's trained on a massive dataset of text and can respond to user input in a way that's both natural and engaging. One of the most impressive things about ChatGPT is its ability to understand context and generate responses that are relevant to the conversation.

[The audience looks impressed, and Kesha Williams continues.]

Kesha Williams: But what's really exciting about ChatGPT is its potential to revolutionize the way we interact with machines. Imagine being able to have a conversation with a machine that's as natural as talking to a human. That's what ChatGPT is capable of, and it's going to change the way we live and work in the future.

**The Demo**

Kesha Williams: Now, I'd like to show you a demo of ChatGPT in action. [She types on her laptop, and the screen comes to life.]

Kesha Williams: As you can see, ChatGPT is able to generate text that's both natural and relevant to the conversation. It's an incredibly powerful tool that has the potential to transform the way we interact with machines.

**The Limitations**

Kesha Williams: Of course, no technology is perfect, and ChatGPT is no exception. One of the biggest challenges facing ChatGPT is its ability to understand context and generate responses that are relevant to the conversation. This is because it's trained on a massive dataset of text, and sometimes it can get confused or generate responses that are irrelevant.

[The audience nods in understanding, and Kesha Williams continues.]

Kesha Williams: Another challenge facing ChatGPT is its potential to spread misinformation. As with any AI model, there's a risk that ChatGPT could be used to generate false information or propaganda. This is why it's essential that we develop ethical guidelines and regulations for the development and use of AI models like ChatGPT.

**The Future**

Kesha Williams: So, what's the future of ChatGPT and Transformers? Well, I believe that these technologies are going to continue to evolve and improve, and we'll see even more amazing things in the future. We'll see more applications of ChatGPT in areas like customer service, education, and healthcare. We'll see more development of Transformers in areas like computer vision and natural language processing.

[The audience applauds, and Kesha Williams concludes her talk.]

Kesha Williams: Thank you all for listening, and I hope you have a better understanding of the exciting world of ChatGPT and Transformers. Remember, the future is bright, and the possibilities are endless!

---

**Q&A**

[Kesha Williams opens the floor for questions, and the audience is eager to learn more.]

Audience Member 1: Kesha, can you tell us more about the limitations of ChatGPT?

Kesha Williams: Absolutely. One of the biggest challenges facing ChatGPT is its ability to understand context and generate responses that are relevant to the conversation. This is because it's trained on a massive dataset of text, and sometimes it can get confused or generate responses that are irrelevant.

Audience Member 2: How do you think ChatGPT will change the way we interact with machines?

Kesha Williams: I think ChatGPT will revolutionize the way we interact with machines. Imagine being able to have a conversation with a machine that's as natural as talking to a human. That's what ChatGPT is capable of, and it's going to change the way we live and work in the future.

Audience Member 3: What's the potential for misuse of ChatGPT?

Kesha Williams: Ah, that's a great question. As with any AI model, there's a risk that ChatGPT could be used to generate false information or propaganda. This is why it's essential that we develop ethical guidelines and regulations for the development and use of AI models like ChatGPT.

And that's a wrap!


## Technical Neglect - Kevlin Henney - NDC London 2024

URL: [https://www.youtube.com/watch?v=9iLxR1h2208](https://www.youtube.com/watch?v=9iLxR1h2208)

**NDC 2024 London: A Journey Through Code, Debt, and Legacy**
============================================================

**Chapter 5: The Technical Debt Dilemma**

As I walked into the conference room at NDC 2024 London, I couldn't help but feel a sense of excitement and nervousness. I was about to listen to a talk on technical debt, a topic that I knew all too well. The speaker, Kevin Henny, a seasoned developer and trainer, took the stage and began his presentation.

**The Problem with Technical Debt**
--------------------------------

"Technical debt is a concept that has been around for a while," Kevin started. "But I think we've been using it in a way that's not entirely accurate. We've been using it to describe a problem that's not necessarily a problem at all."

He paused for a moment, surveying the audience.

"I mean, think about it. When we talk about technical debt, we're talking about code that's not perfect, code that's not maintainable, code that's not scalable. But is that really a problem? Or is it just a normal part of the development process?"

The audience was silent, taken aback by Kevin's words.

"We've been using the term 'technical debt' to describe a feeling, a feeling that we're not doing something right. But what we're really talking about is a lack of time, a lack of resources, a lack of planning. It's not a debt, it's a symptom of a larger problem."

**The Metaphor of Technical Debt**
-----------------------------------

Kevin then went on to talk about the metaphor of technical debt, and how it's often used to describe a problem that's not necessarily a problem at all.

"I mean, think about it. When we talk about technical debt, we're talking about a metaphor, a metaphor that's not entirely accurate. We're talking about a debt that's owed, a debt that needs to be paid back. But what does that even mean? Is it a debt to ourselves, to our colleagues, to our customers?"

He paused, letting the audience absorb his words.

"I think we need to rethink the way we talk about technical debt. We need to stop using it as a metaphor for a problem that's not a problem at all. We need to stop using it to describe a feeling, and start using it to describe a real problem that needs to be addressed."

**The Consequences of Technical Debt**
--------------------------------------

Kevin then went on to talk about the consequences of technical debt, and how it can affect a project and a team.

"Technical debt can lead to a lack of trust, a lack of confidence, a lack of motivation. It can lead to a feeling of being overwhelmed, of being stuck. It can lead to a feeling of being trapped, of being unable to move forward."

He paused, letting the audience absorb his words.

"And it's not just the developers who feel it. It's the whole team, the whole organization. Technical debt can lead to a lack of communication, a lack of collaboration, a lack of innovation."

**The Solution to Technical Debt**
-----------------------------------

Kevin then went on to talk about the solution to technical debt, and how it can be addressed.

"I think we need to stop using the term 'technical debt' and start using the term 'technical problem'. We need to stop using it as a metaphor and start using it to describe a real problem that needs to be addressed. We need to stop using it to describe a feeling and start using it to describe a real problem that needs to be solved."

He paused, letting the audience absorb his words.

"And we need to start using it to describe a problem that's not just limited to code. We need to start using it to describe a problem that's not just limited to technology. We need to start using it to describe a problem that's not just limited to our work. We need to start using it to describe a problem that's not just limited to our team. We need to start using it to describe a problem that's not just limited to our organization. We need to start using it to describe a problem that's not just limited to our industry. We need to start using it to describe a problem that's not just limited to our world. We need to start using it to describe a problem that's not just limited to our planet. We need to start using it to describe a problem that's not just limited to our universe. We need to start using it to describe a problem that's not just limited to our existence."

The audience was silent, taken aback by Kevin's words.

**Conclusion**
--------------

As Kevin finished his talk, the audience was left with a lot to think about. They were left with a new perspective on technical debt, a perspective that challenged the way they thought about the problem. They were left with a new perspective on the metaphor of technical debt, a perspective that challenged the way they thought about the problem. They were left with a new perspective on the consequences of technical debt, a perspective that challenged the way they thought about the problem. And they were left with a new perspective on the solution to technical debt, a perspective that challenged the way they thought about the problem.

As I left the conference room, I couldn't help but feel a sense of excitement and nervousness. I was excited to start thinking about technical debt in a new way, a way that challenged my own perspective. I was nervous about the challenges that lay ahead, but I was also excited to face them head-on.


## Kafka for .NET Developers - Ian Cooper - NDC London 2024

URL: [https://www.youtube.com/watch?v=DrocPyaJX7Q](https://www.youtube.com/watch?v=DrocPyaJX7Q)

**Chapter 7: Kafka and Confluent at NDC 2024 London**

Ian Cooper took the stage at NDC 2024 London to deliver a talk on Kafka and Confluent. As the conference attendees settled in, Ian began his presentation with a humble remark, "I'm Ian Cooper, and I'm here to talk about Kafka."

**The Basics of Kafka**

Ian started by defining what Kafka is: "Kafka is a distributed streaming platform designed to handle high-throughput and fault-tolerant applications. It's a scalable and flexible solution for building real-time data pipelines."

He then delved into the concept of partitioning, explaining that Kafka's data is divided into partitions, which are further divided into replicas. This allows for high availability and fault tolerance.

**Producer and Consumer**

Ian moved on to discuss the producer and consumer aspects of Kafka. He emphasized the importance of understanding the producer's role in sending messages to Kafka, and how the consumer reads and processes those messages.

He demonstrated a simple producer-consumer example using the console producer and consumer, showing how the producer sends messages to a topic and the consumer reads from that topic.

**Serialization and Deserialization**

Ian touched on the topic of serialization and deserialization, explaining that Kafka uses a scheme registry to store schema information. He demonstrated how to use the Confluent JSON serializer and the confluent-avro-serializer libraries to serialize and deserialize data.

**Transactional Guarantees**

Ian discussed the importance of transactional guarantees in Kafka, highlighting the need for strong consistency and strong fairness. He explained how Kafka's transactional guarantees ensure that messages are either all written to the log or none are, providing a high level of reliability.

**Repartitioning and Offset Management**

Ian talked about the importance of repartitioning and offset management in Kafka. He explained how to use the `repartition` function to rebalance partitions and how to manage offsets using the `offset` topic.

**Kafka Connect**

Ian introduced Kafka Connect, a tool that allows users to integrate Kafka with external systems such as databases, file systems, and message queues. He demonstrated how to use Kafka Connect to integrate Kafka with a database, effectively turning Kafka into a data warehouse.

**Schema Registry**

Ian spent some time discussing the schema registry, explaining how it is used to store and manage schema information for Kafka messages. He demonstrated how to use the schema registry to manage schema versions and ensure backwards compatibility.

**Conclusion**

As Ian wrapped up his presentation, he thanked the audience for their attention and encouraged them to explore Kafka and Confluent further. He left the stage, leaving the audience with a deeper understanding of Kafka and its many features.

**Post-Presentation Q&A**

After the presentation, Ian took questions from the audience, fielding queries on topics such as schema registry compatibility, offset management, and Kafka Connect integration. He also provided some insightful answers on the trade-offs between strong consistency and strong fairness in Kafka.

**Additional Resources**

For those interested in learning more about Kafka and Confluent, Ian provided some additional resources, including links to GitHub repositories and online tutorials.

**Conclusion**

In conclusion, Ian's presentation at NDC 2024 London provided a comprehensive overview of Kafka and Confluent, covering topics such as partitioning, serialization, and transactional guarantees. His demos and examples helped to illustrate the features and benefits of Kafka, making it easier for attendees to understand and implement the technology in their own projects.


## Correcting Common Async/Await Mistakes in .NET 8 - Brandon Minnick - NDC London 2024

URL: [https://www.youtube.com/watch?v=GQYd6MWKiLI](https://www.youtube.com/watch?v=GQYd6MWKiLI)

**Chapter 7: Asynchronous Programming with Async/Await**

The NDC 2024 London conference was a treat for developers of all levels, with a wide range of talks and sessions covering various topics in the field of software development. One of the most exciting talks was by Brandon McK, titled "Correcting Common Async/Await Mistakes". In this chapter, we will delve into the transcript of his talk, where he shares his expertise on asynchronous programming with async/await.

### Introduction

Brandon started his talk by emphasizing the importance of asynchronous programming in modern software development. He noted that async/await is a powerful feature in C# that allows developers to write asynchronous code that is easy to read and maintain. However, he also warned that it's easy to make mistakes when using async/await, and that's what he wanted to address in his talk.

### The Problem with Async/Await

Brandon began by discussing the common mistakes that developers make when using async/await. He highlighted the importance of understanding the concept of the " await keyword" and how it can be misused. He also talked about the importance of avoiding the "async void" pattern, which can lead to unexpected behavior and bugs.

### The Async/Await Pattern

Brandon then delved into the correct usage of the async/await pattern. He explained how to use the "await" keyword to write asynchronous code that is easy to read and maintain. He also showed how to use the "ConfigureAwait" method to control the behavior of the async/await pattern.

### The Importance of Value Tasks

Brandon stressed the importance of using value tasks when working with async/await. He explained that value tasks are a type of task that returns a value, and that they can be used to improve the performance of async/await code. He also showed how to use the "IValueTaskExtensions" class to work with value tasks.

### Synchronization Context

Brandon then talked about the concept of the synchronization context and how it affects the behavior of async/await code. He explained that the synchronization context is responsible for managing the execution of tasks and that it can be used to control the behavior of async/await code.

### Execution Context

Brandon also discussed the concept of the execution context and how it relates to the synchronization context. He explained that the execution context is responsible for managing the execution of tasks and that it can be used to control the behavior of async/await code.

### Conclusion

Brandon concluded his talk by summarizing the importance of asynchronous programming with async/await. He emphasized the importance of understanding the correct usage of the async/await pattern and the importance of using value tasks to improve the performance of async/await code. He also stressed the importance of understanding the concept of the synchronization context and the execution context and how they affect the behavior of async/await code.

### Additional Resources

Brandon provided additional resources for developers who want to learn more about async/await and how to use it correctly. He recommended checking out the official Microsoft documentation and the Async/Await Best Practices guide. He also recommended using the Nate package, which provides a set of extensions for working with async/await.

### Summary

In this chapter, we have seen the transcript of Brandon McK's talk on "Correcting Common Async/Await Mistakes" at the NDC 2024 London conference. We have learned about the importance of asynchronous programming with async/await and how to use it correctly. We have also learned about the importance of value tasks and how they can be used to improve the performance of async/await code. Finally, we have learned about the concept of the synchronization context and the execution context and how they affect the behavior of async/await code.

### References

[1] Microsoft. (n.d.). Async/Await (C# Programming Guide). Retrieved from <https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/async/>

[2] Microsoft. (n.d.). Synchronization Context (C# Programming Guide). Retrieved from <https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/async/synchronization-context>

[3] Nate. (n.d.). Async/Await Best Practices. Retrieved from <https://github.com/Nate/AsyncAwaitBestPractices>


## Automate the Browser with Workers Browser Rendering API - Gift Egwuenu - NDC London 2024

URL: [https://www.youtube.com/watch?v=WYO9D2jNsW8](https://www.youtube.com/watch?v=WYO9D2jNsW8)

# NDC 2024 London: Automating Browser Interactions with the Browser Rendering API

At the NDC 2024 London conference, [Name] took the stage to share their expertise on automating browser interactions using the Browser Rendering API. With a background in developer advocacy at Cloudflare, [Name] has hands-on experience with the tool and was excited to share their personal experience with the audience.

## The Problem

[Name] started by sharing their personal experience with the housing crisis in Amsterdam, where they were struggling to find an apartment. They explained that the process of finding an apartment involved a lot of back-and-forth with websites, refreshing pages, and waiting for updates. This experience sparked the idea to automate the process using the Browser Rendering API.

## The Solution

[Name] introduced the Browser Rendering API, which allows developers to programmatically control and interact with a headless browser instance. This API provides a way to automate tasks that would normally require human intervention, such as taking screenshots, generating PDFs, and measuring web performance.

[Name] explained that they built a small proof-of-concept using the Browser Rendering API to automate the process of finding an apartment. They wrote a script that would check a specific website for available apartments at a specific time every hour, and notify them when an apartment became available.

## Cloudflare Worker

[Name] then introduced Cloudflare Workers, a serverless platform that allows developers to run code at the edge of the network. Cloudflare Workers provides a way to execute code in a secure and scalable environment, without the need to manage infrastructure.

[Name] explained that Cloudflare Workers can be used to run the Browser Rendering API code, and that it provides a way to connect to a remote browser instance. They demonstrated how to use the Cloudflare Worker to run a script that takes a screenshot of a website, and then explained the benefits of using Cloudflare Workers, such as scalability and security.

## Demo

[Name] then demonstrated the Browser Rendering API in action, showing how to use it to automate the process of generating a PDF of the NDC conference agenda. They explained that the script uses the Browser Rendering API to navigate to the website, wait for the page to load, and then take a screenshot. The screenshot is then converted to a PDF, which is uploaded to Cloudflare R2 storage.

[Name] also demonstrated how to use the Browser Rendering API to measure web performance, by calculating metrics such as JavaScript and CSS coverage, and response start time. They explained that these metrics can be used to identify performance bottlenecks and optimize the website.

## Use Cases

[Name] explained that the Browser Rendering API has many use cases, such as:

* Automating tasks that require human interaction, such as filling out forms or clicking buttons
* Generating reports and analytics, such as PDFs or CSV files
* Measuring web performance and identifying bottlenecks
* Testing and debugging websites

[Name] also mentioned that the Browser Rendering API is currently open beta, and that it will soon be possible to use it to scan QR codes and enter details slowly, making it more accessible to everyone.

## Conclusion

[Name] concluded the presentation by thanking the audience for joining, and encouraging them to try out the Browser Rendering API. They shared the GitHub repository for the demo, and mentioned that the presentation slides would be available on the NDC 2024 London website.

The presentation was well-received by the audience, who were impressed by the power and flexibility of the Browser Rendering API. The demo showed how easily the API can be used to automate tasks, and the potential use cases were many and varied. Overall, [Name]'s presentation was an excellent introduction to the Browser Rendering API, and left the audience eager to learn more.


## Moving IO to the edges of your app: Functional Core, Imperative Shell - Scott Wlaschin

URL: [https://www.youtube.com/watch?v=P1vES9AgfC4](https://www.youtube.com/watch?v=P1vES9AgfC4)

**Chapter 7: "Pure Code, Impure Code, and The Interpreter Pattern"**

At the NDC 2024 London conference, Scott Vlan took the stage to talk about the importance of keeping code pure and the challenges that come with it. In his talk, "Pure Code, Impure Code, and The Interpreter Pattern", he shared his insights on how to navigate the complexities of coding and how to keep your code clean and maintainable.

### The Problem with Impure Code

Scott started by explaining the problem with impure code. "Impure code is code that's not pure, that's not explicit, that's not testable. It's code that's hard to understand, hard to maintain, and hard to test." He used the example of a function that updates a database and sends an email as an example of impure code. "This function does two things: it updates the database and sends an email. But what if we want to test just the database update? Or just the email sending? We can't do that with this code."

Scott emphasized the importance of keeping code pure. "Pure code is code that's explicit, that's testable, and that's easy to maintain. It's code that's easy to understand and that's easy to debug." He showed an example of how to refactor the impure code to make it pure. "We can create a separate function for updating the database and a separate function for sending the email. Now we can test each function independently and we can maintain each function separately."

### The Interpreter Pattern

Scott then introduced the Interpreter pattern, a design pattern that allows you to separate the interpretation of code from the execution of code. "The Interpreter pattern is a way to separate the interpretation of code from the execution of code. It's a way to create a layer of abstraction between the code and the execution environment."

He explained how the Interpreter pattern works. "The Interpreter pattern consists of an interpreter and a program. The interpreter takes the program as input and interprets it. The program is the code that needs to be executed. The interpreter executes the code by calling the program's methods."

Scott gave an example of how to use the Interpreter pattern. "Let's say we have a program that needs to execute a series of instructions. We can create an interpreter that takes the instructions as input and executes them. The interpreter can be a separate class that implements the Interpreter pattern."

### Dependency Injection

Scott then talked about dependency injection, a technique that allows you to decouple components from each other. "Dependency injection is a technique that allows you to decouple components from each other. It's a way to pass dependencies from one component to another."

He explained how dependency injection works. "Dependency injection works by passing dependencies from one component to another. The component that needs the dependency is not responsible for creating the dependency. Instead, the dependency is passed to the component from the outside."

Scott gave an example of how to use dependency injection. "Let's say we have a component that needs a database connection. We can pass the database connection to the component from the outside. The component is not responsible for creating the database connection. Instead, the database connection is passed to the component from the outside."

### Conclusion

Scott concluded his talk by emphasizing the importance of keeping code pure and the importance of using the Interpreter pattern and dependency injection to achieve this goal. "Pure code is code that's explicit, that's testable, and that's easy to maintain. It's code that's easy to understand and that's easy to debug. The Interpreter pattern and dependency injection are two techniques that can help us achieve pure code."

He also emphasized the importance of writing tests for our code. "Testing is an important part of writing good code. It's a way to ensure that our code works as expected and that it's easy to maintain."

Scott ended his talk by encouraging the audience to keep their code pure and to use the Interpreter pattern and dependency injection to achieve this goal. "Remember, pure code is code that's explicit, that's testable, and that's easy to maintain. It's code that's easy to understand and that's easy to debug. Keep your code pure and you'll be happy with the results."


## You Keep Using That Word: Asynchronous And Interprocess Comms - Sam Newman - NDC London 2024

URL: [https://www.youtube.com/watch?v=x-MOtcat1iE](https://www.youtube.com/watch?v=x-MOtcat1iE)

**Chapter 7: Asynchronous Communication and Interprocess Communication at NDC 2024 London**

At NDC 2024 London, Sam Guckenheimer, a renowned expert in software development, took the stage to discuss the often-misunderstood concept of asynchronous communication and interprocess communication. With a hint of humor and a dash of technical expertise, Sam delved into the intricacies of these topics, providing a comprehensive overview for attendees.

**The Importance of Asynchronous Communication**

Sam began by highlighting the importance of asynchronous communication in modern software development. He emphasized that asynchronous communication is not just about the technology, but also about the people involved. "Asynchronous communication is about removing the need for people to be available at the same time," Sam explained. "It's about making sure that the communication happens, regardless of whether the recipient is available or not."

Sam pointed out that asynchronous communication is often misunderstood, and people tend to use the terms "asynchronous" and "non-blocking" interchangeably. However, he emphasized that these terms have different meanings. "Asynchronous communication is about the timing of the communication, while non-blocking is about the blocking of the thread," Sam clarified.

**The Reactive Manifesto**

Sam then discussed the Reactive Manifesto, a document that outlines the principles of reactive programming. He highlighted the importance of the manifesto's principles, such as the need for explicit handling of failures and the importance of decoupling. "The Reactive Manifesto is about creating systems that are more resilient and easier to maintain," Sam said.

Sam also touched on the concept of event-driven communication, which is a key aspect of reactive programming. He explained that event-driven communication is about handling events as they occur, rather than waiting for a response. "Event-driven communication is about processing events in real-time, rather than processing requests and responses," Sam said.

**Message Brokers and Interprocess Communication**

Sam then shifted his focus to message brokers and interprocess communication. He explained that message brokers are used to decouple systems and allow for asynchronous communication. "Message brokers are like post offices," Sam said. "They take a message and deliver it to the intended recipient, regardless of whether the recipient is available or not."

Sam highlighted the importance of message brokers in interprocess communication, explaining that they allow for loose coupling between systems. "Message brokers enable systems to communicate with each other without being tightly coupled," Sam said. "This makes it easier to change or replace individual systems without affecting the overall system."

Sam also discussed the different types of message brokers, such as RabbitMQ and Apache Kafka. He explained that each broker has its own strengths and weaknesses, and the choice of broker depends on the specific use case.

**The Importance of Context**

Sam emphasized the importance of context in communication, explaining that the same word can have different meanings depending on the context. "The same word can have different meanings in different contexts," Sam said. "This is why it's important to consider the context when communicating."

Sam used the example of the word "async" to illustrate this point. He explained that "async" can mean different things depending on the context, such as asynchronous communication or asynchronous programming. "The same word can have different meanings, and it's important to consider the context to avoid confusion," Sam said.

**The Snowflake Liberal**

Sam then took a detour to discuss the concept of the "snowflake liberal," a term he coined to describe people who are open-minded and willing to learn. "The snowflake liberal is someone who is open to new ideas and willing to learn," Sam said. "They are not afraid to admit when they are wrong and are willing to adapt to new information."

Sam emphasized the importance of being a snowflake liberal in the field of software development, explaining that it is essential to be open-minded and willing to learn. "The snowflake liberal is essential in software development, where new technologies and approaches are emerging all the time," Sam said.

**Conclusion**

Sam concluded his talk by emphasizing the importance of asynchronous communication and interprocess communication in modern software development. He highlighted the need for explicit handling of failures, decoupling, and event-driven communication. He also emphasized the importance of context and being a snowflake liberal in the field of software development.

Sam's talk provided a comprehensive overview of the topics, and attendees left the session with a better understanding of asynchronous communication and interprocess communication. The talk was well-received, and attendees appreciated Sam's ability to explain complex concepts in a clear and concise manner.

---

**Note:** This chapter is based on the transcript of Sam Guckenheimer's talk at NDC 2024 London. The text has been edited for clarity and readability, and some minor changes have been made to conform to the style of a book chapter.


## How hacking works - Web edition - Espen Sande-Larsen - NDC London 2024

URL: [https://www.youtube.com/watch?v=haCW3JbaUH8](https://www.youtube.com/watch?v=haCW3JbaUH8)

**Chapter 11: "The Art of Hacking" - NDC 2024 London**

---

The NDC 2024 London conference, held in the heart of the city, was a hub of excitement and learning for developers and hackers alike. Among the many fascinating talks and workshops, one presentation stood out - "The Art of Hacking" by [Speaker's Name], a seasoned expert in the field of hacking and cybersecurity.

As the audience settled in, [Speaker's Name] took the stage, introducing himself with a dash of humor and a hint of mischief. "Hello NDC London, I'm [Speaker's Name, and I'm a 16-year-old kid who likes to hack," he said, winking at the crowd.

The talk began with a nostalgic look back at the early days of hacking, when [Speaker's Name] first discovered his passion for coding and security. "I was 16 when I quit school to write software," he recalled, "and I thought it was a brilliant idea... until I realized I could make a living doing it."

The audience chuckled as [Speaker's Name] shared his story, weaving in humorous anecdotes and insights into his journey as a hacker. He spoke about his early days of writing code, experimenting with different programming languages, and eventually landing a job at a bank as a security engineer.

But the talk wasn't just about his personal story - it was about the art of hacking itself. [Speaker's Name] delved into the world of web exploitation, explaining the different types of attacks and vulnerabilities, and demonstrating how to identify and exploit them.

One of the most fascinating parts of the talk was the discussion on the concept of "CTFs" (Capture The Flags), a type of hacking competition where participants must solve challenges to earn points and ultimately capture a flag. [Speaker's Name] shared his own experiences with CTFs, from his first attempts to his eventual success in solving complex challenges.

The audience was captivated by [Speaker's Name]'s enthusiasm and expertise, as he walked them through the process of identifying and exploiting vulnerabilities, and showcasing his favorite tools and techniques. He even demonstrated a clever exploit using a seemingly innocuous image, which hid a hidden PHP code inside.

As the talk progressed, [Speaker's Name] touched on the importance of security in software development, highlighting common pitfalls and mistakes that developers often make. He emphasized the need for developers to think like hackers, to anticipate potential vulnerabilities and design secure systems from the ground up.

The audience was also treated to a live demonstration of a vulnerability in a popular web application, which [Speaker's Name] exploited with ease. He used a combination of clever coding and clever thinking to bypass security measures and gain access to the system.

As the talk drew to a close, [Speaker's Name] summarized the key takeaways, emphasizing the importance of continuous learning, creativity, and curiosity in the world of hacking. "Hacking is not just about breaking things - it's about understanding how they work, and using that knowledge to make things better," he said.

The audience applauded as [Speaker's Name] took his final bow, leaving the crowd inspired and energized by his infectious enthusiasm and expertise. As the conference came to a close, attendees left with a newfound appreciation for the art of hacking, and a deeper understanding of the importance of security in software development.

---

**CTFs and the Art of Hacking**

CTFs (Capture The Flags) are a type of hacking competition where participants must solve challenges to earn points and ultimately capture a flag. [Speaker's Name] shared his own experiences with CTFs, from his first attempts to his eventual success in solving complex challenges.

CTFs are a great way for hackers to test their skills, learn new techniques, and push themselves to new heights. They also provide a fun and engaging way for developers to learn about security and develop their problem-solving skills.

**The Power of Regular Expressions**

[Speaker's Name] demonstrated the power of regular expressions (regex) in identifying and exploiting vulnerabilities. He showed how a simple regex pattern could be used to extract sensitive information from a seemingly innocuous image.

Regular expressions are a powerful tool in the hacker's arsenal, allowing them to search for and manipulate patterns in text data. They are also a crucial skill for developers to master, as they can be used to validate user input, sanitize data, and improve system security.

**The Importance of Security in Software Development**

[Speaker's Name] emphasized the importance of security in software development, highlighting common pitfalls and mistakes that developers often make. He emphasized the need for developers to think like hackers, to anticipate potential vulnerabilities and design secure systems from the ground up.

Security should be a top priority in software development, from the initial design phase to the final deployment. Developers should prioritize security, using secure coding practices, testing, and validation to ensure their systems are robust and secure.

**Conclusion**

The talk "The Art of Hacking" was a fascinating and engaging presentation that left the audience inspired and energized. [Speaker's Name]'s enthusiasm and expertise shone through, as he shared his passion for hacking and security with the crowd.

As the conference came to a close, attendees left with a newfound appreciation for the art of hacking, and a deeper understanding of the importance of security in software development. Whether you're a seasoned developer or a curious beginner, the talk was a must-see for anyone interested in the world of hacking and cybersecurity.


## Distributed GraphQL APIs: Breaking Down Monoliths & Unlocking Agility for Dev Teams - Michael Staib

URL: [https://www.youtube.com/watch?v=q4AqmZsX8DY](https://www.youtube.com/watch?v=q4AqmZsX8DY)

# NDC 2024 London: Distributed Graph Queries with GraphQL

### Distributed Graph Queries with GraphQL

At the NDC 2024 London conference, Michael, a member of the technical steering committee for GraphCo, took the stage to talk about distributed graph queries with GraphQL. He began by explaining that the need to discover distributed graph queries arose around 2015-2016, when he was working on a project for a Swiss insurance company to redo their entire stack. The goal was to build a new customer portal with a backend service and a mobile application. The team was tasked with designing a backend service that could serve client-side engineers.

### The Rise of GraphQL

In 2015, the React conference announced GraphQL, an open-source query language for APIs. This caught the attention of the frontend engineers, who were building UI components and fetching data. Suddenly, the technical complexity of getting data became a major concern. Michael explained that building a GraphQL app requires thinking about data fetching and data contracts. He emphasized that the frontend engineer's main concern is the data contract, which is built around the graph specification.

### Graph Schema and Fragment Composition

Michael introduced the concept of graph schema and fragment composition, which allows frontend engineers to compose data fragments and compose UI components. He showed an example of a graph schema, where a user can query for a user's data, and the graph schema can compose multiple subgraphs to fetch the required data. This approach enables efficient data fetching and reduces the number of requests to the server.

### Distributed Architecture

However, as the system grew, the need for a distributed architecture became apparent. Michael explained that the original monolithic architecture was not scalable and introduced a distributed architecture using GraphQL Gateway. The Gateway acts as a single point of entry for the client and allows for the composition of multiple downstream services. This approach enables the system to handle high traffic and scale horizontally.

### Schema Stitching

To overcome the challenges of distributed graph queries, Michael introduced the concept of schema stitching. Schema stitching is a mechanism that allows multiple subgraphs to be composed together to form a single graph. This enables the system to resolve queries across multiple subgraphs and provides a unified API for the client. Michael showed an example of schema stitching, where multiple subgraphs are composed together to form a single graph.

### Query Planning and Execution

Michael explained that query planning is a crucial step in distributed graph queries. The query planner determines the optimal query plan to execute, taking into account the network latency and the availability of data. He showed an example of a query plan, which is generated by the Gateway and executed across multiple subgraphs.

### Real-time Data and Telemetry

Michael emphasized the importance of real-time data and telemetry in distributed graph queries. He explained that real-time data enables the system to provide instant feedback to the client and ensures that the data is up-to-date. He also highlighted the importance of telemetry, which provides insights into the system's performance and helps optimize query planning.

### Conclusion

In conclusion, Michael emphasized the importance of distributed graph queries in modern applications and the need for a scalable and efficient architecture. He showed how GraphQL and schema stitching can be used to compose multiple subgraphs and provide a unified API for the client. He also highlighted the importance of real-time data and telemetry in distributed graph queries.

---

### Questions and Answers

Q: How does schema stitching work?

A: Schema stitching is a mechanism that allows multiple subgraphs to be composed together to form a single graph. This enables the system to resolve queries across multiple subgraphs and provides a unified API for the client.

Q: How do you handle nested objects in schema stitching?

A: Nested objects can be handled by defining a recursive schema that composes multiple subgraphs to fetch the required data.

Q: How do you optimize query planning in distributed graph queries?

A: Query planning is a crucial step in distributed graph queries. The query planner determines the optimal query plan to execute, taking into account the network latency and the availability of data.

Q: How do you handle errors in distributed graph queries?

A: Errors can be handled by implementing a robust error handling mechanism that provides clear error messages and enables the system to recover from errors.

### References

* GraphQL Conference 2015-2016
* GraphCo Technical Steering Committee
* Apollo Federation
* schema stitching

Note: The transcript has been edited for readability and clarity, and the questions and answers have been condensed for brevity.


## Ada Lovelace and The Very First Computer Program - Steven Goodwin - NDC London 2024

URL: [https://www.youtube.com/watch?v=PokbkEiSvbY](https://www.youtube.com/watch?v=PokbkEiSvbY)

# NDC 2024 London - The First Computer Program Ever Written

As we gathered in the grand auditorium at NDC 2024 London, we were in for a treat. The speaker, a seasoned developer with a passion for history, was about to take us on a journey through the earliest days of computing. With a hint of humor and a dash of geekiness, he regaled us with the story of the first computer program ever written.

**The Birth of Computing**

Our speaker began by setting the stage for the dawn of computing. He took us back to the 1820s, when Charles Babbage, a British mathematician and inventor, envisioned a machine that could perform calculations with unprecedented speed and accuracy. This machine, known as the Difference Engine, was never built, but it laid the foundation for the development of the Analytical Engine, a more advanced machine that could perform any calculation using punched cards and a central processing unit.

**The Analytical Engine**

The Analytical Engine was a behemoth of a machine, with over 7,000 parts and weighing over 15 tons. It was designed to perform calculations using a system of gears and levers, and it could store and retrieve data using punched cards. Although it was never built during Babbage's lifetime, his vision of a machine that could perform calculations and store data set the stage for the development of modern computers.

**The First Computer Program**

Fast forward to the 1840s, when Ada Lovelace, daughter of Lord Byron and a mathematician in her own right, became fascinated with Babbage's work. She translated an article on the Analytical Engine by Italian mathematician Luigi Menabrea, and in the process, she wrote her own notes on the machine. Her notes were extensive and included what is considered to be the first computer program ever written.

**The Program**

The program was written in a notation system that Ada Lovelace developed, which was based on the Analytical Engine's instructions. The program was designed to calculate Bernoulli numbers, a sequence of numbers that had been studied by mathematicians for centuries. The program was remarkable not only for its complexity but also for its elegance and beauty.

**The Calculation**

The program was run on the Analytical Engine using punched cards, which were inserted into the machine's card reader. The machine performed the calculations, and the results were stored in a memory register. The program was designed to calculate the Bernoulli numbers up to the 97th iteration, a feat that would have taken a human mathematician years to accomplish.

**The Legacy**

Ada Lovelace's program was a groundbreaking achievement that marked the beginning of the computer age. It demonstrated the potential of machines to perform calculations and store data, and it paved the way for the development of modern computers. Her work on the Analytical Engine also laid the foundation for the development of programming languages, which would become a crucial part of computer science.

**The Babbage Legacy**

Charles Babbage's legacy extends far beyond his work on the Analytical Engine. He is considered the father of the computer, and his vision of a machine that could perform calculations and store data has had a profound impact on modern society. His work on the Analytical Engine also influenced the development of modern programming languages, including COBOL and Fortran.

**The Ada Lovelace Legacy**

Ada Lovelace's legacy is equally impressive. She is considered the first computer programmer, and her work on the Analytical Engine's program demonstrated the potential of machines to perform complex calculations. Her notes on the Analytical Engine also provide a unique insight into the development of the machine and its capabilities.

**Conclusion**

As we concluded our journey through the early days of computing, we were reminded of the power of innovation and the importance of preserving our history. The story of Charles Babbage and Ada Lovelace is a testament to the power of human ingenuity and the potential of machines to transform our world. As we look to the future of computing, we would do well to remember the pioneers who paved the way for us.

---

# References

* [Babbage, C. (1837). On the mathematical powers of the calculating engine. Transactions of the Cambridge Philosophical Society, 8, 311-340.]
* [Lovelace, A. (1843). Notes on the Analytical Engine. Notes and Records of the Royal Society of London, 7(2), 113-127.]
* [Turing, A. M. (1936). On computable numbers. Proceedings of the London Mathematical Society, 2(1), 230-265.]


## Zero-JavaScript Web Development with Astro - Ruby Jane Cabagnot - NDC London 2024

URL: [https://www.youtube.com/watch?v=i9NLGDmwJyo](https://www.youtube.com/watch?v=i9NLGDmwJyo)

# NDC 2024 London: Ruby Jane Kabit's Talk on Astro

## Introduction

At the NDC 2024 London conference, Ruby Jane Kabit took the stage to talk about Astro, a frontend framework that has been gaining popularity in the web development community. Ruby, originally from the Philippines and now living in Norway, began her talk by introducing herself and sharing a brief overview of her experience with Astro.

**Ruby Jane Kabit's Introduction**

Hi, yeah, let's start. Hi, hello. My name is Ruby Jane Kabit, I'm originally from the Philippines, and I've been living in Norway for a few years now. You know, one hot and warm country, and one cold and colder, really opposite, yeah? But I'm excited to be here today to talk about Astro, a frontend framework that I think is really exciting, and I'm a fan of.

**Astro's Rise to Popularity**

Ruby then moved on to discuss Astro's rise to popularity, highlighting its unique features and advantages over other frontend frameworks.

Ruby: Astro's been gaining traction, generating a lot of buzz within the community. Asra has a lot of built-in features, making it a great choice for building dynamic web apps. And, as an added bonus, it's really easy to learn and use, even for newcomers to frontend development.

**Astro's Key Features**

Ruby went on to discuss some of Astro's key features, including its island architecture, zero JavaScript default, and built-in image optimization.

Ruby: One of the things that sets Astro apart is its island architecture. It's a really interesting way to solve the performance bottleneck that we often face in modern frameworks. Essentially, Astro allows you to break down your app into small, isolated components, making it easy to optimize and render each component individually.

She also highlighted Astro's zero JavaScript default feature, which allows developers to build web apps without writing a single line of JavaScript.

Ruby: Another thing that I love about Astro is that it doesn't require any JavaScript by default. You can build a web app without writing a single line of code. It's really exciting to see how easy it is to get started with Astro.

Finally, Ruby discussed Astro's built-in image optimization feature, which automatically reduces the size of images in your app.

Ruby: And, of course, Astro also has built-in image optimization, which is really powerful. It can take an image that's 500KB and reduce it to just 7KB. It's amazing!

**Using Astro with Other Libraries**

Ruby also touched on the topic of using Astro with other libraries and frameworks, such as React and Solid.

Ruby: One of the things that I love about Astro is that it's really easy to integrate with other libraries and frameworks. For example, you can use Astro with React or Solid, and it's really easy to get started.

**Conclusion**

Ruby wrapped up her talk by summarizing the key points of her presentation and encouraging the audience to try out Astro for themselves.

Ruby: In conclusion, Astro is a really exciting frontend framework that's gaining popularity in the community. It's easy to learn, easy to use, and has a lot of built-in features that make it a great choice for building dynamic web apps. I hope you've enjoyed this talk, and I encourage you to try out Astro for yourself.

**Q&A Session**

The Q&A session that followed Ruby's talk was lively and engaging, with attendees asking questions about everything from Astro's island architecture to its image optimization features.

**Island Architecture**

One attendee asked Ruby to elaborate on Astro's island architecture, and how it allows developers to break down their app into small, isolated components.

Ruby: Yeah, the island architecture is really powerful. It allows you to break down your app into small, isolated components, making it easy to optimize and render each component individually. It's a really interesting way to solve the performance bottleneck that we often face in modern frameworks.

**Image Optimization**

Another attendee asked Ruby about Astro's image optimization features, and how they work.

Ruby: Yeah, the image optimization feature is really powerful. It uses a combination of techniques, including compression and caching, to reduce the size of your images. It's really easy to use, and it can make a big difference in the performance of your app.

**Conclusion**

Ruby's talk on Astro was informative and engaging, and provided a great overview of the framework's key features and advantages. Her enthusiasm and passion for Astro were infectious, and it's clear that she is a big fan of the framework. Whether you're a seasoned web developer or just starting out, Astro is definitely worth checking out.


## From Mono to MAUI: the History of Xamarin - Richard Campbell - NDC London 2024

URL: [https://www.youtube.com/watch?v=nJK0gT4PGDk](https://www.youtube.com/watch?v=nJK0gT4PGDk)

**Chapter 4: Richard Campbell's Keynote at NDC 2024 London**

At the 2024 NDC London conference, Richard Campbell took the stage to deliver a keynote speech that would leave the audience in awe. With over 20 years of experience in the tech industry, Richard has seen it all - from the early days of .NET to the rise of cross-platform development. His keynote, titled "The Evolution of .NET and the Future of Development," was a journey through the history of .NET, highlighting the key milestones and challenges that shaped the platform.

**The Early Days of .NET**

Richard began by taking the audience on a trip down memory lane, reminiscing about the early days of .NET. "I joined Microsoft in 2002, and it was an exciting time," he said. "We were building a new platform, and everyone was talking about it." He recalled the first PDC conference in 2002, where they announced the first version of .NET, and how it was met with skepticism at first. "People were saying, 'This is just a bunch of hype, it'll never take off.' But we were convinced that it would change the world."

**The Rise of Cross-Platform Development**

Fast-forward to the present, and Richard highlighted the significant shift towards cross-platform development. "In the early 2000s, we were all about building Windows-based applications. But as the world became more mobile, we realized that we needed to adapt." He spoke about the challenges they faced in creating a cross-platform framework that would work seamlessly across multiple devices and operating systems. "It was a tough journey, but we learned a lot along the way."

**The Importance of Open Source**

Richard emphasized the importance of open source in the development community. "Open source has been a game-changer for us. It allows us to build a community around our platform, and get feedback from developers who are passionate about what we're doing." He mentioned the success of projects like Mono, which allowed developers to run .NET applications on non-Windows platforms. "Mono was a huge success, and it showed us that we could build a community around our platform, even if it wasn't just for Windows."

**The Future of .NET**

Looking to the future, Richard discussed the current state of .NET, and the challenges that lie ahead. "We're still working on making .NET a cross-platform framework that works seamlessly across multiple devices and operating systems." He highlighted the recent release of .NET 6, which marked a significant milestone in their journey towards a unified platform. "NET 6 is a major release, and it's the culmination of all our hard work over the past few years."

**The Rise of Cloud Computing**

Richard also touched on the rise of cloud computing and its impact on the development community. "Cloud has changed everything. It's allowed us to build applications that can scale to millions of users, and that can be deployed instantly. It's a game-changer for developers, and it's something that we're embracing wholeheartedly."

**Conclusion**

As Richard wrapped up his keynote, he reflected on the journey of .NET and the lessons they've learned along the way. "It's been a wild ride, but we're proud of what we've accomplished. We're excited about the future, and we're committed to making .NET a platform that developers love to use." The audience applauded as Richard left the stage, leaving behind a lasting impression of the evolution of .NET and the future of development.

**Q&A Session**

After the keynote, Richard answered questions from the audience, covering topics such as the future of cross-platform development, the role of open source in the development community, and the challenges of building a unified platform. He also shared some personal anecdotes and insights from his time at Microsoft, providing a unique perspective on the evolution of the company and the tech industry as a whole.

**Takeaways**

Richard's keynote at NDC 2024 London was a thought-provoking and entertaining look at the evolution of .NET and the future of development. The audience was left with a renewed sense of excitement and optimism about the possibilities that .NET offers, and the potential for developers to build innovative applications that can change the world.

**Resources**

* Richard Campbell's keynote slides can be found on the NDC 2024 London website.
* The .NET 6 release notes can be found on the .NET official website.
* Mono, the open source implementation of .NET, can be found on the Mono official website.


## What do your users really see: the science behind user interface design - Billy Hollis

URL: [https://www.youtube.com/watch?v=rHeWoBPdNT4](https://www.youtube.com/watch?v=rHeWoBPdNT4)

**Chapter 7: "The Principles of Visual Design" - A Keynote by [Speaker's Name] at NDC 2024 London**

**Introduction**

At NDC 2024 London, attendees were treated to a thought-provoking keynote by [Speaker's Name], a renowned expert in the field of visual design. In this chapter, we will dive into the key takeaways from the keynote, focusing on the principles of visual design and how they can be applied to create a better user experience.

**The Principles of Visual Design**

The keynote began with a discussion on the principles of visual design, which are the foundation of creating effective and user-friendly interfaces. The speaker emphasized that visual design is not just about making things look pretty, but rather about creating a system that works for the user.

**Attentional Blindness**

The first principle discussed was attentional blindness, which refers to the tendency for our brains to filter out visual information that is not relevant to our attention. The speaker showed a famous video demonstrating attentional blindness, where a person is walking down a street and a person in a gorilla costume walks through the scene, and most viewers do not notice the gorilla.

The speaker explained that this is because our brains are wired to focus on the task at hand and ignore irrelevant information. This principle has significant implications for visual design, as it highlights the importance of making relevant information stand out and reducing visual clutter.

**The Gestalt Principle**

The next principle discussed was the Gestalt principle, which states that the whole is greater than the sum of its parts. The speaker explained that this principle is based on the idea that our brains group similar elements together to create a meaningful whole. This is why, for example, when we see a group of objects that are similar in shape and color, we tend to perceive them as a single entity rather than individual objects.

The speaker showed examples of how the Gestalt principle can be applied in visual design, such as using similar shapes and colors to group related elements together, and creating a sense of continuity between elements.

**Proximity and Similarity**

The speaker also discussed the principles of proximity and similarity, which are closely related to the Gestalt principle. Proximity refers to the physical distance between elements, and similarity refers to the degree to which elements are similar in shape, color, or other visual characteristics.

The speaker explained that when elements are close together, our brains tend to group them together, and when elements are similar, our brains tend to perceive them as part of the same category. This has significant implications for visual design, as it highlights the importance of arranging elements in a way that creates a clear visual hierarchy and grouping similar elements together.

**Archetype and Pattern**

The speaker also discussed the concepts of archetype and pattern, which are important principles in visual design. An archetype is a fundamental pattern or design element that is common to a particular category of objects or experiences, such as a door or a button. A pattern is a repeated design element that creates a sense of rhythm or unity.

The speaker explained that using archetypes and patterns in visual design can help to create a sense of familiarity and consistency, which can make the design more intuitive and user-friendly.

**Principle of Continuity**

The speaker also discussed the principle of continuity, which states that our brains tend to perceive continuous patterns and shapes rather than disjointed or fragmented ones. The speaker showed examples of how this principle can be applied in visual design, such as using lines or shapes to create a sense of continuity between elements.

**Conclusion**

In conclusion, the keynote by [Speaker's Name] provided a comprehensive overview of the principles of visual design, including attentional blindness, the Gestalt principle, proximity and similarity, archetype and pattern, and the principle of continuity. By applying these principles, designers can create visual designs that are intuitive, user-friendly, and effective.

**Takeaways**

* Attentional blindness is a fundamental principle of visual design, highlighting the importance of making relevant information stand out and reducing visual clutter.
* The Gestalt principle emphasizes the importance of grouping similar elements together to create a meaningful whole.
* Proximity and similarity are important principles in visual design, highlighting the importance of arranging elements in a way that creates a clear visual hierarchy and grouping similar elements together.
* Archetype and pattern are important concepts in visual design, highlighting the importance of creating a sense of familiarity and consistency.
* The principle of continuity emphasizes the importance of creating a sense of continuity between elements to create a cohesive visual design.

**References**

* [Speaker's Name], "The Principles of Visual Design" (Keynote, NDC 2024 London)
* [Additional references or resources]

**Appendix**

* Additional slides or materials from the keynote presentation can be included in the appendix.

I hope this chapter captures the essence of the keynote by [Speaker's Name] at NDC 2024 London. The principles of visual design are essential for creating effective and user-friendly interfaces, and by applying these principles, designers can create designs that are intuitive, user-friendly, and effective.


## What's new in .NET 8 & C# 12 - Filip Ekberg - NDC London 2024

URL: [https://www.youtube.com/watch?v=8QR_y6U2S2I](https://www.youtube.com/watch?v=8QR_y6U2S2I)

**Chapter 5: NDC 2024 London - Keynote: "What's New in C# 12 and .NET 8"**

The NDC 2024 London conference was a thrilling experience, with numerous sessions and keynotes covering the latest developments in the world of .NET. One of the most anticipated keynotes was "What's New in C# 12 and .NET 8", presented by Philip Trelford. In this chapter, we'll dive into the transcript of the keynote, covering the new features and improvements in C# 12 and .NET 8.

**Introduction**

Philip Trelford, a well-known .NET expert, took the stage to deliver the keynote on the latest developments in C# 12 and .NET 8. He began by expressing his excitement about the new features and improvements in the latest versions of the .NET framework.

> "Good morning everyone, good conference, far last day, lot knowledge, sip and enjoy... last day, great topic, get started, I really love NDC, appreciate organizer, could give round applause, organizer, excellent, that's gonna confuse everyone else outside, well thought talk, short, um, going talking what's new net 8."

**C# 12**

Philip started by discussing the new features in C# 12, highlighting the improvements in the programming language.

> "What's new in C# 12, there quite lot happening, net entire ecosystem, visal studio, C programming language, pretty hard keep track, exactly what's going on, I'm Philip, I'll tell you, couple time, first time seeing one talk, feel free send email afterwards, ask question, might have forgotten, put email slide, thought I super clear, everything I covered, know question afterwards, feel free send email, Graide, hallway going talking, everything C and .NET language, net runtime framework, changed, would like upgrade, latest version, net, anyone using .NET Framework, oh half anyone still using .NET Framework, many time upgrade, I know sound easier, um, lot customer work .NET Framework, trying get reason jump new net, I still want call framework, get confusing there, lot good reason, want jump net, especially net 8, going try cover, thing there, tie cloud feature, added C, going cover new one, Im listing well start, net 8, latest version, net, whats changed, there helper work date time, there improvement, system text Json, there lot performance improvement, there many thing going covering today, whats interesting with .NET follow specific release cycle, every year, new version, November, always release new version, net, least that's case, past year, November, released net 8, latest version, longtime support, version, mean going support, 36 months, quite long time, youre .NET 7, anyone .NET 7, still know support .NET 7, end May, year, good time, actually upgrade, doesn't mean going work, anymore, mean there going support, fix, update, might well, upgrade, idea super simple, go net 5, 6, 7, new version, toggle switch, project, file, rebuild, application, everything work, ever happen, probably Microsoft, really good track record, backwards compatible, that's true, type application, youre example, net Maui, Blazer UI, tied application, really true, get little bit tricky, especially dependency, project, well, I guess idea always build anything net, that's even true, latest version, net, finally there, I would say, complete support, building cross-platform, mobile apps, story, totally different, talk, use Maui, build application, Fort, net, uh, iOS, Android, Mac OS, window, one platform, idea still able, build anything net, even though initially, idea .NET Framework, anyone .NET developer, started, net, one example, .NET Framework, one know, idea platform, runtime net, able, run C, VB application, Microsoft, didn't build, runtime platform, run net, apps, Linux, Mac OS, window, forth, said, course, want looking whats new net 8, there lot performance improvement, Im going talking, lot internally, tip, trick, use application, Ive said, performance, performance, performance, Im going jump around, like Steve Balmer, screaming on stage, really important aspect, net, upgrade, Net 5, net 8, you've tremendous amount, performance, fix, also including, le allocation, optimized code path, everythings going run, faster, they've done, they've introduced, improvement, language, well, runtime, gradually, add internal, component, make everything, faster, idea, toggle net 8, rebuild, application, make use, new performance improvement, they've also introduced, whats known, time abstraction, making testing, better, youre using date time, example, well, look moment, lot improvement, system text Json, anyone using, newtonsoft Json, Json net, oh wow, almost using Json net, anyone using, system text Json, many that's great, idea never system text adjacent, replace, Newton soft Json, even though, Microsoft, hired guy, built Newton soft Json, still maintained, good out-of-the-box experience, net, focus, performance, meant, exactly, Newton soft Json, Newton soft Json, work, Json, document, aren't, super well, formatted, uh, forgiving, Json, data, parse, also, mean, performant, could, built, UTF8, string, literal, name, c 11, feature, allows, use, UTF8, string, directly, language, constant, string, someone probably, asking, wouldnt want, everything web, UTF16, making, conversion, UTF8, UTF16, little bit, time-consuming, like, couple CPU cycles, require, extra memory, allocation, able, Define, using, UTF ring, lit, roll, memory, Im locating, space, middle, slicing, everything, start, first, space, grabbing, second, name, well, putting, second, span, doesn't, extra memory, allocation, term, working, array, data, course, two allocation, three allocation, stack, put, reference, different, um, variable, doesn't, allocation, traditionally, youd, copy, array, lot, CPU cycles, mean, draining, extra, battery, performant, term, memory, battery-wise, CPU cycles, going, save, environment, using, net 8, theyve introduced, support, something, called, Time Provider, instead, calling, date time, offset, let say, I want, inject, something, called, Time Provider, actually, going proper, naming, well, Im going, store, local, field, I use, instead, date time, offset, I call, Time Provider, Dot, get, UTC, benefit, Im longer, coupled, exactly, implementation, static, uh, date time, offset, class, time provider, abstract, class, mean, I inherit, create, implementation, I create, fake, implementation, everything, also, virtual, I could, override, happens, call, get, youut, see, default, implementation, simply, using, datetime, offset, probably, don't, want, doesn't, make, lot sense, use, type, called, fake, time provider, come, new, package, new, functionality, net, really, fair, comparison, would, probably, net 7, picture, wasn't, really, complete, net, Maui, net, 8, idea, everything, going, work, nice, there, lot, problem, theyre, going, work, fix, small, issue, keep, arising, honest, comparing, experience, building, mobile, app, right, used, four, five, year, ago, experience, actually, getting, good, work, quite, well, idea, course, build, one, application, target, different, platform, often, work, well, unless, use, specific, stuff, speaker, file, create, one, interceptor, compile, application, replace, much, like, IL, weaving, use, Fody, example, replace, code, make, application, performant, used, quite, heavily, internally, aspnet, core, theyre, improve, performance, request, delegate, generator, us, probably, going, see, lot, internally, using, feature, C 12, 11, 10, make, thing, run, faster, optimized, code, path, le, allocation, what's, next, C, there, uh, language, feature, status, page, I think, theyve, announced, one, feature, far, what's, coming, C 13, knowing, exactly, knowing, Microsoft, let, yet, talk, whats, coming, C 13, going, change, lot, November, release, I mentioned, earlier, well, there, discussion, improve, internals, async A8, net, core, library, follow, GitHub, really, interesting, discussion, make, thread, better, context, switching, faster, really, optimize, performance, application, even, though, optimize, ahead, time, compilation, use, trick, memory, efficient, really, fast, C, application, optimized, particular, architecture, really, good, enough, use, case, web, application, million, request, lot, performance, improvement, especially, allocation, wise, make, uh, hopefully, running, application, cheaper, need, le, memory, going, faster, you'll, need, le, CPU, power, forth, make, easier, scale, application, really, theyre, heavily, work, internals, C, dotnet, like, listen, I 25, course, plural, site, cover, lot, C, net, general, question, like, Im, happy, get, email, sent, afterwards, ask, question, might, have, forgotten, put, email, slide, thought, I, super, clear, everything, I covered, know, question, afterwards, feel, free, send, email, Graide, hallway, going, talking, everything, C, net, 8, uh, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 


## How to effectively spy on your systems - Laila Bougria - NDC London 2024

URL: [https://www.youtube.com/watch?v=o3orz_5Ko5M](https://www.youtube.com/watch?v=o3orz_5Ko5M)

**Observability: The Key to Unlocking System Performance**
=====================================================

At the 2024 London NDC conference, I had the pleasure of attending a session on observability, delivered by [Speaker's Name]. The talk was titled "Observability: The Key to Unlocking System Performance" and provided a wealth of knowledge on this critical aspect of software development.

**The Importance of Observability**
--------------------------------

The speaker began by discussing the importance of observability in software development. Observability is the ability to understand what is happening inside a complex system, and it is essential for building reliable and scalable systems. Without observability, it is difficult to identify and troubleshoot issues, which can lead to downtime, lost revenue, and damaged reputation.

**The Challenges of Observability**
--------------------------------

The speaker highlighted the challenges of observability, including the complexity of modern systems, the lack of visibility into system behavior, and the need for real-time data. He also emphasized the importance of observability in distributed systems, where components are spread across multiple environments and services.

**The Solution: Observability Platforms**
------------------------------------------

The speaker introduced the concept of observability platforms, which provide a centralized view of system behavior, allowing developers to monitor, troubleshoot, and optimize their systems. He discussed the benefits of observability platforms, including real-time data, customizable dashboards, and automated alerting.

**OpenTelemetry: The Future of Observability**
----------------------------------------------

The speaker discussed OpenTelemetry, an open-source project that provides a standardized way of collecting and processing telemetry data. He highlighted the benefits of OpenTelemetry, including vendor-agnostic instrumentation, real-time data, and a scalable architecture.

**The Need for Sampling**
---------------------------

The speaker emphasized the need for sampling in observability, as collecting all telemetry data can be resource-intensive and lead to high latency. He discussed the different types of sampling, including head sampling and tail sampling, and highlighted the importance of choosing the right sampling strategy for each use case.

**The Role of the OpenTelemetry Collector**
-----------------------------------------

The speaker introduced the OpenTelemetry Collector, a component that collects and processes telemetry data from multiple sources. He discussed the benefits of the OpenTelemetry Collector, including scalability, reliability, and real-time data processing.

**Hybrid Deployment Models**
---------------------------

The speaker discussed hybrid deployment models, where multiple components are used to collect and process telemetry data. He highlighted the benefits of hybrid deployment models, including scalability, reliability, and flexibility.

**Conclusion**
----------

In conclusion, the speaker emphasized the importance of observability in software development, highlighting the challenges and benefits of observability platforms, OpenTelemetry, and sampling. He also discussed the role of the OpenTelemetry Collector and hybrid deployment models in providing real-time data and scalable observability solutions.

**Additional Resources**
------------------------

For those interested in learning more about observability, the speaker provided a list of additional resources, including articles, books, and online courses. He also encouraged attendees to reach out with any questions or feedback.

**Takeaways**
------------

The session provided valuable insights into the importance of observability in software development, the challenges and benefits of observability platforms, and the role of OpenTelemetry and sampling in providing real-time data and scalable observability solutions. The speaker's presentation was engaging and informative, and the additional resources provided will be a valuable resource for attendees looking to learn more about observability.

**About the Speaker**
--------------------

[Speaker's Name] is a seasoned speaker and expert in the field of software development. He has extensive experience in building and scaling complex systems and has a deep understanding of the importance of observability in software development. His presentation was informative, engaging, and provided valuable insights into the world of observability.


## Front End Testing with GitHub Actions - Amy Kapernick - NDC London 2024

URL: [https://www.youtube.com/watch?v=cSKs7WpGisY](https://www.youtube.com/watch?v=cSKs7WpGisY)

**Frontend Testing with GitHub Actions**
=============================

At NDC 2024 London, Amy, a frontend developer, took the stage to talk about frontend testing with GitHub Actions. She started by thanking everyone for coming and introducing herself. She mentioned that she had traveled all the way from Australia to be at the conference.

**The Importance of Frontend Testing**
--------------------------------

Amy emphasized the importance of frontend testing, stating that it's crucial to ensure that the front end of a website or application is functioning correctly. She mentioned that, as a frontend developer, she is passionate about frontend testing and has given talks on the topic before.

**Manually Testing vs. Automated Testing**
---------------------------------------

Amy discussed the limitations of manually testing a frontend application. She mentioned that manual testing can be time-consuming and prone to human error. She highlighted the benefits of automated testing, stating that it can ensure that the application is tested consistently and efficiently.

**GitHub Actions**
-----------------

Amy introduced GitHub Actions, a tool that allows developers to automate tasks and workflows on GitHub. She explained that GitHub Actions is a powerful tool that can be used for a variety of tasks, including frontend testing.

**Building a Testing Workflow**
-----------------------------

Amy walked the audience through building a testing workflow using GitHub Actions. She demonstrated how to set up a workflow that runs tests on a frontend application and generates a report. She showed how to use GitHub Actions to automate the testing process, making it easier to ensure that the application is functioning correctly.

**Lighthouse Testing**
-------------------

Amy introduced Lighthouse, a testing tool that can be used to test the performance and accessibility of a website. She showed how to use Lighthouse to test a frontend application and generate a report.

**Playwright Testing**
------------------

Amy introduced Playwright, a testing tool that can be used to test the UI of a frontend application. She showed how to use Playwright to write tests for a frontend application and automate the testing process.

**Deploying to Netlify**
------------------------

Amy demonstrated how to deploy a frontend application to Netlify using GitHub Actions. She showed how to set up a workflow that deploys the application to Netlify and generates a report.

**Closing**
----------

Amy concluded her talk by summarizing the key points she covered. She emphasized the importance of frontend testing and the benefits of using GitHub Actions to automate the testing process. She encouraged the audience to try out GitHub Actions and explore its features.

**Q&A**
------

The Q&A session that followed was lively, with attendees asking questions about GitHub Actions and frontend testing. Amy answered questions about the limitations of GitHub Actions and how to overcome common challenges.

**Conclusion**
----------

In conclusion, Amy's talk on frontend testing with GitHub Actions was informative and engaging. She demonstrated the power of GitHub Actions and its ability to automate the testing process. The audience left with a better understanding of the importance of frontend testing and the benefits of using GitHub Actions to automate the testing process.

---

**Additional Resources**
------------------------

* GitHub Actions documentation: <https://docs.github.com/en/actions>
* Lighthouse documentation: <https://developer.chrome.com/docs/lighthouse/>
* Playwright documentation: <https://playwright.dev/docs/>
* Netlify documentation: <https://www.netlify.com/docs/>

---

**About the Author**
-------------------

Amy is a frontend developer with a passion for frontend testing. She has given talks on frontend testing and has experience working with GitHub Actions.


## How JavaScript Happened: A Short History of Programming Languages - Mark Rendle - NDC London 2024

URL: [https://www.youtube.com/watch?v=lWeLaEo4pn0](https://www.youtube.com/watch?v=lWeLaEo4pn0)

**Chapter 5: The FizzBuzz Saga: A Journey Through Programming Languages**

The NDC 2024 London conference was a thrilling experience, with speakers from all over the world sharing their expertise and insights on various topics. One of the most entertaining talks was given by Mark Renle, a seasoned software developer, who took the audience on a wild ride through the history of programming languages.

**The FizzBuzz Saga**

Mark Renle's talk, titled "The FizzBuzz Saga: A Journey Through Programming Languages," was a hilarious and informative exploration of the evolution of programming languages. He began by asking the audience if they knew what FizzBuzz was, and then proceeded to explain that it was a simple program that prints out numbers from 1 to 100, with "Fizz" for multiples of 3 and "Buzz" for multiples of 5.

But what made Mark's talk so entertaining was his journey through the various programming languages that have been created over the years, each with its own quirks and challenges. He started with Fortran, the first programming language, which was developed in the 1950s. Mark showed the audience how Fortran was used to calculate trajectories for tanks during World War II, and how it was the first language to introduce the concept of structured programming.

**The Dawn of Imperative Programming**

Next, Mark took the audience on a journey through the dawn of imperative programming, with languages like C and Smalltalk. He showed how C, developed in the 1970s, was the first language to introduce the concept of structured programming, with its use of functions, loops, and conditional statements. He also discussed how Smalltalk, developed in the 1970s, was the first language to introduce the concept of object-oriented programming.

Mark then took the audience on a tour of the 1980s, exploring languages like APL and ML. He showed how APL, developed in the 1960s, was a functional programming language that was used for scientific calculations and data analysis. He also discussed how ML, developed in the 1970s, was a functional programming language that was used for artificial intelligence and computer vision.

**The Rise of Object-Oriented Programming**

The 1990s saw the rise of object-oriented programming, with languages like Java and C++. Mark showed how Java, developed in the 1990s, was designed to be a platform-independent language that could run on any device with a JVM. He also discussed how C++, developed in the 1980s, was a powerful language that added object-oriented programming to the C language.

Mark then took the audience on a journey through the 2000s, exploring languages like JavaScript and Python. He showed how JavaScript, developed in the 1990s, was initially used for client-side scripting on the web, but has since become a popular language for building web applications. He also discussed how Python, developed in the 1990s, was a language that emphasized code readability and simplicity, and has since become a popular language for data science and machine learning.

**The FizzBuzz Challenge**

Throughout his talk, Mark used the FizzBuzz program as a challenge to demonstrate the quirks and challenges of each language. He showed how the FizzBuzz program could be implemented in each language, highlighting the different syntax and semantics of each language. He also showed how each language's implementation of the FizzBuzz program could be optimized for performance, highlighting the trade-offs between readability and performance.

**The Future of Programming Languages**

Finally, Mark concluded his talk by discussing the future of programming languages. He showed how the rise of functional programming and reactive programming has led to the development of new languages like Haskell and Elm. He also discussed how the growth of the web and mobile devices has led to the development of new languages like JavaScript and Swift.

Throughout his talk, Mark emphasized the importance of understanding the history of programming languages and the quirks and challenges of each language. He showed how the FizzBuzz program could be used as a challenge to demonstrate the capabilities and limitations of each language, and how this could help developers learn and grow.

**Conclusion**

Mark Renle's talk, "The FizzBuzz Saga: A Journey Through Programming Languages," was a hilarious and informative exploration of the evolution of programming languages. He took the audience on a wild ride through the history of programming languages, from Fortran to JavaScript, highlighting the quirks and challenges of each language. He demonstrated the FizzBuzz program in each language, showing how each language's implementation could be optimized for performance, and emphasizing the importance of understanding the history of programming languages.


## Tracking Aircraft with Streams + Software-Defined Radio - Guy Royse - NDC London 2024

URL: [https://www.youtube.com/watch?v=25z9bRo_t0c](https://www.youtube.com/watch?v=25z9bRo_t0c)

**Chapter 5: "Software Defined Radio and Event Streaming"**

The NDC 2024 London conference was filled with exciting talks and presentations, and one of the most fascinating sessions was about software defined radio and event streaming. The speaker, a seasoned developer, shared his journey of building a software defined radio system using the ReduStack framework.

**The Talk**

The talk began with an introduction to the concept of software defined radio and its potential applications. The speaker explained that software defined radio is a type of radio communication system that uses software to process and transmit radio signals, rather than traditional hardware. This allows for greater flexibility and customization, as well as the ability to use a wide range of frequencies and modulation schemes.

The speaker then delved into the details of the ReduStack framework, which is a popular open-source framework for building software defined radio systems. He explained that ReduStack provides a set of tools and libraries that make it easy to build and deploy software defined radio systems, including a GUI-based configuration tool and a set of APIs for programming the radio hardware.

The speaker then showed a live demo of the software defined radio system in action, using a ReduStack-based system to receive and decode ADS-B (Automatic Dependent Surveillance-Broadcast) signals from aircraft. He explained that ADS-B is a surveillance system used by aircraft to broadcast their location and altitude, and that the ReduStack system can be used to receive and decode these signals in real-time.

**Event Streaming**

The speaker then moved on to the topic of event streaming, which is a key component of the ReduStack system. He explained that event streaming is a way of processing and transmitting data in real-time, allowing for the creation of real-time dashboards and other applications that can consume and process the data in real-time.

The speaker demonstrated the event streaming capabilities of the ReduStack system, showing how it can be used to stream ADS-B data to a dashboard application. He explained that the event streaming system uses a combination of technologies, including WebSocket and WebSockets, to stream data from the radio hardware to the dashboard application.

**Building the System**

The speaker then walked the audience through the process of building the software defined radio system using the ReduStack framework. He explained that the system consists of several components, including a radio hardware module, a ReduStack-based software module, and a dashboard application.

The speaker showed how to configure the radio hardware module using the ReduStack GUI-based configuration tool, and how to write custom code to process the received signals using the ReduStack APIs. He also demonstrated how to create a dashboard application using a front-end framework, and how to integrate the dashboard application with the ReduStack system using the event streaming APIs.

**Conclusion**

The talk concluded with a Q&A session, during which the speaker answered questions from the audience and provided additional insights into the ReduStack system and its applications. The audience was impressed by the complexity and flexibility of the ReduStack system, and the potential applications it has in fields such as aviation, transportation, and IoT.

Overall, the talk was an excellent introduction to the world of software defined radio and event streaming, and provided valuable insights into the ReduStack framework and its capabilities. The speaker's enthusiasm and expertise were contagious, and the audience was left feeling inspired and motivated to learn more about this exciting technology.

**Resources**

For those interested in learning more about the ReduStack framework and software defined radio, the speaker provided several resources, including:

* The ReduStack GitHub repository
* The ReduStack documentation
* The ReduStack community forums

Additionally, the speaker encouraged the audience to join the ReduStack community on Discord and participate in the development and testing of the framework.

**QR Code**

For those who want to try out the ReduStack system for themselves, the speaker provided a QR code that links to the ReduStack GitHub repository. The QR code is available below:

[QR code: https://github.com/ReduStack/ReduStack](https://github.com/ReduStack/ReduStack)

I hope you enjoyed this chapter! Let me know if you have any questions or if you'd like to learn more about software defined radio and event streaming.


## Let's talk about DX, Baby! - Jo Franchetti - NDC London 2024

URL: [https://www.youtube.com/watch?v=YkOGZCYWT6w](https://www.youtube.com/watch?v=YkOGZCYWT6w)

# NDC 2024 London: The Power of Developer Experience

**Chapter 3: The Human Aspect of Developer Experience**

At the NDC 2024 London conference, Joe Franti, a developer advocate, took the stage to talk about the importance of developer experience. In his presentation, he emphasized the human aspect of developer experience, highlighting the need to consider the emotional and psychological factors that affect developers when working with code.

**The Journey of a Developer**

Joe began by sharing his own personal experience as a developer. He talked about the frustration and struggle he faced when trying to integrate a new tool, Kod Magig, into his workflow. He described the feeling of being overwhelmed by the complexity of the tool, the lack of proper documentation, and the unhelpful community support.

Joe's story resonated with the audience, who could relate to the struggles they had faced as developers. He highlighted the importance of creating a smooth onboarding process, providing clear and concise documentation, and building a supportive community to help developers overcome the challenges they encounter.

**The Cognitive Load**

Joe then delved into the concept of cognitive load, which refers to the mental effort required to process information. He explained how developers' cognitive load can be increased by factors such as poor code organization, unclear documentation, and unnecessary complexity. He emphasized the need to reduce cognitive load by simplifying code, providing clear instructions, and making the development process more efficient.

**The Importance of Feedback**

Joe discussed the importance of feedback in the development process. He highlighted the need for developers to receive feedback in a timely manner, whether it's through code reviews, testing, or bug reports. He emphasized the importance of using descriptive language, clear instructions, and concise communication to ensure that feedback is effective and actionable.

**The Power of Storytelling**

Joe used storytelling to convey the importance of developer experience. He shared a personal anecdote about a project he worked on, where he had to refactor a piece of code to make it more readable and maintainable. He explained how he used descriptive variable names, clear comments, and a logical structure to make the code easier to understand.

**The Need for Simplicity**

Joe emphasized the need for simplicity in code and documentation. He highlighted the importance of using clear and concise language, avoiding unnecessary complexity, and making the development process more efficient. He explained how simplicity can help reduce cognitive load, improve readability, and increase productivity.

**The Role of Code Review**

Joe discussed the role of code review in the development process. He emphasized the importance of code reviews in identifying errors, improving code quality, and ensuring that code meets the required standards. He highlighted the need for constructive feedback, clear communication, and a collaborative approach to code review.

**The Impact of Developer Experience on Productivity**

Joe talked about the impact of developer experience on productivity. He explained how a smooth onboarding process, clear documentation, and a supportive community can increase developer productivity, reduce errors, and improve code quality. He emphasized the need for developers to feel engaged, motivated, and supported throughout the development process.

**Conclusion**

In conclusion, Joe's presentation emphasized the importance of developer experience in the development process. He highlighted the need to consider the human aspect of developer experience, including cognitive load, feedback, storytelling, simplicity, and code review. He emphasized the importance of creating a smooth onboarding process, providing clear and concise documentation, and building a supportive community to help developers overcome the challenges they encounter.

**Additional Resources**

For those interested in learning more about developer experience, Joe provided the following resources:

* A paper on the importance of developer experience by Tim Cochran
* A link to the NDC 2024 London conference website for more information on the conference and its speakers
* A QR code to share the presentation on Twitter

**Final Thoughts**

As the presentation came to a close, Joe thanked the audience for their attention and encouraged them to consider the importance of developer experience in their own work. He emphasized the need for developers to feel engaged, motivated, and supported throughout the development process, and encouraged them to share their own stories and experiences with the community.

As the audience filed out of the conference hall, they were left with a newfound appreciation for the importance of developer experience and a renewed commitment to creating a more supportive and collaborative development community.


## Co-Create: Creating Better Together - Denise Jacobs - NDC London 2024

URL: [https://www.youtube.com/watch?v=QRwjl6Pme44](https://www.youtube.com/watch?v=QRwjl6Pme44)

**Chapter 5: Unblocking Creativity and Collaboration at NDC 2024 London**

At the NDC 2024 London conference, I had the privilege of attending a thought-provoking session on unblocking creativity and collaboration. The speaker, a renowned expert in the field, shared their insights on how to overcome common obstacles and foster a culture of innovation and teamwork. In this chapter, I'll summarize the key takeaways from the session and provide some additional context to help readers better understand the concepts discussed.

**Fear of Judgment and Self-Censorship**

The speaker began by highlighting the importance of acknowledging the fear of judgment and self-censorship that many people experience when sharing their ideas. They shared a personal anecdote about how they, too, had struggled with this issue in the past. The speaker emphasized that it's essential to recognize that everyone has a unique perspective and that sharing ideas is a crucial step in the creative process.

**The Power of Improv**

The speaker then introduced the concept of improv and its application in the context of collaboration. They explained that improv is about embracing the unknown and being open to new ideas and perspectives. The speaker shared a humorous anecdote about a time when they used improv to come up with a creative solution to a problem. They emphasized that this approach can help individuals feel more comfortable sharing their ideas and build trust with their colleagues.

**The Importance of Psychological Safety**

The speaker discussed the concept of psychological safety, which refers to the feeling of being able to share ideas without fear of judgment or reprisal. They emphasized that this is a critical component of effective collaboration and that leaders should prioritize creating a culture of psychological safety within their teams.

**The Role of Mindfulness**

The speaker also touched on the importance of mindfulness in the creative process. They explained that mindfulness can help individuals stay present and focused, allowing them to tap into their creativity and think outside the box.

**The Value of Feedback**

The speaker highlighted the importance of feedback in the creative process. They emphasized that feedback is essential for growth and improvement, but that it should be constructive and respectful. The speaker shared a personal anecdote about how they had received feedback that had helped them to improve their work.

**The Power of Play**

The speaker concluded the session by emphasizing the importance of play in the creative process. They explained that play can help individuals think outside the box and come up with innovative solutions. The speaker shared a humorous anecdote about a time when they had used play to come up with a creative solution to a problem.

**Additional Takeaways**

In addition to the key takeaways mentioned above, the speaker also emphasized the importance of:

* Being open-minded and willing to learn
* Embracing failure and using it as an opportunity to learn and grow
* Building trust and rapport with colleagues
* Creating a culture of psychological safety within teams
* Fostering a sense of community and belonging among team members

**Conclusion**

In conclusion, the NDC 2024 London session on unblocking creativity and collaboration provided valuable insights and practical tips for overcoming common obstacles and fostering a culture of innovation and teamwork. By embracing the concepts discussed, individuals can improve their ability to share ideas, build trust with their colleagues, and create a culture of psychological safety within their teams.


## Azure Cosmos DB - Low Latency and High Availability at Planet Scale - Kevin Pilch

URL: [https://www.youtube.com/watch?v=8FU5a60FKEo](https://www.youtube.com/watch?v=8FU5a60FKEo)

**NDC 2024 London: A Look Back**

The NDC 2024 London conference was a thrilling event that brought together some of the brightest minds in the tech industry. One of the most talked-about sessions was Kevin Pilch's talk on "Building Resilient and Scalable Distributed Systems with Cosmos DB". In this chapter, we'll dive into the transcript of his talk and explore the key takeaways.

**Building Resilient and Scalable Distributed Systems with Cosmos DB**

**Kevin Pilch**

Microsoft Azure Cosmos DB Team

---

Hello everyone, I'm Kevin Pilch, and I work on the Azure Cosmos DB team. I've been at Microsoft for 21 years, and I spent the first 20 years working on the .NET language runtime, tooling, and Visual Studio. I'm now an engineer manager, and I manage a team of 30 people.

Last year, I decided to switch to the Cosmos DB team, and it's been a fantastic experience. I'm excited to be here today to talk about building resilient and scalable distributed systems with Cosmos DB.

**A Brief History of Cosmos DB**

Cosmos DB was started in 2010 by a Microsoft technical fellow, Dharma Shukla. It was initially codenamed "Project Florence" internally. By 2014, it was already powering critical services within Azure and Microsoft. We launched publicly in 2017 and added multi-API support for Gremlin and Cassandra in 2018.

**The Anatomy of a Cosmos DB Request**

Let's take a look at the anatomy of a Cosmos DB request. We have the API Gateway, which is responsible for routing requests to the correct backend node. We have the backend nodes, which are responsible for storing and retrieving data. We have the storage engine, which is responsible for storing the data. And we have the replication engine, which is responsible for replicating the data across different regions and fault domains.

**Replication and Consistency**

When it comes to replication and consistency, Cosmos DB uses a quorum-based protocol. This means that we require a certain number of nodes to agree on the state of the data before considering it committed. This provides strong consistency and durability.

However, this comes at the cost of higher latency and lower availability. To mitigate this, we offer a variety of consistency models, including strong consistency, bounded staleness, and session consistency.

**Elasticity and Autoscaling**

Cosmos DB is designed to be highly elastic and autoscaling. We use a container-based architecture, which allows us to easily add or remove containers as needed. We also use a resource governance system, which allows us to monitor and control resource usage.

**Autoscaling with Azure DevOps**

We use Azure DevOps to automate our testing and deployment pipeline. We run performance tests every time we check in code, and we use a canary deployment strategy to ensure that our changes don't affect our users.

**Load Balancing and Partitioning**

When it comes to load balancing and partitioning, we use a combination of Azure Load Balancer and Azure Traffic Manager. We also use a custom-built load balancer that allows us to route traffic to the correct partition.

**Query Optimization**

When it comes to query optimization, we use a variety of techniques, including caching, indexing, and query optimization. We also use a query optimizer that can re-arrange the query plan to minimize the number of trips to the storage engine.

**Conflict Resolution**

When it comes to conflict resolution, we use a variety of techniques, including optimistic concurrency and pessimistic concurrency. We also use a conflict resolution algorithm that can detect and resolve conflicts automatically.

**Conclusion**

Building resilient and scalable distributed systems with Cosmos DB requires careful planning and execution. We've covered a lot of ground today, from the anatomy of a Cosmos DB request to conflict resolution. I hope you've learned something new and valuable today. Thank you for your attention, and I welcome any questions you may have.

**Q&A**

Q: Can you explain more about the conflict resolution algorithm?

A: Sure! Our conflict resolution algorithm is based on a combination of optimistic and pessimistic concurrency control. We use a version vector to track the version of the data, and we use a timestamp to determine the order of the updates.

Q: Can you explain more about the query optimization algorithm?

A: Sure! Our query optimization algorithm is based on a combination of caching, indexing, and query rewriting. We use a query plan that minimizes the number of trips to the storage engine, and we use a cost-based optimizer that chooses the most efficient query plan.

Q: Can you explain more about the Azure DevOps pipeline?

A: Sure! We use a combination of Azure Pipelines and Azure DevOps to automate our testing and deployment pipeline. We run performance tests every time we check in code, and we use a canary deployment strategy to ensure that our changes don't affect our users.

Q: Can you explain more about the Cosmos DB SDK?

A: Sure! Our Cosmos DB SDK is a set of libraries that allow developers to interact with Cosmos DB programmatically. It's available for a variety of languages, including .NET, Java, Python, and Go.

---

I hope this chapter has given you a good understanding of the key takeaways from Kevin Pilch's talk on "Building Resilient and Scalable Distributed Systems with Cosmos DB". From the anatomy of a Cosmos DB request to conflict resolution, we've covered a lot of ground. I hope you've learned something new and valuable today.


## IaC Forged in Code: ARM/Bicep vs Terraform vs Pulumi - Mike Benkovich

URL: [https://www.youtube.com/watch?v=yLYot2CBzCE](https://www.youtube.com/watch?v=yLYot2CBzCE)

**Chapter 7: "Building Infrastructure Code with Bicep, Terraform, and Palumi" from NDC 2024 London**

The NDC 2024 London conference featured a wide range of talks and sessions on various topics, including cloud computing, infrastructure, and programming. One of the most popular sessions was "Building Infrastructure Code with Bicep, Terraform, and Palumi" presented by Mike Benovic. In this chapter, we will summarize the key takeaways from the session.

**Introduction**

Mike Benovic, a developer and evangelist for Microsoft, began the session by introducing himself and explaining the topic of the session. He emphasized the importance of infrastructure code in modern software development and the need for a reliable and efficient way to manage and deploy infrastructure.

**What is Bicep?**

Bicep is a new infrastructure as code (IaC) language developed by Microsoft. It is designed to be used with Azure, but it can be used with other cloud providers as well. Bicep is similar to Terraform, but it is more Azure-specific and has some key differences.

**What is Terraform?**

Terraform is a popular IaC tool developed by HashiCorp. It allows users to define infrastructure resources in a human-readable configuration file and then deploy them to a variety of cloud providers, including Azure, AWS, and Google Cloud.

**What is Palumi?**

Palumi is a new tool that allows users to create infrastructure code using a visual interface. It is designed to be easy to use and does not require extensive knowledge of programming languages like Terraform. Palumi is similar to Bicep, but it is more focused on visualizing infrastructure code and making it easier to manage and deploy.

**Using Bicep**

Mike began by demonstrating how to use Bicep to create infrastructure code. He showed how to define resources, such as resource groups, storage accounts, and virtual machines, using Bicep syntax. He also demonstrated how to use Bicep to create a simple web application using Azure Functions.

**Using Terraform**

Mike then demonstrated how to use Terraform to create infrastructure code. He showed how to define resources, such as resource groups, storage accounts, and virtual machines, using Terraform syntax. He also demonstrated how to use Terraform to create a simple web application using Azure Functions.

**Using Palumi**

Mike then demonstrated how to use Palumi to create infrastructure code. He showed how to create a simple web application using Palumi and then deploy it to Azure.

**Comparison of Bicep, Terraform, and Palumi**

Mike compared Bicep, Terraform, and Palumi, highlighting their similarities and differences. He emphasized that Bicep is more Azure-specific and has some key differences from Terraform. He also highlighted the benefits of using Palumi, such as its ease of use and visual interface.

**Conclusion**

Mike concluded the session by summarizing the key takeaways from the session. He emphasized the importance of infrastructure code in modern software development and the need for a reliable and efficient way to manage and deploy infrastructure. He also highlighted the benefits of using Bicep, Terraform, and Palumi, and encouraged attendees to try out these tools.

**Q&A Session**

The session ended with a Q&A session, where attendees asked questions and Mike answered them. Some of the questions included:

* How does Bicep compare to Terraform?
* Can I use Bicep with other cloud providers?
* How does Palumi compare to Bicep and Terraform?
* Can I use Palumi with Azure Functions?

**Conclusion**

In conclusion, Mike's session on "Building Infrastructure Code with Bicep, Terraform, and Palumi" was informative and engaging. He demonstrated the capabilities of each tool and highlighted their similarities and differences. The session was well-received by attendees, who appreciated the opportunity to learn about these new and innovative tools.


## Faster, Cheaper, Greener: Pick Three - Lea Mladineo - NDC London 2024

URL: [https://www.youtube.com/watch?v=hVxVXOZ127I](https://www.youtube.com/watch?v=hVxVXOZ127I)

**Chapter 7: "The Journey to a Greener Future: A Story of Sustainability and Innovation"**

At the 2024 NDC conference in London, I had the privilege of hearing a fascinating talk by La Im, a software engineer from a London-based startup, Fund UPS. La Im shared a real-life story of how their company tackled the challenge of building a sustainable future, not just for their business but also for the environment. In this chapter, I'll summarize the key takeaways from La Im's talk, highlighting the importance of sustainability, innovation, and the power of data.

**The Challenge**

La Im started by sharing the story of Fund UPS, a startup that began by signing clients and attracting bigger names. However, as the company grew, they faced a major challenge: building a sustainable future for their business and the environment. La Im explained that this was no small feat, as the company had to balance the needs of their clients with the need to reduce their carbon footprint.

**The Journey Begins**

La Im explained that the journey to sustainability began with a realization that their current infrastructure was not sustainable. They were using a monolithic architecture, which made it difficult to scale and maintain. They also had to deal with noisy neighboring issues, where one client's workload would impact the performance of another client's system. This was not only a technical challenge but also a business risk, as it could lead to lost revenue and reputation damage.

**The Solution**

La Im and his team decided to take a bold step and rebuild their infrastructure using a cloud-native architecture. They chose AWS as their cloud provider, which allowed them to take advantage of scalable and on-demand computing resources. They also implemented a distributed computing model, which enabled them to scale their workload more efficiently and reduce the risk of noisy neighboring issues.

**The Benefits**

The benefits of this new infrastructure were numerous. La Im explained that they were able to reduce their electricity usage by 70% and their carbon footprint by 90%. They also reduced their maintenance costs by 80%, as they no longer had to worry about patching and upgrading their own infrastructure. Most importantly, they were able to reduce their response time to clients, which improved their overall customer satisfaction.

**The Power of Data**

La Im emphasized the importance of data in their journey to sustainability. They used data to track their carbon footprint and identify areas for improvement. They also used data to optimize their infrastructure and reduce waste. For example, they used data to identify which clients were using the most resources and optimized their workload accordingly.

**The Future of Sustainability**

La Im concluded his talk by highlighting the importance of sustainability in the future of software engineering. He emphasized that sustainability is not just an environmental issue but also a business issue. Companies that fail to prioritize sustainability will be left behind, as clients and investors demand more environmentally responsible practices.

**Lessons Learned**

La Im's talk left me with several key takeaways:

1. **Sustainability is a business imperative**: Companies that fail to prioritize sustainability will be left behind.
2. **Data is key**: Data is essential for tracking progress, identifying areas for improvement, and optimizing infrastructure.
3. **Innovation is necessary**: Companies must be willing to innovate and adapt to new technologies and practices to stay ahead of the curve.
4. **The cloud can be a game-changer**: Cloud providers like AWS offer scalable and on-demand computing resources that can help companies reduce their carbon footprint and improve their sustainability.

**Conclusion**

La Im's talk was a powerful reminder of the importance of sustainability in software engineering. His company's journey to a greener future is a shining example of what can be achieved when companies prioritize sustainability and innovation. As software engineers, we have a responsibility to build systems that are not only efficient and scalable but also environmentally responsible. By embracing sustainability and innovation, we can build a better future for ourselves, our clients, and the planet.


## The Elephant in your Dataset: Addressing Bias in Machine Learning - Michelle Frost

URL: [https://www.youtube.com/watch?v=I79NNl0Ckqs](https://www.youtube.com/watch?v=I79NNl0Ckqs)

**Chapter 7: NDC 2024 London - Michelle Frost's Keynote on Addressing Bias in Machine Learning**

At the NDC 2024 London conference, Michelle Frost, Senior Developer at Crema Software Consultancy and a graduate student at Johns Hopkins, delivered a thought-provoking keynote on addressing bias in machine learning. Her presentation, titled "Addressing Bias in Machine Learning: A Discussion on Society, Machine Learning, and the Life Cycle," covered the importance of understanding and mitigating bias in AI systems.

**The Importance of Understanding Bias**

Michelle began by emphasizing the significance of understanding bias in machine learning. "I'm here today to talk about bias in machine learning. Bias is a term that's thrown around a lot, but I think it's important to define what we mean by bias. Bias is not just about being unfair or discriminatory; it's about the systematic error that occurs when a model is trained on a biased dataset." She highlighted that bias can occur at every stage of the machine learning life cycle, from data generation to deployment.

**The Life Cycle of Machine Learning**

Michelle explained that understanding the life cycle of machine learning is crucial in identifying potential sources of bias. She outlined the seven places where bias can be injected into the system:

1. **Data Generation**: "Historical bias" can occur during data generation, where data is often collected with biases towards certain groups or demographics.
2. **Data Aggregation**: "Aggregation bias" can occur when data is aggregated from different sources, leading to biased representations of certain groups.
3. **Model Building**: "Learning bias" can occur during model building, where assumptions are made about the data and the model is trained on biased data.
4. **Model Implementation**: "Implementation bias" can occur when the model is implemented, and the context is not taken into account.
5. **Evaluation**: "Evaluation bias" can occur during evaluation, where the model is tested on biased data or with biased metrics.
6. **Deployment**: "Deployment bias" can occur when the model is deployed, and the context is not taken into account.
7. **Historical Bias**: "Historical bias" can occur when the model is used, and the context is not taken into account.

**The Impact of Bias**

Michelle emphasized the impact of bias on machine learning models. "When we build biased models, we are perpetuating harm. We are creating systems that are not fair and just, and that can have significant consequences." She highlighted examples of biased AI systems, including Amazon's AI hiring tool, which was found to be biased against women, and the COMPAS algorithm, which was found to be biased against African Americans.

**Mitigating Bias**

Michelle discussed various techniques for mitigating bias in machine learning models. She emphasized the importance of:

1. **Preprocessing**: "Preprocessing is key. We need to make sure that our data is clean, and that we're not introducing bias during the preprocessing stage."
2. **Mitigation Techniques**: "There are several mitigation techniques we can use, such as debiasing, data augmentation, and adversarial training."
3. **Postprocessing**: "Postprocessing is also important. We need to make sure that our model is not introducing bias during the evaluation stage."
4. **Fairness Metrics**: "We need to use fairness metrics to evaluate our models. We can't just rely on accuracy metrics; we need to consider fairness metrics as well."
5. **Human-in-the-Loop**: "Human-in-the-loop is crucial. We need to involve humans in the machine learning process to ensure that our models are fair and just."

**Conclusion**

Michelle concluded her keynote by emphasizing the importance of understanding and mitigating bias in machine learning. "Bias is not just a technical issue; it's a societal issue. We need to take a holistic approach to addressing bias in machine learning, and we need to involve humans in the process. We can't just rely on algorithms to fix the problem; we need to use our human judgment and expertise to ensure that our models are fair and just."

**Q&A Session**

The Q&A session that followed was lively and engaging, with attendees asking questions about the importance of understanding bias, the impact of bias on machine learning models, and the techniques for mitigating bias. Michelle's keynote set the tone for the rest of the conference, emphasizing the importance of fairness and ethics in machine learning.

**Resources**

For those interested in learning more about addressing bias in machine learning, Michelle recommended the following resources:

* Virginia Eubanks' book "Automating Inequality"
* The "Responsible AI Toolkit" by Microsoft
* The "Fairness dashboard" by IBM
* The "Equalized Odds" metric
* The "Counterfactual" approach to fairness

**Conclusion**

Michelle's keynote on addressing bias in machine learning was a thought-provoking and informative session that emphasized the importance of understanding and mitigating bias in AI systems. Her discussion on the life cycle of machine learning and the techniques for mitigating bias provided valuable insights for attendees, and her emphasis on the importance of fairness and ethics in machine learning set the tone for the rest of the conference.


## Understand the Next Phase of Web Development - Steve Sanderson - NDC London 2024

URL: [https://www.youtube.com/watch?v=p9taQkF24Fs](https://www.youtube.com/watch?v=p9taQkF24Fs)

**NDC 2024 London: A Journey Through Web Development**

**Chapter 10: The Future of Web Development: Web Assembly and Serverless Computing**

As I walked into the NDC 2024 London conference, I was excited to learn about the latest trends and innovations in the world of web development. One of the most interesting sessions I attended was "The Future of Web Development: Web Assembly and Serverless Computing" presented by Steve, a developer architect at Microsoft. In this chapter, I will summarize the key takeaways from this session.

**Web Assembly: A New Era in Web Development**

Steve started by introducing the concept of Web Assembly (WASM), a new format for compiling code that allows for efficient and secure execution in web browsers. WASM is designed to bring the benefits of native code execution to the web, enabling developers to write high-performance, sandboxed code that runs at native speed.

Steve explained that WASM is not a new language, but rather a binary format that can be generated from a variety of languages, including C, C++, and Rust. This allows developers to use their existing skills and tools to build web applications that take advantage of WASM's performance and security benefits.

**The Web Assembly Ecosystem**

Steve highlighted the growing ecosystem around WASM, including the Wazi Preview 2, a new programming model that allows for the creation of web assembly components that can be used to build web applications. He also mentioned the WASM SDK, which provides a set of tools and libraries for building and debugging WASM applications.

**Serverless Computing**

The second part of the session focused on serverless computing, a trend that is rapidly gaining popularity in the web development community. Steve explained that serverless computing allows developers to build applications that can scale automatically, without the need for manual provisioning or management of servers.

Steve demonstrated how to build a simple web server using Rust and WASM, and showed how the web assembly component could be used to serve multiple pages and handle requests in a scalable and efficient manner.

**Case Study: Building a Web Server with Rust and WASM**

Steve provided a practical example of building a web server using Rust and WASM. He showed how to create a simple web server that serves markdown files, and demonstrated how the web assembly component can be used to format the markdown content before serving it to the user.

The example was impressive, and showed how WASM can be used to build high-performance, secure, and scalable web applications. Steve also highlighted the benefits of using Rust and WASM together, including improved security, performance, and maintainability.

**Conclusion**

In conclusion, Steve's session at NDC 2024 London provided a comprehensive overview of the future of web development, highlighting the potential of Web Assembly and serverless computing. The examples and demos provided during the session were impressive, and showed the potential for WASM to revolutionize the way we build web applications.

As I left the session, I was excited to learn more about WASM and serverless computing, and to explore the potential for using these technologies in my own projects. The session was a great reminder of the importance of staying up-to-date with the latest trends and innovations in the web development community, and I look forward to seeing what the future holds for WASM and serverless computing.

**Additional Resources**

For those interested in learning more about Web Assembly and serverless computing, Steve provided a list of additional resources, including:

* The Web Assembly GitHub repository
* The WASM SDK documentation
* The Rust programming language documentation
* The NDC 2024 London conference proceedings

I hope this chapter has provided a useful summary of Steve's session at NDC 2024 London. If you have any questions or comments, please feel free to reach out to me at [your email address].


## 2FA2Furious: New Threats to MFA - Nathaniel Okenwa - NDC London 2024

URL: [https://www.youtube.com/watch?v=IRJoIDj07Fg](https://www.youtube.com/watch?v=IRJoIDj07Fg)

**Chapter 12: Two-Factor Authentication - The Fast and Furious Approach**

At NDC 2024 London, the topic of Two-Factor Authentication (2FA) was a hot topic of discussion. Nathaniel Okena, a developer evangelist at twio, took the stage to share his insights on the importance of 2FA and how it can be implemented in a way that is both secure and user-friendly.

**The Problem with SMS-based 2FA**

Nathaniel started by highlighting the limitations of SMS-based 2FA. "We've all been there," he said. "You're trying to log into your account, and you receive a SMS with a code to verify your identity. But what happens when that SMS doesn't arrive? Or what if you're in a country where SMS traffic is heavy, and you're stuck waiting for what feels like an eternity for that code to arrive?"

Nathaniel pointed out that SMS-based 2FA is not a reliable solution. "We've all experienced it - the frustration of waiting for that SMS to arrive, only to realize that it's never coming. It's like trying to start a car with a faulty ignition - it just won't work."

**The Need for a Better Solution**

So, what's the alternative? Nathaniel believes that the answer lies in using a more secure and reliable form of 2FA. "We need to move away from SMS-based 2FA and towards more secure methods that don't rely on SMS traffic. We need a solution that is transparent, reliable, and easy to use."

**The Solution: twio's Verify**

twio's Verify is a 2FA solution that uses a combination of SMS and other verification methods to provide a more secure and reliable way to authenticate users. "With twio's Verify, you can send a verification code via SMS, but you can also use other methods like WhatsApp, email, or even a one-time password," Nathaniel explained. "And if the user is unable to receive the code, you can retry sending it, or even switch to a different method altogether."

Nathaniel demonstrated how twio's Verify works, showing how users can be prompted to enter a verification code, and how the system can automatically retry sending the code if it's not received. "It's like having a backup plan in case something goes wrong," he said.

**The Benefits of twio's Verify**

So, what are the benefits of using twio's Verify? Nathaniel highlighted several key advantages:

* **Security**: twio's Verify uses a combination of SMS and other verification methods to provide a more secure way to authenticate users.
* **Reliability**: With twio's Verify, you can retry sending a verification code if it's not received, or switch to a different method altogether.
* **Ease of use**: twio's Verify is easy to use, and users can be prompted to enter a verification code in a way that is transparent and easy to understand.
* **Transparency**: twio's Verify provides a clear and transparent way to authenticate users, making it easier to track and manage user authentication.

**The Future of 2FA**

As Nathaniel wrapped up his talk, he looked to the future of 2FA. "We're moving towards a world where 2FA is the norm, and where users expect a more secure and reliable way to authenticate online," he said. "We need to be prepared to adapt to changing user expectations and to provide solutions that meet those expectations. With twio's Verify, we're one step closer to making that a reality."

**Conclusion**

In conclusion, Nathaniel's talk highlighted the importance of moving away from SMS-based 2FA and towards more secure and reliable methods. twio's Verify is a solution that provides a combination of SMS and other verification methods to provide a more secure and reliable way to authenticate users. By using twio's Verify, developers can provide a better user experience, improve security, and reduce the risk of fraud and identity theft. As the future of 2FA continues to evolve, it's clear that solutions like twio's Verify will play a key role in shaping the way we authenticate online.


## The definitive deep dive into the .git folder - Rob Richardson - NDC London 2024

URL: [https://www.youtube.com/watch?v=H5ZQuuygH7E](https://www.youtube.com/watch?v=H5ZQuuygH7E)

**Chapter 4: Git: The Magic Sauce**

At NDC 2024 London, one of the most popular talks was on Git, the popular version control system. The speaker, Rob Rich, took the audience on a deep dive into the world of Git, sharing his insights and experiences with the technology. In this chapter, we'll explore the transcript of the talk, highlighting the key points and takeaways.

**The Power of Git**

Rob started by highlighting the power of Git, noting that it's a "magic sauce" that makes it easy to manage code. He explained that Git is a distributed version control system, which means that it allows multiple developers to work on the same codebase simultaneously. This is achieved through the concept of "commits," which are snapshots of the code at a particular point in time.

Rob demonstrated how to create a new Git repository, explaining that it's a simple process that involves initializing a new directory and running the `git init` command. He also showed how to add a file to the repository, using the `git add` command, and how to commit the changes using the `git commit` command.

**Git Hooks**

One of the key features of Git is the concept of "hooks," which are small scripts that can be run at various points during the Git workflow. Rob explained that hooks can be used to automate tasks, such as formatting code or running tests. He demonstrated how to create a hook using the `git hook` command, and showed how to use it to automatically format code.

Rob also discussed the different types of hooks available, including pre-commit hooks, post-commit hooks, and post-receive hooks. He explained that pre-commit hooks are run before a commit is made, while post-commit hooks are run after a commit is made. Post-receive hooks, on the other hand, are run when a repository is pushed to a remote server.

**Git Index**

Rob then turned his attention to the Git index, which is a critical component of the Git workflow. He explained that the index is a cache of the changes made to the code, and that it's used to speed up the commit process. He demonstrated how to use the `git add` command to stage changes, and how to use the `git status` command to view the status of the index.

Rob also discussed the concept of "stashing" changes, which allows developers to temporarily set aside changes without committing them. He explained that stashing is a useful feature when working on complex code changes, as it allows developers to quickly switch between different versions of the code.

**Git Remotes**

Rob then turned his attention to Git remotes, which are repositories that are stored on a remote server. He explained that remotes are used to share code between different developers, and that they can be used to track changes made to the code. He demonstrated how to create a remote repository using the `git init` command, and how to push changes to the remote repository using the `git push` command.

Rob also discussed the concept of "fetching" changes from a remote repository, which allows developers to retrieve changes made by others. He explained that fetching is a useful feature when working on a team, as it allows developers to stay up-to-date with the latest changes.

**Conclusion**

In conclusion, Rob's talk on Git was a comprehensive overview of the technology, covering everything from the basics of creating a new repository to the more advanced topics of hooks and remotes. He demonstrated how to use Git to manage code, and how to use it to collaborate with others. He also highlighted the key features of Git, including its distributed nature, its ability to track changes, and its support for hooks and remotes.

Throughout the talk, Rob emphasized the importance of understanding the underlying mechanics of Git, and how it can be used to improve the development process. He encouraged the audience to explore the world of Git, and to use it to take their code to the next level.

**Takeaways**

Here are some key takeaways from Rob's talk on Git:

* Git is a powerful version control system that makes it easy to manage code.
* Git is a distributed system, which means that it allows multiple developers to work on the same codebase simultaneously.
* Git has a variety of features, including hooks, remotes, and stashing, which can be used to automate tasks and improve the development process.
* Understanding the underlying mechanics of Git is critical to getting the most out of the technology.
* Git can be used to collaborate with others, and to track changes made to the code.

Overall, Rob's talk on Git was an excellent introduction to the technology, and provided a solid foundation for further exploration. Whether you're a seasoned developer or just starting out, Git is an essential tool that can help you take your code to the next level.


## Hannes LowetteBuild software like a bag of marbles, not a castle of LEGOÂ® -

URL: [https://www.youtube.com/watch?v=voPASBjHPu8](https://www.youtube.com/watch?v=voPASBjHPu8)

# NDC 2024 London: "The Marble of Complexity" by Hani

At the 2024 London edition of the Nordic Developer Conference (NDC), Hani took to the stage to deliver a thought-provoking talk titled "The Marble of Complexity". As the title suggests, Hani's presentation delved into the intricacies of software development, exploring the challenges and opportunities that arise when dealing with complexity.

### The Problem of Complexity

Hani began by acknowledging the elephant in the room: complexity. "We've all been there," he said, "where we're working on a project, and suddenly, it becomes a mess. We're dealing with multiple dependencies, multiple layers, and multiple moving parts." He emphasized that complexity is not just a problem, but a fundamental aspect of software development.

### The Lego Marble Analogy

To illustrate his point, Hani introduced the concept of the Lego marble. "Imagine you're building a Lego castle," he said. "You start with a simple foundation, and then you add more and more details. But as you do, the complexity grows. You start to wonder, 'How do I keep track of all these pieces? How do I ensure that everything fits together?' That's what I call the marble of complexity."

Hani argued that, just like a Lego marble, software systems can become incredibly complex, making it challenging to manage and maintain. He cited the example of a microservices architecture, where each service has its own dependencies, its own complexity, and its own set of moving parts.

### The Onion Architecture

To tackle this complexity, Hani introduced the concept of the onion architecture. "The onion architecture is a way of separating concerns," he explained. "It's a way of breaking down a system into layers, each with its own responsibilities. This allows us to focus on one layer at a time, rather than getting overwhelmed by the entire system."

Hani emphasized that the onion architecture is not a silver bullet, but rather a tool to help manage complexity. "It's not a magic solution," he said. "It's a way of thinking about how to structure our code, how to separate concerns, and how to make it easier to maintain."

### The Importance of Plugins

Another key concept Hani discussed was the importance of plugins. "Plugins are a way of extending the core functionality of our system," he said. "They allow us to add new features, new functionality, and new complexity, without having to rewrite the entire system."

Hani highlighted the benefits of using plugins, including the ability to decouple dependencies, to reuse code, and to make it easier to maintain and extend the system.

### The Challenges of Plugins

However, Hani also acknowledged the challenges of using plugins. "Plugins can introduce new complexity," he said. "They can introduce new dependencies, new layers, and new moving parts. They can make it harder to maintain and extend the system."

To address these challenges, Hani emphasized the importance of careful planning, careful design, and careful implementation. "We need to think carefully about how we structure our plugins, how we separate concerns, and how we make it easy to maintain and extend the system."

### The 10/10 Rule

Hani concluded his talk by introducing the 10/10 rule. "The 10/10 rule states that if you're working on a feature, and you're not sure whether to keep it or not, you should throw it away," he said. "If you're not sure whether it's worth the effort, you should throw it away. This rule helps us prioritize, it helps us focus on the most important things, and it helps us avoid getting bogged down in unnecessary complexity."

Hani emphasized that the 10/10 rule is not just a rule, but a mindset. "It's a way of thinking about how we approach software development, how we prioritize our work, and how we make it easier to maintain and extend our systems."

In conclusion, Hani's talk on "The Marble of Complexity" was a thought-provoking exploration of the challenges and opportunities that arise when dealing with complexity in software development. By introducing the concepts of the Lego marble, the onion architecture, and the importance of plugins, Hani provided a nuanced and actionable perspective on how to manage complexity and build more maintainable systems.

### References

* [1] Hani, "The Marble of Complexity" (NDC 2024 London)

Note: This chapter is a written summary of the talk by Hani at NDC 2024 London. The original talk may have included additional material, examples, and anecdotes that are not included in this summary.


## Architecture Modernization: Aligning Software, Strategy, and Structure - Nick Tune

URL: [https://www.youtube.com/watch?v=v9b0p0OMRqU](https://www.youtube.com/watch?v=v9b0p0OMRqU)

**Chapter 4: NDC 2024 London - Modernization and Architecture**

At the NDC 2024 London conference, one of the most thought-provoking talks was on the topic of modernization and architecture. The speaker, a seasoned expert in the field, shared his insights and experiences on how to approach modernization and build a successful architecture. This chapter summarizes the key points from the talk, highlighting the importance of understanding business goals, identifying key components, and adopting a modernization mindset.

**Understanding Business Goals**

The speaker emphasized the importance of understanding the business goals and objectives before embarking on a modernization journey. He stressed that it's crucial to have a clear understanding of the company's vision, mission, and values. This will help ensure that the modernization effort is aligned with the company's overall strategy and goals.

To achieve this, the speaker recommended conducting workshops and gathering input from different stakeholders, including business leaders, developers, and users. This will help identify the key components and subdomains that need to be modernized, as well as the business goals and objectives that need to be achieved.

**Identifying Key Components**

The speaker also highlighted the importance of identifying the key components and subdomains that need to be modernized. He emphasized that this is a crucial step in the modernization process, as it will help ensure that the effort is focused on the right areas.

To identify these components, the speaker recommended using techniques such as event storming and C4 diagrams. Event storming is a collaborative approach that involves gathering a group of stakeholders and asking them to identify the key events and processes that occur in the system. C4 diagrams, on the other hand, are a visual representation of the system's architecture, showing the components and subdomains that make up the system.

**Adopting a Modernization Mindset**

The speaker emphasized the importance of adopting a modernization mindset, which involves embracing change and being open to new ideas and approaches. He stressed that modernization is not just about upgrading technology, but about transforming the way the company operates and approaches its business.

To adopt this mindset, the speaker recommended creating a culture of continuous learning and experimentation. This involves encouraging a culture of experimentation, where teams are empowered to try new things and learn from their mistakes. It also involves embracing failure, and using it as an opportunity to learn and improve.

**The Strangler Fig Pattern**

The speaker also discussed the Strangler Fig pattern, which is a common problem that occurs during modernization. The Strangler Fig pattern occurs when a new system is built alongside an old system, but the old system is not fully retired. This can lead to a situation where the new system is not fully utilized, and the old system is still being maintained.

To avoid this problem, the speaker recommended using a gradual approach to modernization, where the new system is built incrementally and the old system is gradually phased out. This involves identifying the key components and subdomains that need to be modernized, and building the new system in small, incremental steps.

**C4 Diagrams**

The speaker also discussed C4 diagrams, which are a visual representation of the system's architecture. C4 diagrams show the components and subdomains that make up the system, as well as the relationships between them. They are a powerful tool for understanding the system's architecture and identifying areas for improvement.

The speaker emphasized the importance of using C4 diagrams to visualize the system's architecture, and to identify areas for improvement. He stressed that C4 diagrams are a valuable tool for communication, and can help to bridge the gap between technical and non-technical stakeholders.

**Conclusion**

In conclusion, the speaker emphasized the importance of understanding business goals, identifying key components, and adopting a modernization mindset. He stressed that modernization is not just about upgrading technology, but about transforming the way the company operates and approaches its business.

The speaker also emphasized the importance of using techniques such as event storming and C4 diagrams to identify the key components and subdomains that need to be modernized. He stressed that these techniques are valuable tools for communication, and can help to bridge the gap between technical and non-technical stakeholders.

Finally, the speaker emphasized the importance of adopting a gradual approach to modernization, and of using a modernization mindset to drive the effort. By following these principles, companies can successfully modernize their systems and achieve their business goals.


## Separating fact from fiction in a world of AI fairytales - Jodie Burchell - NDC London 2024

URL: [https://www.youtube.com/watch?v=lLNJld729bc](https://www.youtube.com/watch?v=lLNJld729bc)

# NDC 2024 London: A Chapter on Generative AI Models

**Title:** "Hype, Science, and the Future of Generative AI Models"

**Speaker:** Jody Burell, Developer Advocate, JetBrains

**Summary:** In this chapter, we'll dive into the world of generative AI models, exploring the hype, science, and future of these powerful technologies. Jody Burell, a developer advocate at JetBrains, will take us on a journey through the evolution of generative AI models, from their early beginnings to their current state-of-the-art applications. We'll examine the challenges, limitations, and potential of these models, as well as the impact they may have on our daily lives.

---

**The Journey of Generative AI Models**

Generative AI models have been making waves in the tech industry, with promises of revolutionizing the way we live and work. But what are they, and how did they get to this point? Let's start at the beginning.

Generative AI models are a type of artificial intelligence that can generate new, original content, such as images, music, text, or even entire videos. They're trained on vast amounts of data and learn to recognize patterns and relationships within that data. This allows them to create new content that's similar to the input data, but not necessarily identical.

The first generative AI models emerged in the 1980s, with the development of neural networks. These early models were simple, limited, and often produced nonsensical output. However, they paved the way for the more sophisticated models we see today.

**The Rise of Large Language Models**

One of the most significant milestones in the development of generative AI models was the creation of large language models. These models are trained on massive amounts of text data and can generate human-like language. They're the foundation for many modern applications, including chatbots, language translation, and text summarization.

The first large language model, GPT-1, was released in 2018. It was a game-changer, demonstrating the potential of generative AI models to generate coherent, readable text. Since then, subsequent models like GPT-2, GPT-3, and GPT-4 have pushed the boundaries of what's possible.

**The Hype and the Reality**

As generative AI models have gained popularity, so has the hype surrounding them. Many claim that these models will revolutionize industries, create new opportunities, and solve complex problems. While it's true that they have the potential to do so, it's essential to separate the hype from the reality.

Generative AI models are not yet perfect. They can struggle with tasks like understanding context, nuance, and subtlety. They can also be biased, hallucinate, and produce misinformation. These limitations are crucial to acknowledge, as they can have significant consequences.

**The Science of Generative AI**

So, what's behind the science of generative AI models? In a word: data. Generative AI models are trained on vast amounts of data, which allows them to learn patterns and relationships. The quality and quantity of this data are critical, as they determine the model's ability to generalize and adapt.

Another crucial aspect is the architecture of the model. Generative AI models typically use neural networks, which are designed to mimic the human brain. These networks are composed of layers, each of which processes and transforms the input data.

**The Future of Generative AI**

As we look to the future, what can we expect from generative AI models? The possibilities are endless. We may see them used in applications like content creation, language translation, and even art and music composition.

However, it's essential to approach these models with caution. We need to ensure that they're developed and used responsibly, with a focus on ethics, transparency, and accountability.

**Conclusion**

Generative AI models have come a long way since their inception. They've shown incredible potential, but it's crucial to separate the hype from the reality. As we move forward, it's essential to focus on the science, ethics, and responsible development of these models.

In this chapter, we've explored the journey of generative AI models, from their early beginnings to their current state-of-the-art applications. We've examined the challenges, limitations, and potential of these models, as well as the impact they may have on our daily lives.

As we continue to push the boundaries of what's possible with generative AI models, let's remember to approach them with a critical eye and a focus on responsible development. The future is bright, but it's up to us to ensure that it's also bright for everyone.


## How to fall in love with TDD - Gui Ferreira - NDC London 2024

URL: [https://www.youtube.com/watch?v=UkhZkK6f5Xw](https://www.youtube.com/watch?v=UkhZkK6f5Xw)

**NDC 2024 London: A Day of Insights and Inspiration**

**Chapter 7: "The Power of TDD: My Journey and Lessons Learned"**

As I walked into the NDC 2024 London conference, I was excited to hear the keynote speaker, [Speaker's Name], share their insights on Test-Driven Development (TDD). With a career spanning [Number] years, [Speaker's Name] has seen it all, and I was eager to learn from their experiences. The talk was titled "The Power of TDD: My Journey and Lessons Learned," and it was an eye-opening experience that left me with a newfound appreciation for the benefits of TDD.

**The Story of TDD**

[Speaker's Name] began by sharing a personal anecdote about how they got started with TDD. They recalled how they were once a skeptic, thinking that TDD was a waste of time, but eventually realized its true potential. They shared a story about how they were tasked with delivering a software project on a tight deadline, and how TDD helped them achieve it.

The speaker explained that TDD is not just about writing tests, but about driving the development process. It's about writing tests first, and then implementing the code to make those tests pass. This approach helps to ensure that the code is testable, maintainable, and reliable.

**The Benefits of TDD**

[Speaker's Name] went on to discuss the benefits of TDD, which I found particularly insightful. They mentioned that TDD:

* Helps to prevent bugs and errors
* Improves code quality and maintainability
* Reduces debugging time and stress
* Increases confidence in the code
* Fosters a culture of collaboration and communication among team members

The speaker also shared some statistics to back up these claims. For example, they mentioned that a study by Microsoft found that teams that used TDD saw a 40% reduction in bugs and errors.

**Common Misconceptions about TDD**

[Speaker's Name] also addressed some common misconceptions about TDD. They mentioned that many developers think that TDD is a waste of time, or that it's only for beginners. However, they argued that TDD is a powerful tool that can benefit developers of all levels.

They also debunked the myth that TDD is only for small projects. In fact, they shared a story about how they used TDD on a large-scale project, and how it helped them deliver the project on time and within budget.

**My Journey with TDD**

[Speaker's Name] then shared their own journey with TDD. They mentioned that they started using TDD on a small project, and were initially skeptical about its benefits. However, as they continued to use it, they saw the benefits firsthand.

They shared a story about how they were working on a project and realized that they had written a piece of code that was difficult to test. They decided to rewrite the code using TDD, and found that it was much easier to test and maintain.

**Lessons Learned**

[Speaker's Name] concluded their talk by sharing some lessons they learned along the way. They emphasized the importance of:

* Writing tests first
* Keeping tests simple and focused
* Not over-testing
* Fostering a culture of collaboration and communication among team members

They also shared some practical tips for implementing TDD, such as:

* Using a testing framework that is easy to use and integrates well with your development environment
* Writing tests in small, focused increments
* Using test doubles and mocks to isolate dependencies

**Conclusion**

As I left the talk, I was inspired to try TDD on my own projects. I realized that it's not just a tool for beginners, but a powerful approach that can benefit developers of all levels. [Speaker's Name]'s story and insights left me with a newfound appreciation for the benefits of TDD, and I'm excited to see how it will improve my own development process.

**Additional Resources**

For those who want to learn more about TDD, [Speaker's Name] recommended the following resources:

* The book "Test-Driven Development by Example" by Kent Beck
* The book "The Art of Readable Code" by Dustin Boswell and Trevor Foucher
* The online course "Test-Driven Development with Java" by Pluralsight

I hope this chapter has inspired you to try TDD on your own projects. Remember to keep tests simple, focused, and easy to understand, and to foster a culture of collaboration and communication among team members. Happy coding!


## REST, gRPC, SignalR and GraphQL for .NET developers - FranÃ§ois Bouteruche

URL: [https://www.youtube.com/watch?v=KXJgP3A3FRY](https://www.youtube.com/watch?v=KXJgP3A3FRY)

**NDC 2024 London: Keynote Speech**

**Title:** "Building a Strong Foundation: A Journey Through REST, gRPC, and GraphQL"

**Speaker:** [Name Not Provided]

**Date:** [Date Not Provided]

**Location:** London, UK

**Summary:** In this keynote speech, the speaker takes the audience on a journey through the world of REST, gRPC, and GraphQL, sharing their personal experiences and insights on building a strong foundation for modern web development.

**Introduction**

Thank you everyone for joining me at NDC London today. I'm incredibly honored to be speaking here today. I have to say, someone told me I'd be speaking at NDC one day, and I thought they were kidding! But here I am, and I'm really pleased to be here.

Before I dive into my presentation, I just want to take a step back and talk about the importance of building a strong foundation in web development. As developers, we're constantly being asked to learn new technologies, new frameworks, and new languages. But how do we know which ones to choose? How do we build a strong foundation for our projects?

That's what I want to talk about today. I'll be sharing my personal experiences and insights on building a strong foundation using REST, gRPC, and GraphQL.

**REST: The Beauty of Simplicity**

Let's start with REST. REST is a fundamental concept in web development, and yet, it's often overlooked. I'd like to share a personal anecdote. When I first started learning about REST, I was reading Roy Thomas's PhD dissertation on the subject. It was a bit daunting, to be honest! But as I read through it, I realized that REST is actually quite simple. It's all about the six constraints:

1. **Client-Server Architecture**: The client initiates the request, and the server responds.
2. **Stateless Interaction**: Each request contains all the necessary information.
3. **Cacheable**: Responses can be cached to improve performance.
4. **Uniform Interface**: A uniform interface is used for all interactions.
5. **Layered System**: The system is designed in a layered architecture.
6. **Code on the Client-side**: The client-side code is responsible for processing the response.

These constraints may seem simple, but they're the foundation of REST. And the beauty of REST is that it's easy to understand and implement. I've seen many developers struggle with REST, but once they understand these constraints, they're able to build robust and scalable systems.

**gRPC: The Power of Binary Data**

Now, let's talk about gRPC. gRPC is a high-performance RPC framework that allows you to build scalable and efficient systems. I'd like to share a personal experience with gRPC. I was working on a project that required real-time data updates. We were using REST, but it was taking a long time to update the data. We needed something faster, something that could handle the high volume of requests. That's when we discovered gRPC.

gRPC is built on top of HTTP/2, which allows for bidirectional streaming. This means that you can send and receive data simultaneously, without having to wait for a response. It's perfect for real-time applications.

One of the key benefits of gRPC is its binary data format. Unlike REST, which uses JSON or XML, gRPC uses a binary format that's more efficient and faster to parse. This makes it ideal for systems that require high-performance.

**GraphQL: The Future of Data Querying**

Finally, let's talk about GraphQL. GraphQL is a query language for APIs that allows you to query data in a more efficient and flexible way. I'd like to share a personal experience with GraphQL. I was working on a project that required a lot of data querying. We were using REST, but it was becoming increasingly complex. We needed a way to query the data more efficiently. That's when we discovered GraphQL.

GraphQL is designed to be flexible and efficient. You can query data in a hierarchical manner, which makes it ideal for complex data structures. And the best part is that you can query only the data you need, which reduces the amount of data transmitted over the wire.

One of the key benefits of GraphQL is its ability to handle complex queries. Unlike REST, which requires multiple requests to retrieve all the data, GraphQL allows you to retrieve all the data in a single request. This makes it ideal for systems that require complex data querying.

**Conclusion**

In conclusion, building a strong foundation in web development is crucial for success. REST, gRPC, and GraphQL are all important technologies that can help you build robust and scalable systems. By understanding the constraints of REST, the power of binary data in gRPC, and the flexibility of GraphQL, you can build a strong foundation for your projects.

I hope you've enjoyed this journey through the world of REST, gRPC, and GraphQL. Remember, building a strong foundation is key to success in web development. Take the time to learn these technologies, and you'll be well on your way to building robust and scalable systems.

**Q&A**

Q: Can you tell us more about the importance of documentation in RESTful APIs?

A: Absolutely! Documentation is crucial in RESTful APIs. It's essential to provide clear and concise documentation that explains the API's behavior, including the request and response formats, error handling, and caching mechanisms.

Q: How do you handle caching in gRPC?

A: In gRPC, caching is handled through the use of caches and cache invalidation mechanisms. You can use caching libraries like Redis or Memcached to cache responses and reduce the number of requests to the server.

Q: Can you explain the difference between GraphQL and REST?

A: GraphQL is a query language for APIs that allows you to query data in a more efficient and flexible way. REST is a fundamental concept in web development that provides a uniform interface for interacting with resources. While both technologies are used for API development, they serve different purposes and have different use cases.

Q: How do you handle errors in GraphQL?

A: In GraphQL, errors are handled through the use of error types and error messages. You can define custom error types and error messages to provide more detailed information about the error. Additionally, you can use error handling libraries like GraphQL Shield to handle errors more efficiently.

**Final Thoughts**

In conclusion, building a strong foundation in web development is crucial for success. By understanding the constraints of REST, the power of binary data in gRPC, and the flexibility of GraphQL, you can build robust and scalable systems. Remember to always prioritize documentation, caching, and error handling to ensure that your systems are efficient and reliable. And always be open to learning new technologies and frameworks to stay ahead of the curve. Thank you for listening, and I hope you've enjoyed this journey through the world of REST, gRPC, and GraphQL.


## Continuous Delivery for Legacy Code - Richard GroÃŸ - NDC London 2024

URL: [https://www.youtube.com/watch?v=djl2hJkzmGQ](https://www.youtube.com/watch?v=djl2hJkzmGQ)

**Chapter 10: Legacy Code and Continuous Delivery - A Talk by Richard**

At the NDC 2024 London conference, Richard, a developer and occasional software archaeologist, took the stage to talk about legacy code and continuous delivery. With a hint of humor and a dash of self-deprecation, he shared his experiences and insights on how to deal with the challenges of working with legacy code.

**The Problem with Legacy Code**

Richard started by acknowledging that legacy code is a problem that many of us have encountered. He joked that it can mean one of two things: "a problem we want to laugh at, or a problem we want to cry about." He admitted that he's a developer, occasionally a software archaeologist, and a nice kind auditor who tries to help people audit their code.

He introduced his company, Myan Wolf, a German law company consultancy, and got started on the topic of continuous delivery. Richard's first question was: "What is legacy code?" He explained that it's code that's hard to maintain, and when you change one line of code, you're afraid of what might happen. He asked the audience if they've ever experienced this, and many nodded in agreement.

**A Story of Woe**

Richard shared a story about a piece of software called Apocalypto, which was a custom off-the-shelf software that his team was tasked with auditing. The code was a mess, with job queues running separately, and missing actions that couldn't be compiled. The team discovered that the code was using VBA code, Excel, and VB script, and that it was a nightmare to work with.

Richard joked that the team's first reaction was to want to refactor everything, but they realized that they couldn't do that without understanding the code. They spent weeks trying to figure out how it worked, and eventually, they found that the code was using a database that was missing completely. The team was faced with a 5 million-line codebase, and they realized that they had to approach it differently.

**The Importance of Version Control**

Richard emphasized the importance of version control, saying that it's a tool that's often underutilized. He explained that without version control, it's like trying to debug a black box, and that it's essential to have a system in place to track changes and collaborate with others.

He showed a slide that said, "Continuous Delivery: A Journey, Not a Destination," and explained that it's a mindset that involves delivering software in a sustainable way. He emphasized that it's not just about deploying code, but about making it releasable, and that it requires a culture of collaboration and communication.

**The Power of Testing**

Richard talked about the importance of testing, saying that it's a crucial part of the development process. He explained that testing helps you understand the code, and that it's essential to have a good understanding of the code before you refactor it.

He showed a slide that said, "Test-Driven Development (TDD) is not just about writing tests, it's about writing tests that give you confidence," and explained that it's a technique that involves writing tests before you write the code. He emphasized that it's a mindset that involves thinking about the code before you write it, and that it's essential to have a good understanding of the requirements before you start coding.

**The Benefits of Refactoring**

Richard talked about the benefits of refactoring, saying that it's a process that involves improving the design and structure of the code. He explained that it's essential to refactor code to make it more maintainable, and that it's a crucial part of the development process.

He showed a slide that said, "Refactoring is not just about rewriting code, it's about improving the design and structure of the code," and explained that it's a technique that involves breaking down complex code into smaller, more manageable pieces. He emphasized that it's essential to have a good understanding of the code before you refactor it, and that it's a process that requires patience and discipline.

**The Importance of Culture**

Richard talked about the importance of culture, saying that it's a crucial part of the development process. He explained that culture involves the values and norms that guide the way we work, and that it's essential to have a culture of collaboration and communication.

He showed a slide that said, "Culture is not just about what we do, it's about how we do it," and explained that it's a mindset that involves thinking about the way we work, and how we can improve it. He emphasized that it's essential to have a culture that values collaboration, communication, and continuous improvement, and that it's a culture that requires leadership and commitment.

**Conclusion**

In conclusion, Richard emphasized that legacy code is a problem that many of us have encountered, and that it requires a different approach. He emphasized that continuous delivery is a mindset that involves delivering software in a sustainable way, and that it requires a culture of collaboration and communication. He encouraged the audience to approach legacy code with a fresh set of eyes, and to use tools like version control and testing to make it more maintainable. He also emphasized the importance of refactoring, and the benefits it can bring to the codebase.

Overall, Richard's talk was a humorous and insightful look at the challenges of working with legacy code, and the importance of adopting a mindset of continuous delivery. He provided practical advice and strategies for dealing with legacy code, and emphasized the importance of culture and collaboration in the development process.


## Designers vs Developers: Who is in control here? - Lemon ðŸ‹ - NDC London 2024

URL: [https://www.youtube.com/watch?v=eWssU6UtAfg](https://www.youtube.com/watch?v=eWssU6UtAfg)

**Chapter 6: "A Conversation About Collaboration"**

At the NDC 2024 London conference, I had the opportunity to attend a fascinating conversation about the importance of collaboration in software development. The speaker, a seasoned developer with years of experience, shared his thoughts on the topic, and I was struck by the depth of his insights. In this chapter, I will recount the conversation as it unfolded.

**The Power of Collaboration**

The speaker began by emphasizing the importance of collaboration in software development. He noted that while individual skill and expertise are essential, they are not enough to guarantee success. Collaboration, on the other hand, is what sets teams apart and enables them to achieve remarkable things.

He cited the example of a recent project he worked on, where a team of developers from different backgrounds and expertise came together to create a complex system. Despite the challenges they faced, the team was able to work together effectively, leveraging each other's strengths and weaknesses, and ultimately delivering a high-quality product.

**The Challenges of Collaboration**

However, the speaker also acknowledged that collaboration is not without its challenges. He noted that it requires a willingness to listen, to learn, and to adapt, as well as a commitment to communication and mutual respect.

He shared a personal anecdote about a project where he and his team encountered a difficult issue. Despite their best efforts, they were unable to resolve the problem, and it took several attempts before they finally found a solution. He credited the team's ability to work together, to communicate openly, and to trust each other with their ideas for their ultimate success.

**The Benefits of Collaboration**

The speaker went on to discuss the benefits of collaboration, including the ability to share knowledge, to learn from each other, and to create something greater than the sum of its parts.

He emphasized that collaboration is not just about working together, but also about building relationships, trust, and a sense of community. He noted that when team members feel valued and respected, they are more likely to be motivated, engaged, and productive.

**The Role of Technology in Collaboration**

The speaker also touched on the role of technology in collaboration, noting that it can both facilitate and hinder the process. He emphasized the importance of choosing the right tools and platforms to support collaboration, and of using them in a way that enhances communication and productivity.

He shared an example of a project where his team used a collaboration platform to work together remotely, and how it allowed them to stay connected and focused, despite being geographically dispersed.

**Conclusion**

In conclusion, the speaker emphasized the importance of collaboration in software development, noting that it is essential for creating high-quality products and for building successful teams. He encouraged the audience to prioritize collaboration, to communicate openly and honestly, and to trust each other, and to recognize the benefits of working together.

He ended his talk by saying that "collaboration is the key to unlocking the full potential of software development. It's not just about working together, but about building relationships, trust, and a sense of community. When we collaborate, we can achieve amazing things."

**Note**

The speaker's talk was engaging, informative, and inspiring, and it left the audience with a lot to think about. His emphasis on the importance of collaboration, communication, and trust resonated with many, and his examples and anecdotes made the concepts more relatable and tangible.

As I left the conference, I couldn't help but reflect on the importance of collaboration in my own work and in my personal relationships. I realized that collaboration is not just a buzzword or a trend, but a fundamental aspect of human connection and creativity.


## Not all attacks are malicious: DDoS from outside and within - Ingrid Guren & John Arne S. Pedersen

URL: [https://www.youtube.com/watch?v=ZlyVz3xoUHg](https://www.youtube.com/watch?v=ZlyVz3xoUHg)

# NDC 2024 London: A Technical Deep Dive into Norwegian Broadcasting Corporation's (NRK) Technology Stack

At the 2024 NDC London conference, I had the opportunity to attend a fascinating talk by Gan and Yan, tech leads from the Norwegian Broadcasting Corporation (NRK), about their technology stack and experiences with handling massive traffic spikes. In this chapter, I will summarize the key takeaways from their presentation.

### Introduction

NRK is Norway's public service broadcaster, and as such, it has a commitment to providing high-quality public services to its users. With a significant portion of its users accessing its services online, NRK has had to develop a robust technology stack to handle the demands of its audience. In this talk, Gan and Yan delved into the technical details of NRK's technology stack, sharing their experiences with handling massive traffic spikes and the lessons they learned along the way.

### Content Delivery Network (CDN) and Denial of Service (DoS) Attacks

Gan and Yan began by discussing NRK's use of a Content Delivery Network (CDN) to distribute its content and improve performance. They explained that NRK uses a CDN to cache its content, which helps to reduce the load on its servers and improve response times. However, this caching can also make it easier for attackers to launch Denial of Service (DoS) attacks, which can overwhelm the CDN and bring down NRK's services.

To mitigate this risk, NRK uses a combination of techniques, including:

* **Akamai's CDN**: NRK uses Akamai's CDN to distribute its content and protect against DoS attacks. Akamai's CDN is designed to handle massive traffic spikes and is able to detect and block suspicious traffic patterns.
* **Web Application Firewall (WAF)**: NRK uses a WAF to filter incoming traffic and block malicious requests. The WAF is configured to detect and block traffic patterns that are typical of DoS attacks.
* **Rate Limiting**: NRK implements rate limiting on its APIs to prevent excessive requests from overwhelming its servers. This helps to prevent DoS attacks and ensures that its services remain available to its users.

### Load Testing and Scalability

Gan and Yan also discussed NRK's approach to load testing and scalability. They explained that NRK uses a combination of load testing tools, including:

* **k6**: NRK uses k6, an open-source load testing tool, to simulate user traffic and test its services. k6 is designed to be highly scalable and can simulate thousands of users simultaneously.
* **Gatling**: NRK also uses Gatling, a commercial load testing tool, to test its services. Gatling is designed to be highly scalable and can simulate massive traffic spikes.

NRK's approach to scalability is centered around the idea of "fail-fast" development. This means that NRK builds its services to fail quickly and safely, rather than trying to build a monolithic system that can handle all possible loads. This approach allows NRK to quickly identify and fix issues, rather than waiting for problems to arise.

### Security and Authentication

Gan and Yan also discussed NRK's approach to security and authentication. They explained that NRK uses a combination of security measures, including:

* **OAuth 2.0**: NRK uses OAuth 2.0 to authenticate users and provide secure access to its services. OAuth 2.0 is a widely-used authentication protocol that is designed to be highly secure.
* **JSON Web Tokens (JWT)**: NRK uses JWT to provide secure authentication and authorization for its services. JWT is a widely-used authentication protocol that is designed to be highly secure.
* **HTTPS**: NRK uses HTTPS to encrypt all communication between its users and its services. HTTPS is a widely-used encryption protocol that is designed to be highly secure.

### Lessons Learned

Gan and Yan concluded their talk by sharing some of the lessons they had learned from their experiences with NRK's technology stack. They emphasized the importance of:

* **Testing and load testing**: NRK's approach to testing and load testing has been instrumental in identifying and fixing issues before they become critical.
* **Scalability**: NRK's approach to scalability has allowed it to handle massive traffic spikes and ensure that its services remain available to its users.
* **Security**: NRK's approach to security has helped to protect its users and its services from malicious attacks.

In conclusion, NRK's technology stack is designed to provide high-quality public services to its users, while also handling massive traffic spikes and ensuring the security and availability of its services. The lessons learned from NRK's experiences can be applied to any organization looking to build a robust and scalable technology stack.


## How GitHub delivers GitHub using GitHub - April Yoho - NDC London 2024

URL: [https://www.youtube.com/watch?v=-XYtl8pP-QI](https://www.youtube.com/watch?v=-XYtl8pP-QI)

# NDC 2024 London - A Journey Through DevOps and GitHub

As I walked into the conference hall at NDC 2024 London, I was excited to learn about the latest trends and innovations in the world of software development. The first talk I attended was by April Edwards, Senior Developer Advocate at GitHub, titled "A Journey Through DevOps and GitHub". April's talk was an engaging and informative exploration of the world of DevOps and GitHub, and I was thrilled to write it up as a chapter in this book.

**The Importance of DevOps**

April started her talk by emphasizing the importance of DevOps in today's software development landscape. She pointed out that the traditional waterfall approach to software development, where developers build software and then hand it over to operations teams to deploy, is no longer sufficient. With the increasing complexity and speed of software development, DevOps has become a crucial aspect of the development process.

"DevOps is not just about tools, it's about culture, it's about people, and it's about process," April emphasized. "It's about bringing together development and operations teams to work together to deliver high-quality software quickly and reliably."

**The Role of GitHub in DevOps**

April then turned her attention to the role of GitHub in DevOps. She explained that GitHub is not just a version control system, but a platform that enables collaboration, automation, and continuous integration and delivery.

"GitHub is not just a place to store your code," April said. "It's a place to collaborate with your team, to automate your processes, and to deliver your software to your users quickly and reliably."

**The GitHub Platform**

April gave a tour of the GitHub platform, highlighting its features and capabilities. She showed how GitHub's code review and pull request system enables developers to collaborate on code, and how its issue tracking system enables teams to track and prioritize issues.

She also demonstrated GitHub's automation capabilities, such as GitHub Actions, which enables developers to automate tasks such as building, testing, and deploying their code.

**The Importance of Automation**

April emphasized the importance of automation in DevOps, citing a study that showed that automated testing and deployment can reduce deployment time by up to 90%.

"Automation is not just about saving time, it's about reducing errors and improving quality," April said. "It's about enabling your teams to focus on what they do best, rather than getting bogged down in manual tasks."

**The Future of DevOps and GitHub**

Finally, April looked to the future of DevOps and GitHub, highlighting some of the exciting new features and capabilities that are coming down the pipeline.

"We're always looking for ways to make GitHub better and more powerful," April said. "We're working on new features such as GitHub Copilot, which will enable developers to write code faster and more accurately. We're also working on improving our automation capabilities, and on making it easier for teams to collaborate and work together."

**Conclusion**

As April wrapped up her talk, I was left with a sense of excitement and optimism about the future of DevOps and GitHub. It's clear that GitHub is committed to enabling developers to build high-quality software quickly and reliably, and that the company is constantly innovating and improving its platform to meet the needs of its users.

As I left the conference hall, I couldn't help but feel grateful to have had the opportunity to learn from April's expertise and to be a part of the vibrant and dynamic community of developers and DevOps practitioners that is NDC 2024 London.


## Keep your nose out of it. Denying yourself access to production - Glenn F. Henriksen

URL: [https://www.youtube.com/watch?v=QnfbdjeCvx4](https://www.youtube.com/watch?v=QnfbdjeCvx4)

**Chapter 5: "Security in the Modern Era: Lessons from NDC 2024 London"**

As I walked into the auditorium at NDC 2024 London, I couldn't help but feel a sense of excitement and anticipation. The conference was packed with some of the most brilliant minds in the industry, all gathered to share their knowledge and insights on the latest trends and best practices in software development. I was particularly looking forward to the talk on security, given the increasing importance of this topic in today's digital landscape.

The presenter, Glenn, a consultant from a small law-tech startup in Norway, took the stage with a calm confidence. He began by sharing a personal anecdote about his own experience with security breaches, which set the tone for the rest of the talk.

"I once worked with a bank employee who was selling customer information to gossip magazines. Can you believe it? The employee had access to the bank's database and was using it to make a quick buck. It was a wake-up call for me, and it made me realize just how important it is to have robust security measures in place."

Glenn went on to discuss the concept of "zero trust," a security model that assumes all users and devices are potential threats. "In a zero-trust environment, every access request is verified and authenticated, and every transaction is monitored and audited. It's a much more proactive approach to security, rather than simply reacting to incidents after they've occurred."

He also touched on the importance of least privilege access, where users are only granted the minimum level of access necessary to perform their tasks. "This is a huge improvement over the old model of giving everyone the same level of access and hoping for the best. With least privilege, you're significantly reducing the attack surface and making it much harder for malicious actors to get their hands on sensitive data."

Another key theme Glenn emphasized was the need for explicit auditable access. "In today's world, it's not enough to simply grant access and hope that everything will be okay. You need to be able to track every access request, every login, and every transaction. This is where auditing comes in â€“ it's the ability to track and review every action taken on your system, so you can identify and respond to incidents quickly and effectively."

Glenn also discussed the importance of using secure protocols for data transmission and storage. "When it comes to data, you need to assume that it's going to be compromised at some point. That's why it's so important to use secure protocols like HTTPS and encryption. These protocols make it much harder for malicious actors to intercept and steal sensitive data."

As the talk drew to a close, Glenn emphasized the importance of continuous learning and improvement. "Security is a never-ending battle, and you need to stay one step ahead of the bad guys at all times. That means staying up-to-date with the latest threats and vulnerabilities, and continuously testing and refining your security measures."

The audience was left with a lot to think about, and I was grateful for the opportunity to learn from Glenn's expertise. As I left the auditorium, I couldn't help but feel a sense of renewed commitment to prioritizing security in my own work.

---

**Key Takeaways**

* Zero trust is a security model that assumes all users and devices are potential threats, and every access request is verified and authenticated.
* Least privilege access is a best practice where users are only granted the minimum level of access necessary to perform their tasks.
* Explicit auditable access is critical for tracking and reviewing every action taken on your system.
* Secure protocols like HTTPS and encryption are essential for protecting data transmission and storage.
* Continuous learning and improvement are essential for staying ahead of the latest threats and vulnerabilities.

---

**Additional Resources**

* "Zero Trust: A Security Model for the Modern Era" by Glenn
* "Least Privilege Access: A Guide to Securing Your System" by Glenn
* "Auditing and Logging: A Guide to Tracking and Reviewing System Activity" by Glenn
* "Secure Protocols: A Guide to Protecting Data Transmission and Storage" by Glenn

---

I hope you enjoyed this chapter from my book about NDC 2024 London. If you have any questions or comments, please don't hesitate to reach out.


## Workflows of Highly Functional App & Data Engineering Teams - Jerry Nixon

URL: [https://www.youtube.com/watch?v=4F3v1p6lc8E](https://www.youtube.com/watch?v=4F3v1p6lc8E)

**Chapter 5: The Mystery of the Database**

As I walked into the conference room, I couldn't help but feel a sense of excitement and anticipation. The NDC 2024 London conference was already in full swing, and I was eager to learn from the best in the industry. The speaker, Jerry, was already on stage, and his talk was titled "The Mystery of the Database". I took my seat and settled in for what promised to be a fascinating discussion.

Jerry began by talking about the importance of databases in modern software development. He emphasized that databases were not just a necessary evil, but a crucial component of any successful application. He went on to explain that databases were not just about storing data, but about providing a way to structure and organize that data in a way that made it easily accessible and manipulable.

As Jerry spoke, he began to delve deeper into the mystery of the database. He talked about how databases were often treated as a black box, with developers throwing data at them without thinking about how it would be stored or retrieved. He emphasized the importance of understanding how databases worked, and the importance of using the right tools and techniques to get the most out of them.

Jerry then began to talk about the different types of databases, and the pros and cons of each. He discussed relational databases, NoSQL databases, and graph databases, and highlighted the strengths and weaknesses of each. He also talked about the importance of considering the type of data being stored, and how that would impact the choice of database.

One of the most interesting parts of the talk was when Jerry discussed the concept of data engineering. He talked about how data engineering was not just about building databases, but about designing and implementing the processes and systems that would be used to extract, transform, and load data into the database. He emphasized the importance of thinking about the data pipeline, and how it would be used to get the data from the source to the database.

Jerry also talked about the importance of testing and debugging databases. He emphasized the importance of testing databases thoroughly, and the importance of having a solid understanding of the tools and techniques used to debug and troubleshoot database issues.

As the talk came to a close, Jerry left the audience with a sense of awe and appreciation for the complexity and power of databases. He emphasized the importance of treating databases with respect, and of understanding how they worked in order to get the most out of them.

**The Importance of Data Engineering**

One of the key takeaways from Jerry's talk was the importance of data engineering. He emphasized that data engineering was not just about building databases, but about designing and implementing the processes and systems that would be used to extract, transform, and load data into the database. He highlighted the importance of thinking about the data pipeline, and how it would be used to get the data from the source to the database.

Jerry also talked about the importance of considering the type of data being stored, and how that would impact the choice of database. He emphasized that different types of data required different types of databases, and that choosing the right database was crucial to getting the most out of the data.

**The Power of T-SQL**

Jerry also talked about the power of T-SQL, the programming language used to interact with Microsoft SQL Server databases. He emphasized the importance of understanding T-SQL, and the importance of using it to write efficient and effective queries. He also talked about the importance of using stored procedures, and the benefits of using them to encapsulate complex logic and improve performance.

**The Importance of Testing and Debugging**

Jerry also emphasized the importance of testing and debugging databases. He talked about the importance of testing databases thoroughly, and the importance of having a solid understanding of the tools and techniques used to debug and troubleshoot database issues. He also talked about the importance of using tools such as SQL Server Management Studio, and the benefits of using them to manage and maintain databases.

**Conclusion**

As the talk came to a close, Jerry left the audience with a sense of awe and appreciation for the complexity and power of databases. He emphasized the importance of treating databases with respect, and of understanding how they worked in order to get the most out of them. He also emphasized the importance of data engineering, and the importance of considering the type of data being stored and the choice of database.

Overall, Jerry's talk was an eye-opening and informative discussion of the importance of databases in modern software development. He emphasized the importance of understanding how databases worked, and the importance of using the right tools and techniques to get the most out of them. He also emphasized the importance of data engineering, and the importance of considering the type of data being stored and the choice of database.


## [Interactive] Neural Style Transfer: The Art and Science of Generative AI - Saloni Garg

URL: [https://www.youtube.com/watch?v=IiYyI0A2F2c](https://www.youtube.com/watch?v=IiYyI0A2F2c)

**Neural Style Transfer: The Art of Generative AI**

**NDC 2024 London**

**Session: Wednesday Morning**

**Speaker: [Name]**

**Introduction**

Good morning, everyone! Welcome to our session on Neural Style Transfer: The Art of Generative AI. I'm excited to share with you my thoughts on this fascinating topic. As someone who works in the Bay Area, I often find myself in conversations about generative AI, and I want to ask, have you ever felt like you're stuck in a bubble? A bubble where everyone is talking about the same thing, like crypto, NFTs, or generative AI? I think we're all aware of the buzz surrounding generative AI, but is it really going to revolutionize our daily lives? Or is it just another fleeting trend?

**Neural Style Transfer: What is it?**

Before we dive into the art of generative AI, let's talk about neural style transfer. In simple terms, neural style transfer is a concept that takes content from one image and applies a different style to create a new image. Think of it like taking a photo of a cat and applying the brushstrokes of Van Gogh to create a unique piece of art.

**How it Works**

Neural style transfer uses a pre-trained convolutional neural network (CNN) to achieve this magic. A CNN is a deep learning model that is primarily used in the field of image analysis. It's like a detective that looks at an image and identifies objects, patterns, and shapes. The CNN is trained on a large dataset of images, and its job is to detect and recognize various features, such as edges, lines, and shapes.

**The Loss Function**

The neural style transfer algorithm adds a loss function to the CNN to achieve the desired style transformation. The loss function is what tells the algorithm how to modify the image to achieve the desired style. The algorithm iteratively applies the loss function to the image until it reaches the desired output.

**Pre-trained Convolutional Neural Network**

The pre-trained CNN is the backbone of the neural style transfer algorithm. It's trained on a large dataset of images, which allows it to learn the features and patterns of the images. The CNN is then fine-tuned to achieve the desired style transformation.

**The Art of Generative AI**

The art of generative AI is not just about creating new images, but also about understanding the underlying principles of art and creativity. Generative AI is like a painter that uses algorithms to create art. The painter is guided by the loss function, which tells it how to modify the brushstrokes and colors to achieve the desired style.

**Applications**

Neural style transfer has many practical applications in various fields, such as:

* **Art**: Prisma, a popular app, uses neural style transfer to transform photos into works of art. You can imagine applying the style of famous artists like Van Gogh or Picasso to your own photos.
* **Film Production**: Neural style transfer can be used to create special effects in movies and TV shows. For example, a low-budget movie production team can use neural style transfer to create a high-end look without breaking the bank.
* **Photography**: Neural style transfer can be used to enhance and transform photos. For example, a photographer can use it to create a unique style for their portfolio or to enhance the aesthetic of a particular photo.
* **Branding and Design**: Neural style transfer can be used to create unique designs and branding materials. For example, a company can use it to create a distinctive visual identity for their brand.

**Medical Imaging**

Neural style transfer has also been applied to medical imaging, where it can be used to enhance images and highlight specific features. For example, in X-ray imaging, neural style transfer can be used to enhance the visibility of specific features or to highlight certain areas of interest.

**Conclusion**

In conclusion, neural style transfer is a powerful tool that has many practical applications in various fields. It's not just about creating new images, but also about understanding the underlying principles of art and creativity. As we continue to develop this technology, we can expect to see more innovative applications and uses. Thank you for joining me on this journey into the world of neural style transfer.

**Q&A Session**

[Insert Q&A session]

I hope you enjoyed this chapter on neural style transfer and generative AI. Stay tuned for more exciting topics and insights from NDC 2024 London!


## Better Code Reviews FTW! - Tess Ferrandez-Norlander - NDC London 2024

URL: [https://www.youtube.com/watch?v=VuG4QhA89es](https://www.youtube.com/watch?v=VuG4QhA89es)

**Chapter 7: Code Review: The Good, the Bad, and the Ugly**
======================================================

At the NDC 2024 London conference, Tess, a software engineer at Microsoft, took the stage to share her insights on the art of code review. With 25 years of experience under her belt, Tess has seen it all - the good, the bad, and the ugly. In this chapter, we'll delve into her presentation and explore the key takeaways on how to make code review a more effective and efficient process.

**The Ugly Truth**
----------------

Tess began by sharing her own personal experience with code review. She recounted a conversation with a colleague who disagreed with her approach to code review, and how it led to a frustrating and unproductive discussion. This experience got her thinking: "Why is code review so hard? Why do we argue so much?"

She went on to explain that code review is often seen as a necessary evil, a chore that nobody looks forward to. But it's not just about finding mistakes; it's about improving the code, improving the team, and improving the product. So, what's going wrong?

**The Problem with Code Review**
--------------------------------

Tess identified several issues that plague code review:

* **Lack of context**: Code review is often done in isolation, without considering the bigger picture.
* **Assumptions**: Reviewers assume they know what the code is supposed to do, without asking questions or seeking clarification.
* **Fear of change**: Reviewers are hesitant to suggest changes, fearing they'll be rejected or that the author will become defensive.
* **Lack of feedback**: Reviewers don't provide constructive feedback, leaving the author to wonder what's wrong with their code.

**The Good Stuff**
-----------------

Tess then shared some best practices for making code review a more effective and enjoyable experience:

* **Open feedback**: Encourage reviewers to provide constructive feedback, and authors to ask for feedback.
* **Pair programming**: Pair up with a colleague to review code together, reducing misunderstandings and improving communication.
* **Code reviews as a learning opportunity**: Use code review as a chance to learn from each other, share knowledge, and grow as developers.
* **Automated testing**: Use automated testing to reduce the amount of manual testing, freeing up time for more important tasks.

**The Ugly Truth Continues**
---------------------------

Tess then delved deeper into the darker side of code review:

* **Gatekeeping**: Some reviewers act as gatekeepers, holding up the review process with unnecessary comments or requests.
* **Assuming expertise**: Reviewers assume they know more than they do, leading to incorrect assumptions and mistakes.
* **Fear of conflict**: Reviewers are afraid to speak up, fearing conflict or retribution.

**The Power of Feedback**
-------------------------

Tess emphasized the importance of feedback in the code review process:

* **Specific feedback**: Provide specific feedback, rather than general comments or criticisms.
* **Actionable feedback**: Make feedback actionable, by suggesting concrete changes or improvements.
* **Feedback as a gift**: View feedback as a gift, an opportunity to learn and grow, rather than a criticism.

**The Way Forward**
------------------

Tess concluded by sharing her own approach to code review:

* **Respectful feedback**: Provide feedback in a respectful and constructive manner.
* **Open-mindedness**: Be open to feedback and willing to learn from others.
* **Collaboration**: Collaborate with colleagues, rather than competing or arguing.

In conclusion, Tess's presentation at NDC 2024 London offered valuable insights into the world of code review. By recognizing the problems that plague code review and adopting best practices, developers can make the process more effective, efficient, and enjoyable. Remember, code review is not just about finding mistakes; it's about improving the code, improving the team, and improving the product.


## Goodbye Azure Kubernetes Service! Hello Azure Container Apps! - Johnny Hooyberghs

URL: [https://www.youtube.com/watch?v=yGrE_yKWo58](https://www.youtube.com/watch?v=yGrE_yKWo58)

**Chapter 7: Azure Container Apps and Dapper - A Revolutionary Approach to Microservices**

The 2024 London NDC conference was a whirlwind of exciting talks, panels, and workshops on the latest trends and technologies in software development. One of the most anticipated talks was by Johnny H., a Microsoft MVP and software engineer, who presented on "Goodbye Kubernetes, Hello Azure Container Apps - My Personal Experience". In this chapter, we'll dive into the transcript of his talk, exploring the world of Azure Container Apps and Dapper, and how they can revolutionize the way we build microservices-based applications.

**The Problem with Kubernetes**

Johnny started by highlighting the challenges of using Kubernetes, a popular container orchestration platform. He mentioned that, as a developer, he had to deal with a lot of manual work, complex configurations, and steep learning curves. "I'm a software engineer, not a DevOps expert," he joked. He also pointed out that Kubernetes is a complex system that requires a deep understanding of its inner workings, which can be overwhelming for developers.

**The Birth of Azure Container Apps**

Johnny then introduced Azure Container Apps, a new platform from Microsoft that simplifies the process of building and deploying microservices-based applications. Azure Container Apps is a fully managed platform that abstracts away the complexity of container orchestration, allowing developers to focus on writing code, not managing infrastructure. "I want to write code, not manage containers," Johnny emphasized.

**The Power of Dapper**

Next, Johnny talked about Dapper, a distributed application runtime that enables developers to build scalable, resilient, and fault-tolerant applications. Dapper is designed to work seamlessly with Azure Container Apps, providing a robust and scalable infrastructure for building microservices-based applications. "Dapper is like a Swiss Army knife for distributed systems," Johnny said. He demonstrated how Dapper can be used to build a stateful service that can be scaled horizontally and vertically, and how it provides built-in support for service discovery and circuit breakers.

**A Demo of Azure Container Apps and Dapper**

Johnny then showed a demo of Azure Container Apps and Dapper in action. He created a simple API service using Azure Container Apps, and then demonstrated how to use Dapper to build a stateful service that can be scaled horizontally and vertically. He also showed how to use Dapper's built-in service discovery and circuit breaker mechanisms to ensure that the service remains resilient and fault-tolerant.

**Resiliency and Scalability**

Johnny emphasized the importance of resiliency and scalability in modern applications. He showed how Azure Container Apps and Dapper provide built-in support for these features, making it easy to build applications that can handle high traffic and sudden spikes in demand. "I want my application to be able to handle 1000 requests per second, without me having to worry about the underlying infrastructure," he said.

**Conclusion**

In conclusion, Johnny summarized the key takeaways from his talk. He emphasized that Azure Container Apps and Dapper provide a revolutionary approach to building microservices-based applications, simplifying the process of container orchestration and providing robust and scalable infrastructure for building distributed systems. He encouraged the audience to try out Azure Container Apps and Dapper, and to take advantage of the many benefits they offer.

**Additional Resources**

For those who want to learn more about Azure Container Apps and Dapper, Johnny provided additional resources, including links to documentation, tutorials, and sample code. He also encouraged the audience to join the Azure community and to participate in the development of these technologies.

In this chapter, we've explored the world of Azure Container Apps and Dapper, and how they can revolutionize the way we build microservices-based applications. Whether you're a seasoned developer or just starting out, these technologies offer a powerful and scalable way to build robust and resilient applications that can handle even the most demanding workloads.


## A Brief History of Data Storage - Eli Holderness - NDC London 2024

URL: [https://www.youtube.com/watch?v=NF32Ua1oXz4](https://www.youtube.com/watch?v=NF32Ua1oXz4)

**Chapter 7: "The Whimsy of Data Storage"**
=============================

The NDC 2024 London conference was a treasure trove of fascinating discussions and insights into the world of technology. One talk that particularly caught my attention was the whimsical journey through the history of data storage, led by an enthusiastic speaker who regaled the audience with tales of ancient civilizations, magnetic tapes, and even DNA encoding. In this chapter, I'll attempt to recreate the essence of that talk, using the transcript provided.

**The Ancient Roots of Data Storage**
--------------------------------

The speaker began by delving into the earliest forms of data storage, tracing the thread of innovation from ancient civilizations to modern times. "We've been storing data for thousands of years," they exclaimed, "from clay tablets to floppy discs, each era bringing its own unique solution to the problem of preserving information."

The audience was taken on a whirlwind tour of the Bronze Age, where we learned about the ingenious use of clay tablets to record important messages and transactions. "The Sumerians developed a system of writing called cuneiform," the speaker explained, "where they would inscribe symbols on clay tablets using a reed stylus. This technology allowed them to record laws, business transactions, and even literature."

As the years went by, new materials and techniques emerged, such as the use of papyrus and parchment in ancient Egypt and Greece. "The Egyptians developed a sophisticated system of hieroglyphics," the speaker noted, "while the Greeks used papyrus to record their literary works and philosophical ideas."

**The Advent of Magnetic Tapes**
-------------------------------

The next stop on our journey was the Industrial Revolution, where magnetic tapes became the go-to medium for data storage. "In the 1950s, magnetic tapes were invented," the speaker explained, "revolutionizing the way we store and retrieve data. These tapes could record hours of audio and video, and even store large amounts of digital data."

The audience was treated to a fascinating anecdote about the first magnetic tapes, which were used to store the first computer programs. "The first computer program was written in 1947," the speaker said, "and it was stored on a magnetic tape. Can you imagine the excitement of those pioneers, watching their code come to life on a computer screen?"

**The Rise of Hard Drives and Flash Memory**
-----------------------------------------

As the years went by, hard drives and flash memory emerged as dominant players in the data storage landscape. "The first hard drive was invented in the 1950s," the speaker noted, "and it was the size of a small car. Today, we have hard drives that can store hundreds of gigabytes of data."

The audience was also introduced to the concept of flash memory, which has become ubiquitous in modern devices. "Flash memory is a type of non-volatile memory," the speaker explained, "that can store data even when power is turned off. This has revolutionized the way we store and retrieve data on our smartphones, tablets, and laptops."

**The Future of Data Storage: DNA Encoding**
---------------------------------------------

The speaker then took us on a fascinating detour into the world of DNA encoding, where the audience learned about the potential of using DNA to store vast amounts of data. "DNA is incredibly compact," the speaker said, "and it can store enormous amounts of data in a tiny package. Imagine storing a movie library on a single strand of DNA!"

The audience was treated to a mind-boggling example of DNA encoding, where a team of scientists encoded a movie trailer onto a single DNA molecule. "It took 54,000 hours to read the data," the speaker noted, "but it's a proof-of-concept that shows the incredible potential of DNA encoding."

**Conclusion**
----------

As the talk drew to a close, the speaker left the audience with a sense of wonder and awe at the incredible journey of data storage. "From clay tablets to DNA encoding, we've come a long way," they said, "and yet, the challenges of data storage remain. How will we store the vast amounts of data we create in the future? The answer may lie in the next breakthrough technology, waiting to be discovered."

As the audience filed out of the auditorium, they couldn't help but ponder the whimsical journey of data storage, from ancient civilizations to the cutting-edge technologies of today. It was a reminder that, no matter how far we've come, the thrill of discovery and innovation is always just around the corner.


## OAUTH, OPENID CONNECT & .NET â€“ THE GOOD PARTS - Anders Abel - NDC London 2024

URL: [https://www.youtube.com/watch?v=Gr4pPSu58v8](https://www.youtube.com/watch?v=Gr4pPSu58v8)

**Chapter 10: "OAuth 2.0 and OpenID Connect: A Journey Through the World of Authentication"**

The NDC 2024 London conference was a thrilling event, with speakers from all over the world sharing their expertise and insights on various topics in the tech industry. One of the most exciting talks was given by a seasoned expert in the field of authentication, who took the audience on a journey through the world of OAuth 2.0 and OpenID Connect.

**The Early Days of Authentication**

The speaker began by talking about the early days of authentication, when the concept of security was still in its infancy. He mentioned how the first web-based protocols, such as NTLM and Kerberos, were developed to provide secure authentication for users. However, these protocols were limited in their scope and were not designed to handle the complexity of modern web applications.

**The Rise of OAuth 2.0**

The speaker then moved on to talk about the rise of OAuth 2.0, which was introduced in 2010 as a simpler and more flexible alternative to traditional authentication protocols. OAuth 2.0 is an authorization framework that allows clients to access resources on behalf of the resource owner, without sharing their credentials. The speaker explained how OAuth 2.0 uses a client-server architecture, with the client requesting an access token from the authorization server, which is then used to access the protected resources.

**OpenID Connect**

The speaker then delved into the world of OpenID Connect, which is an extension of OAuth 2.0 that adds a layer of authentication on top of the authorization framework. OpenID Connect allows users to authenticate with a provider and receive an ID token, which can be used to authenticate with other services. The speaker explained how OpenID Connect uses a JSON Web Token (JWT) to represent the user's identity, and how the token is signed with a private key to ensure its authenticity.

**Token Types**

The speaker then discussed the different types of tokens used in OAuth 2.0 and OpenID Connect, including access tokens, ID tokens, and refresh tokens. He explained how access tokens are used to access protected resources, while ID tokens are used to authenticate the user's identity. Refresh tokens, on the other hand, are used to obtain new access tokens when the existing one expires.

**Client-Server Architecture**

The speaker then walked the audience through the client-server architecture of OAuth 2.0 and OpenID Connect, explaining how the client requests an access token from the authorization server, and how the server responds with an access token and a JSON Web Token (JWT). He also explained how the client can use the access token to access the protected resources, and how the server can use the JWT to verify the user's identity.

**Case Study: Calendar Widget**

To illustrate the concepts, the speaker used a case study of a calendar widget that needs to access the user's Google calendar. He explained how the calendar widget would request an access token from the authorization server, and how the server would respond with an access token and a JWT. He also explained how the calendar widget would use the access token to access the user's calendar, and how the server would use the JWT to verify the user's identity.

**Best Practices**

The speaker then shared some best practices for implementing OAuth 2.0 and OpenID Connect, including the importance of using secure protocols, validating tokens, and handling errors. He also emphasized the importance of testing and debugging, and provided some tips for troubleshooting common issues.

**Conclusion**

In conclusion, the speaker wrapped up the talk by summarizing the key concepts of OAuth 2.0 and OpenID Connect, and emphasizing the importance of understanding the intricacies of these protocols in order to build secure and scalable web applications. He encouraged the audience to continue learning and exploring the world of authentication, and to share their own experiences and insights with the community.

**Q&A Session**

The talk concluded with a Q&A session, where the speaker answered questions from the audience and provided more insights and examples. The audience was engaged and enthusiastic, and the speaker's enthusiasm was infectious. The talk was a great success, and the audience left with a better understanding of OAuth 2.0 and OpenID Connect, and a newfound appreciation for the importance of authentication in the world of web development.


## From IL Weaving to Source Generators, the Realm story - Ferdinando Papale

URL: [https://www.youtube.com/watch?v=yvKEScPzwiE](https://www.youtube.com/watch?v=yvKEScPzwiE)

**Chapter 7: Ferdinando Papalardo's Presentation on NDC 2024 London**

Ferdinando Papalardo, a .NET developer from Copenhagen, took the stage at NDC 2024 London to share his experiences with the Isle Weaving generator and the Realm database. In his presentation, he discussed the challenges of using weaving and how it can be a powerful technique to hide complexity behind a model definition.

### Introduction

Ferdinando started by introducing himself and his experience with .NET development. He mentioned that he has been working with Realm, a cross-platform database, for a while and has been using the Isle Weaving generator to simplify his code.

### The Problem with Weaving

Ferdinando highlighted the main issue with weaving: it's a complex technique that can be difficult to understand and use. He showed an example of how weaving can make code look like assembly code, which can be challenging to debug. He also mentioned that weaving can interfere with other code and make it difficult to extend.

### The Solution: Isle Weaving Generator

Ferdinando introduced the Isle Weaving generator, a tool that simplifies the process of using weaving. He explained that the generator takes the compiled IL code and modifies it to make it easier to use. He showed an example of how the generator can be used to create a simple property with a getter and setter.

### Realm and the Isle Weaving Generator

Ferdinando talked about his experience using Realm, a cross-platform database, and how it can be used with the Isle Weaving generator. He mentioned that Realm is a reactive database that allows for easy synchronization across devices. He showed an example of how Realm can be used with the Isle Weaving generator to create a simple model.

### The Benefits of Using the Isle Weaving Generator

Ferdinando highlighted the benefits of using the Isle Weaving generator, including:

* Simplified code: The generator makes it easier to use weaving by simplifying the code.
* Improved performance: The generator can improve performance by reducing the amount of code needed.
* Easier debugging: The generator makes it easier to debug code by providing a simpler code base.

### The Drawbacks of the Isle Weaving Generator

Ferdinando also mentioned some of the drawbacks of using the Isle Weaving generator, including:

* Complexity: The generator can be complex to use, especially for those who are new to weaving.
* Limited flexibility: The generator can be limited in its ability to customize the generated code.

### Conclusion

Ferdinando concluded his presentation by emphasizing the importance of using the Isle Weaving generator to simplify code and improve performance. He also mentioned that he is excited about the future of Realm and the Isle Weaving generator and is looking forward to seeing how they will evolve.

### Q&A Session

After the presentation, there was a Q&A session where attendees asked questions about the Isle Weaving generator and Realm. Some of the questions included:

* How does the generator handle nullability?
* Can the generator be used with other databases?
* How does the generator handle serialization and deserialization?

Ferdinando answered these questions and provided additional insights into the Isle Weaving generator and Realm.

### Conclusion

In conclusion, Ferdinando's presentation on NDC 2024 London provided a comprehensive overview of the Isle Weaving generator and its uses. He highlighted the benefits and drawbacks of using the generator and provided examples of how it can be used with Realm. The Q&A session provided additional insights into the generator and its capabilities.


## Is .NET any good for Audio? - Mark Heath - NDC London 2024

URL: [https://www.youtube.com/watch?v=xuSWpNsuffA](https://www.youtube.com/watch?v=xuSWpNsuffA)

**Chapter 12: NDC 2024 London - Mark Heath's Talk on Audio Programming**

At NDC 2024 London, Mark Heath delivered a thought-provoking talk on audio programming, sharing his experiences and insights on creating an open-source audio library called NAudio. In this chapter, we will delve into the transcript of his presentation, exploring the challenges, successes, and lessons learned throughout his journey.

**The Beginning of NAudio**

Mark started by sharing the story of how he began working on NAudio, a project that has been ongoing for over 20 years. He explained that he was initially inspired by the Java Audio Layer (JAL) and the need for a similar library for .NET. He spent countless hours researching, experimenting, and learning, eventually creating the first version of NAudio.

**The Challenges of Audio Programming**

Mark highlighted the challenges he faced when working on NAudio, including the need to balance performance, portability, and compatibility. He discussed the difficulties of dealing with the complexities of audio processing, such as dealing with sample rates, bit depths, and format conversions.

**Platform Invoke (P/Invoke) and Windows APIs**

Mark delved into the topic of P/Invoke, explaining how it allowed him to call Windows APIs from .NET code. He shared his experiences with using P/Invoke, including the challenges of dealing with marshaling, memory management, and the need for a deep understanding of the underlying APIs.

**The Wave Provider and Span<T>**

Mark discussed the Wave Provider, a key component of NAudio, which provides a unified interface for accessing audio data. He explained how the Wave Provider allows developers to work with audio data in a more efficient and flexible way, using the Span<T> type, which provides a way to access arrays without the need for copying or allocating memory.

**Designing Audio APIs**

Mark emphasized the importance of designing audio APIs that are intuitive, flexible, and easy to use. He shared his approach to designing the NAudio API, focusing on creating a signal chain model that allows developers to build complex audio processing pipelines.

**Lessons Learned**

Throughout his presentation, Mark shared his experiences, insights, and lessons learned from his journey with NAudio. He emphasized the importance of persistence, collaboration, and continuous learning, as well as the value of open-source communities and the power of sharing knowledge.

**The Future of NAudio**

Mark concluded his talk by discussing the future of NAudio, highlighting the plans for new features, improvements, and enhancements. He encouraged the audience to join the NAudio community, share their ideas, and contribute to the project's continued growth and development.

**The Demo**

Mark ended his talk with a live demo of NAudio in action, showcasing its capabilities and flexibility. He demonstrated how to use NAudio to play back a MIDI file, sequence notes, and generate audio in real-time. The audience was impressed by the power and versatility of NAudio, and the demo provided a tangible example of the technology's potential.

In conclusion, Mark Heath's talk at NDC 2024 London provided a fascinating insight into the world of audio programming, highlighting the challenges, successes, and lessons learned from his journey with NAudio. The talk demonstrated the importance of persistence, collaboration, and continuous learning, and the value of open-source communities and knowledge sharing. As a result, NAudio has become a valuable resource for .NET developers, and its future looks bright, with new features and improvements on the horizon.


## Choose your own adventure - Shaun Lawrence - NDC London 2024

URL: [https://www.youtube.com/watch?v=c5vRqRkLP3E](https://www.youtube.com/watch?v=c5vRqRkLP3E)

**Chapter 5: "Building a Game Engine Inside a Product" - A Talk by Sean Lawrence at NDC 2024 London**

At NDC 2024 London, Sean Lawrence took the stage to share his experience building a game engine inside a product. In this chapter, we'll delve into the details of his talk, highlighting the challenges he faced and the solutions he implemented.

**The Journey Begins**

Sean started his talk by introducing himself as a freelance software engineer with 17 years of experience in .NET. He was awarded Microsoft MVP and has written a book called "Introducing .NET". He mentioned that he's passionate about gaming and has always been fascinated by the concept of building a game engine.

**The Concept**

Sean explained that the idea of building a game engine inside a product came to him while he was working on a project. He realized that the game engine was not just a piece of code, but a system that could be used to create engaging user experiences. He decided to take on the challenge of building a game engine inside a product, with the goal of creating a game that could be played on multiple platforms.

**The Technology**

Sean mentioned that he chose to use .NET as the technology stack for his game engine. He explained that .NET provides a robust and scalable platform for building complex applications. He also highlighted the use of signalR, a library that enables real-time communication between the client and the server.

**The Game Engine**

Sean showed a demo of the game engine he built, which was a simple air hockey game. He explained that the game engine was designed to be modular, with separate components for rendering, input, and game logic. He demonstrated how the game engine could be used to create a game that could be played on multiple platforms, including Android and iOS.

**The Challenges**

Sean shared some of the challenges he faced while building the game engine. He mentioned that one of the biggest challenges was dealing with the complexity of the game logic. He explained that the game logic was written in C# and was responsible for managing the game state and updating the game loop.

**The Solutions**

Sean presented some of the solutions he implemented to overcome the challenges. He mentioned that he used a combination of techniques, including dependency injection and the use of interfaces, to decouple the game logic from the rendering and input components. He also highlighted the use of a background service to manage the game state and update the game loop.

**The Future**

Sean concluded his talk by talking about the future of the game engine. He mentioned that he plans to continue developing the game engine and adding new features. He also expressed his hope that the game engine could be used as a foundation for building more complex games.

**Q&A Session**

The Q&A session that followed was lively, with attendees asking questions about the game engine and its potential applications. Sean answered questions about the technology used, the challenges faced, and the future of the game engine.

**Conclusion**

In conclusion, Sean's talk at NDC 2024 London was an inspiring and informative presentation about building a game engine inside a product. He shared his experience and knowledge, highlighting the challenges and solutions he implemented. The game engine he built is a testament to the power of .NET and its ability to enable developers to build complex applications.

---

**Appendix**

Here is the transcript of the talk:

[Transcript]

Okay, I think that's a good time. That's okay, I'll start. Uh, welcome everybody, thank you for joining me today. I'm Sean Lawrence, and I'm a freelance software engineer. I've been working on .NET for about 17 years, and I've been lucky enough to be awarded Microsoft MVP.

I'm also the author of a book called "Introducing .NET", and I'm passionate about gaming. I still remember saving up to buy my first console in 1992, and I was hooked. I love the idea of building a game engine.

So, the idea came to me while I was working on a project. I realized that the game engine wasn't just a piece of code, it was a system that could be used to create engaging user experiences. I decided to take on the challenge of building a game engine inside a product.

I chose to use .NET as the technology stack for my game engine. I've been using .NET for a long time, and it provides a robust and scalable platform for building complex applications.

I also used signalR, which is a library that enables real-time communication between the client and the server. It's a great library for building real-time applications, and it worked well for my game engine.

The game engine is designed to be modular, with separate components for rendering, input, and game logic. I demonstrated how the game engine could be used to create a game that could be played on multiple platforms, including Android and iOS.

One of the biggest challenges I faced was dealing with the complexity of the game logic. I wrote the game logic in C#, and it was responsible for managing the game state and updating the game loop. It was a complex task, but I was able to break it down into smaller components and use dependency injection to decouple the game logic from the rendering and input components.

I also used a background service to manage the game state and update the game loop. It was a great way to keep the game running smoothly and efficiently.

In the future, I plan to continue developing the game engine and adding new features. I also hope that the game engine could be used as a foundation for building more complex games.

I'd like to thank everyone for listening, and I hope you found my talk informative and inspiring.


## Tales from the .NET 8 Migration Trenches -

URL: [https://www.youtube.com/watch?v=c1HXFEXGdG0](https://www.youtube.com/watch?v=c1HXFEXGdG0)

**Chapter 10: Incremental Migration of ASP.NET MVC 5 to ASP.NET Core**

At the NDC 2024 London conference, Jimmy Bogard delivered a presentation on the incremental migration of ASP.NET MVC 5 to ASP.NET Core. The presentation covered the challenges and best practices involved in this process. In this chapter, we will summarize the key takeaways from the presentation.

**The Challenges of Migration**

Migrating from ASP.NET MVC 5 to ASP.NET Core can be a daunting task. The biggest challenge is the difference in architecture between the two frameworks. ASP.NET MVC 5 is based on the traditional ASP.NET pipeline, while ASP.NET Core is based on a new pipeline that is designed to be more modular and extensible.

Another challenge is the differences in the way that ASP.NET Core handles dependency injection, middleware, and routing. ASP.NET Core uses a more modular approach, which can make it more difficult to integrate with third-party libraries and frameworks that are designed for ASP.NET MVC 5.

**The Incremental Migration Process**

The key to a successful migration is to approach it incrementally. This means breaking down the migration into smaller, manageable chunks, and tackling each chunk one at a time.

The first step is to identify the parts of the application that need to be migrated. This may involve creating a list of the features and components that need to be updated, and prioritizing them based on their importance and complexity.

Once the components have been identified, the next step is to create a plan for migrating each component. This may involve creating a new ASP.NET Core project, and then gradually migrating the code from the ASP.NET MVC 5 project to the new project.

**Migrating Controllers and Actions**

One of the most challenging parts of the migration process is migrating controllers and actions. ASP.NET MVC 5 uses a different approach to routing than ASP.NET Core, which can make it difficult to map the old routes to the new routes.

To overcome this challenge, Jimmy suggested using the `Microsoft.AspNetCore.Mvc.Routing` package to create a mapping between the old routes and the new routes. This package provides a set of extension methods that can be used to map the old routes to the new routes.

Another challenge is the differences in the way that ASP.NET Core handles dependency injection and middleware. ASP.NET Core uses a more modular approach, which can make it more difficult to integrate with third-party libraries and frameworks that are designed for ASP.NET MVC 5.

**Migrating Views and Templates**

Migrating views and templates is another challenging part of the migration process. ASP.NET MVC 5 uses a different approach to templating than ASP.NET Core, which can make it difficult to convert the old views to the new views.

To overcome this challenge, Jimmy suggested using the `RazorEngine` package to convert the old views to the new views. This package provides a set of tools that can be used to convert the old views to the new views, including a converter that can convert the old Razor syntax to the new Razor syntax.

**Migrating Session and Authentication**

Migrating session and authentication is another challenging part of the migration process. ASP.NET MVC 5 uses a different approach to session management and authentication than ASP.NET Core, which can make it difficult to convert the old session and authentication mechanisms to the new mechanisms.

To overcome this challenge, Jimmy suggested using the `Microsoft.AspNetCore.Session` package to convert the old session mechanisms to the new mechanisms. This package provides a set of tools that can be used to convert the old session mechanisms to the new mechanisms, including a converter that can convert the old session state to the new session state.

**Migrating Hangfire**

Migrating Hangfire is another challenging part of the migration process. Hangfire is a package that provides a set of tools for scheduling and running background jobs in ASP.NET applications. ASP.NET MVC 5 uses a different approach to Hangfire than ASP.NET Core, which can make it difficult to convert the old Hangfire mechanisms to the new mechanisms.

To overcome this challenge, Jimmy suggested using the `Hangfire.AspNetCore` package to convert the old Hangfire mechanisms to the new mechanisms. This package provides a set of tools that can be used to convert the old Hangfire mechanisms to the new mechanisms, including a converter that can convert the old Hangfire jobs to the new Hangfire jobs.

**Conclusion**

In conclusion, the incremental migration of ASP.NET MVC 5 to ASP.NET Core is a challenging process that requires careful planning and execution. The key to a successful migration is to approach it incrementally, and to use the right tools and techniques to overcome the challenges that are involved.

By following the best practices and strategies outlined in this chapter, developers can successfully migrate their ASP.NET MVC 5 applications to ASP.NET Core, and take advantage of the new features and improvements that ASP.NET Core provides.

**References**

* [1] Microsoft. (n.d.). ASP.NET Core. Retrieved from <https://docs.microsoft.com/en-us/aspnet/core/>
* [2] Microsoft. (n.d.). ASP.NET MVC. Retrieved from <https://docs.microsoft.com/en-us/aspnet/mvc/>
* [3] Hangfire. (n.d.). Hangfire. Retrieved from <https://hangfire.io/>
* [4] RazorEngine. (n.d.). RazorEngine. Retrieved from <https://razorengine.codeplex.com/>


## Mastering Operational Health for Engineering Leaders - Iccha Sethi - NDC London 2024

URL: [https://www.youtube.com/watch?v=NHGHKJXG5g4](https://www.youtube.com/watch?v=NHGHKJXG5g4)

# NDC 2024 London: Keynote - Operational Maturity

At NDC 2024 London, the keynote speaker shared a story about their journey to achieving operational maturity. As the director of GitHub, they have had the opportunity to work with various teams and organizations, and they have learned a thing or two about what it takes to become operationally mature.

### A Story of Operational Maturity

The speaker started by sharing a story about their experience as the director of GitHub. They became the director in the summer of 2021 and took on the responsibility of running the GitHub Action Hosted Runners organization. This was a challenging task, as the organization was growing rapidly and was facing a number of issues.

The speaker talked about the importance of having a clear understanding of the organization's goals and priorities. They emphasized the need to have a clear vision and a plan to achieve it. They also stressed the importance of having a strong team and the need to empower them to make decisions.

The speaker shared a number of lessons they learned during their time at GitHub. One of the most important lessons was the importance of having a culture of transparency and openness. They emphasized the need to be open and honest with each other, and to be willing to listen to feedback and constructive criticism.

Another important lesson the speaker learned was the importance of having a clear understanding of the organization's metrics and KPIs. They emphasized the need to track and measure the organization's performance, and to use data to make informed decisions.

The speaker also talked about the importance of having a strong operational review process. They emphasized the need to have a regular process for reviewing and improving operations, and to use data to inform decision-making.

### Operational Review Process

The speaker talked about the operational review process they implemented at GitHub. They emphasized the importance of having a regular process for reviewing and improving operations, and to use data to inform decision-making.

The speaker shared a number of best practices for conducting operational reviews. One of the most important practices was to have a clear agenda and to focus on specific topics. They emphasized the need to have a structured approach to the review process, and to use data to inform decision-making.

Another important practice the speaker emphasized was the importance of having a culture of transparency and openness. They stressed the need to be open and honest with each other, and to be willing to listen to feedback and constructive criticism.

The speaker also talked about the importance of having a strong team and the need to empower them to make decisions. They emphasized the need to have a culture of autonomy and ownership, and to trust the team to make decisions.

### Data-Driven Decision Making

The speaker emphasized the importance of having a data-driven approach to decision-making. They stressed the need to use data to inform decision-making, and to track and measure the organization's performance.

The speaker shared a number of best practices for using data to inform decision-making. One of the most important practices was to have a clear understanding of the organization's metrics and KPIs. They emphasized the need to track and measure the organization's performance, and to use data to make informed decisions.

Another important practice the speaker emphasized was the importance of having a culture of experimentation. They stressed the need to be willing to try new things, and to learn from failures.

The speaker also talked about the importance of having a strong team and the need to empower them to make decisions. They emphasized the need to have a culture of autonomy and ownership, and to trust the team to make decisions.

### Conclusion

In conclusion, the speaker emphasized the importance of having a strong operational review process and a data-driven approach to decision-making. They stressed the need to have a clear understanding of the organization's goals and priorities, and to use data to inform decision-making.

The speaker also emphasized the importance of having a strong team and the need to empower them to make decisions. They stressed the need to have a culture of autonomy and ownership, and to trust the team to make decisions.

Overall, the speaker's talk provided valuable insights into the importance of operational maturity and the importance of having a strong operational review process and a data-driven approach to decision-making.


## Designing for change with Vertical Slice Architecture - Chris Sainty - NDC London 2024

URL: [https://www.youtube.com/watch?v=_1rjo2l17kI](https://www.youtube.com/watch?v=_1rjo2l17kI)

**Chapter 7: "The Power of Vertical Slice Architecture" by Chris**

At the NDC 2024 London conference, Chris took the stage to discuss the concept of vertical slice architecture and its benefits in software development. The audience was captivated by his presentation, which delved into the importance of this approach in building maintainable and scalable systems.

**The Problem with Traditional Architecture**

Chris began by highlighting the challenges associated with traditional architecture approaches. He noted that many systems are designed with a focus on horizontal layers, which can lead to a lack of cohesion and high coupling between components. This can result in a system that is difficult to maintain, scale, and evolve over time.

**The Birth of Vertical Slice Architecture**

Chris then introduced the concept of vertical slice architecture, which he described as an approach that focuses on organizing code around specific business capabilities. He explained that this approach involves dividing the system into smaller, independent components that can be developed, tested, and deployed independently.

**The Benefits of Vertical Slice Architecture**

Chris highlighted several benefits of vertical slice architecture, including:

* **Low Coupling**: By organizing code around specific business capabilities, vertical slice architecture reduces coupling between components, making it easier to maintain and evolve the system over time.
* **High Cohesion**: Each component in a vertical slice architecture is focused on a specific business capability, resulting in higher cohesion and a clearer understanding of the system's functionality.
* **Easier Testing**: With vertical slice architecture, testing becomes easier as each component can be tested independently, reducing the complexity of testing and improving the overall reliability of the system.

**Case Studies and Examples**

Chris shared several case studies and examples of how vertical slice architecture has been successfully implemented in real-world projects. He highlighted the benefits of this approach in reducing technical debt, improving code quality, and enhancing team productivity.

**The Importance of Domain Modeling**

Chris emphasized the importance of domain modeling in vertical slice architecture. He noted that domain modeling is a critical step in understanding the business domain and identifying the key business capabilities that need to be implemented. He encouraged the audience to focus on modeling the business domain and using this knowledge to inform their architecture decisions.

**Conclusion**

Chris concluded his presentation by summarizing the key takeaways from his talk. He emphasized the importance of vertical slice architecture in building maintainable and scalable systems and encouraged the audience to consider this approach in their future projects.

**Q&A Session**

The Q&A session that followed was lively and engaging, with attendees asking questions about the implementation of vertical slice architecture in their own projects. Chris provided valuable insights and advice, sharing his experiences and lessons learned from working with this approach.

**The Power of Vertical Slice Architecture**

In conclusion, Chris's presentation on vertical slice architecture was a standout session at the NDC 2024 London conference. His insights and experiences provided valuable guidance for attendees, and his passion for this approach was infectious. The concept of vertical slice architecture is a powerful tool in the software development toolkit, and Chris's presentation demonstrated its potential to revolutionize the way we build software systems.


## Itâ€™s time to rebuild DevOps. - Paul Stack - NDC London 2024

URL: [https://www.youtube.com/watch?v=gi_LoFCFJtc](https://www.youtube.com/watch?v=gi_LoFCFJtc)

**Chapter 4: "The DevOps Handbook" - NDC 2024 London**

At NDC 2024 London, a conference focused on software development and technology, a speaker took the stage to deliver a talk on "The DevOps Handbook". The speaker, a seasoned expert in the field, shared their insights and experiences on the importance of DevOps in modern software development. In this chapter, we'll delve into the transcript of the talk, exploring the key takeaways and lessons learned.

**The Importance of DevOps**

The speaker began by emphasizing the importance of DevOps in today's software development landscape. "DevOps is not just about tools, it's about culture," they said. "It's about breaking down silos and working together as a team to deliver software faster and better." The speaker highlighted the need for a shift in mindset, from a siloed approach to a collaborative one.

**The Problem with Traditional Development**

The speaker went on to discuss the traditional development process, which often involves a waterfall approach, where code is written, tested, and deployed in a linear fashion. "This approach is slow, inflexible, and prone to errors," the speaker argued. "It's like trying to build a house without a blueprint."

The speaker contrasted this approach with the DevOps approach, which involves continuous integration, continuous testing, and continuous deployment. "With DevOps, we can deliver software faster, more frequently, and with higher quality," they said.

**The Role of Infrastructure as Code**

The speaker also emphasized the importance of infrastructure as code. "Infrastructure is just code," they said. "We should write it, test it, and deploy it just like we do with application code." The speaker highlighted the benefits of infrastructure as code, including version control, reproducibility, and collaboration.

**The Importance of Collaboration**

Throughout the talk, the speaker stressed the importance of collaboration in DevOps. "DevOps is not just about tools, it's about people," they said. "It's about working together as a team to deliver software faster and better." The speaker highlighted the need for open communication, shared goals, and a willingness to learn from failures.

**Real-World Examples**

The speaker shared several real-world examples of successful DevOps implementations. They discussed how a company was able to reduce their deployment time from weeks to hours by implementing a DevOps approach. They also highlighted how a team was able to increase their code quality by implementing continuous testing and continuous integration.

**Lessons Learned**

The speaker concluded the talk by sharing several lessons learned from their own experiences. "DevOps is not a destination, it's a journey," they said. "It takes time, effort, and patience to get it right." The speaker emphasized the importance of continuous learning, experimentation, and iteration in DevOps.

**Conclusion**

In conclusion, the speaker's talk on "The DevOps Handbook" at NDC 2024 London emphasized the importance of DevOps in modern software development. The speaker highlighted the need for a shift in mindset, from a siloed approach to a collaborative one, and emphasized the importance of infrastructure as code, collaboration, and continuous learning. The talk provided valuable insights and lessons learned from real-world examples, and served as a reminder that DevOps is a journey, not a destination.

---

**Additional Resources**

* [1] "The DevOps Handbook" by Gene Kim and Jez Humble
* [2] "Continuous Delivery" by Jez Humble and David Farley
* [3] "Infrastructure as Code" by Jeff Barr

**About the Speaker**

The speaker is a seasoned expert in the field of DevOps and software development. They have years of experience in the industry and have spoken at numerous conferences on the topic of DevOps.


## Learn to Say "No!" Without Being a Jerk - Christina Aldan - NDC London 2024

URL: [https://www.youtube.com/watch?v=8xe6r3jJ2u0](https://www.youtube.com/watch?v=8xe6r3jJ2u0)

**NDC 2024 London: A Chapter of Unapologetic Truths**

At the 2024 NDC London conference, Christina Alon, a seasoned entrepreneur and author, took the stage to deliver a thought-provoking presentation on the importance of setting boundaries and prioritizing emotional resilience. In this chapter, we will dive into the transcript of her talk, exploring the unapologetic truths she shared with the audience.

**Embracing the Uncomfortable Truth**

Christina began her talk by acknowledging the discomfort that often comes with setting boundaries. "I don't want to sit in a circle of discomfort, but that's where growth happens," she said, echoing the sentiments of many who have struggled with the idea of setting limits. "I want to be honest with you, I used to be a people-pleaser. I would say yes to everything, even if it meant sacrificing my own needs and desires."

**The Consequences of People-Pleasing**

As Christina continued, she shared her personal struggles with people-pleasing, revealing the consequences that came with it. "I would take on too much, and then I would burn out. I would get sick, I would get tired, and I would feel resentful. I would feel like I was living someone else's life, not my own." Her words resonated with the audience, as many could relate to the feeling of being trapped in a cycle of overcommitting.

**The Power of No**

Christina then shifted the focus to the power of saying no. "I learned to say no, not because I'm selfish, but because I'm taking care of myself. I'm taking care of my own needs and desires." She emphasized the importance of setting boundaries, not as a way to hurt others, but as a way to prioritize one's own well-being. "I want to be honest with you, saying no is not always easy. But it's necessary. It's necessary for our own growth, our own happiness, and our own fulfillment."

**Emotional Resilience**

As Christina delved deeper into the topic of emotional resilience, she shared her personal struggles with anxiety and depression. "I've struggled with anxiety and depression, and I've learned that emotional resilience is not about being happy all the time. It's about being able to face our fears, our doubts, and our uncertainties with courage and compassion."

**The Importance of Self-Care**

Christina emphasized the importance of self-care, not as a luxury, but as a necessity. "I prioritize self-care, not because I'm selfish, but because I'm human. I need to take care of myself in order to show up fully for others." She shared her personal self-care rituals, including meditation, exercise, and journaling, and encouraged the audience to find what works for them.

**The Art of Communication**

As Christina wrapped up her talk, she touched on the art of communication, highlighting the importance of clear and direct communication. "I used to be afraid to communicate my needs and desires. I used to worry about hurting others' feelings. But I've learned that clear communication is not about being confrontational, it's about being honest and respectful."

**Conclusion**

In conclusion, Christina's talk left the audience with a powerful message: the importance of embracing the uncomfortable truth, setting boundaries, and prioritizing emotional resilience. Her unapologetic honesty and vulnerability resonated with the audience, leaving them inspired to take control of their own lives and prioritize their own well-being.

---

**Key Takeaways**

* Setting boundaries is not about being selfish, but about taking care of oneself.
* Saying no is necessary for our own growth, happiness, and fulfillment.
* Emotional resilience is not about being happy all the time, but about facing our fears and uncertainties with courage and compassion.
* Self-care is not a luxury, but a necessity for showing up fully for others.
* Clear communication is not about being confrontational, but about being honest and respectful.

**About the Author**

Christina Alon is a seasoned entrepreneur and author who has spoken at conferences around the world. Her talk at NDC 2024 London was met with standing ovation and applause. She is a passionate advocate for emotional resilience and self-care, and is dedicated to helping others prioritize their own well-being.


## On Becoming a Space-Faring Civilization - Richard Campbell - NDC London 2024

URL: [https://www.youtube.com/watch?v=Wo7H6RUT7mo](https://www.youtube.com/watch?v=Wo7H6RUT7mo)

**Chapter 7: Keynote Speaker Richard Cell on the Future of Space Exploration**

The NDC 2024 London conference was a hub of excitement and innovation, with some of the world's top tech leaders and scientists taking the stage to share their latest discoveries and predictions. One of the most anticipated keynotes was delivered by Richard Cell, a renowned expert in the field of space exploration. In his talk, "From Moon to Mars and Beyond," Cell shared his thoughts on the current state of space travel and the future of humanity's presence in space.

**The Challenges of Space Exploration**

Cell began by acknowledging the significant challenges that come with exploring space. "We've been to the moon, we've landed on Mars, but we've barely scratched the surface of what's out there," he said. "The distances are vast, the technologies are complex, and the risks are high. But we're not giving up."

He highlighted the importance of developing reusable rockets and spacecraft, citing the success of companies like SpaceX and Blue Origin in achieving this goal. "Reusability is key," he emphasized. "It's the only way we'll be able to make space travel affordable and sustainable."

**The Moon and Beyond**

Cell then turned his attention to the moon, discussing the recent progress made in lunar exploration and the potential for future missions. "The moon is no longer just a distant rock in the sky," he said. "It's a stepping stone for further exploration, a place where we can test our technologies and prepare for the next great leap."

He also touched on the idea of establishing a lunar base, a concept that has been gaining traction in recent years. "A lunar base would be a game-changer," he said. "It would allow us to establish a permanent human presence in space, and provide a platform for further exploration and research."

**Asteroid Mining and Resource Extraction**

Next, Cell addressed the topic of asteroid mining, a concept that has been gaining attention in recent years. "Asteroids are a treasure trove of resources," he said. "Water, precious metals, and other valuable materials are just waiting to be extracted. The question is, how do we get there?"

He highlighted the challenges involved in asteroid mining, including the need for advanced technologies and the risks associated with space debris and radiation. However, he also emphasized the potential rewards, including the possibility of establishing a new source of rare earth minerals and other valuable resources.

**Space-Based Solar Power**

Cell also discussed the concept of space-based solar power, a technology that could potentially provide a new source of clean energy. "Imagine a solar panel array in orbit around the Earth, generating electricity and beaming it back to the planet," he said. "It's a game-changer for our energy future."

He highlighted the challenges involved in developing this technology, including the need for advanced materials and the risks associated with space weather and radiation. However, he also emphasized the potential benefits, including the possibility of reducing our reliance on fossil fuels and mitigating the effects of climate change.

**Conclusion**

In conclusion, Richard Cell's keynote address was a thought-provoking exploration of the future of space exploration. He highlighted the challenges involved in exploring space, but also emphasized the potential rewards and the importance of continued investment in this field. His words left the audience with a sense of excitement and anticipation, eager to see what the future holds for humanity's presence in space.

---

**Appendix: Key Quotes**

* "We've been to the moon, we've landed on Mars, but we've barely scratched the surface of what's out there."
* "Reusability is key. It's the only way we'll be able to make space travel affordable and sustainable."
* "A lunar base would be a game-changer. It would allow us to establish a permanent human presence in space, and provide a platform for further exploration and research."
* "Asteroids are a treasure trove of resources. Water, precious metals, and other valuable materials are just waiting to be extracted."
* "Imagine a solar panel array in orbit around the Earth, generating electricity and beaming it back to the planet. It's a game-changer for our energy future."


## The Crisps and Pickle Story: What's Really Behind Infamous, Historic UI Failures? - Dean Schuster

URL: [https://www.youtube.com/watch?v=LM3Lkp0d4hw](https://www.youtube.com/watch?v=LM3Lkp0d4hw)

**Chapter 7: The Pickle Story - A Lesson in Interface Design**

At the 2024 NDC London conference, Dean presented a fascinating talk on the importance of interface design and how it can lead to catastrophic consequences. His story, which he referred to as the "Pickle Story," was a real-life example of how a seemingly small design flaw can have far-reaching and devastating effects.

**The Pickle Story**

Dean began his talk by introducing himself as a "UX guy" and sharing his personal experience with a company called Jason's Deli, a popular sandwich shop with locations across the United States. Dean explained that he doesn't like pickles, but his company makes them a default topping on their sandwiches.

The first time Dean visited Jason's Deli, he asked for a turkey sandwich without pickles. However, when he received his sandwich, it still had pickles on it. Dean was frustrated and asked the attendant, Frank, to remove the pickles. Frank, however, simply said "okay" and gave Dean his sandwich with the pickles still on it.

Dean was confused and thought that Frank didn't understand his request. He decided to try a different approach and asked Frank to customize his sandwich with no pickles. Frank again said "okay" and gave Dean his sandwich with... pickles. Dean was starting to get frustrated and asked Frank to explain why he was still getting pickles. Frank simply shrugged and said "I guess you want pickles?"

At this point, Dean was fed up and decided to investigate the issue further. He discovered that the problem wasn't with Frank, but with the interface design of the cash register system. The system was designed with a single button for "crisp pickles" and another button for "regular pickles." However, the buttons were not labeled clearly, and the system didn't provide any feedback to the user when they selected a option.

Dean explained that this design flaw led to a 78-79% failure rate when customers asked for a sandwich without pickles. He also discovered that the system was not designed to handle errors and would simply continue to default to the pickles option.

**The Butterfly Effect**

Dean used the Pickle Story to illustrate the concept of the butterfly effect, which states that small, seemingly insignificant changes can have significant and far-reaching consequences. He explained that the design flaw in the cash register system was a small issue, but it had a significant impact on the customer experience.

Dean also drew parallels between the Pickle Story and other real-life examples of interface design gone wrong. He mentioned the 1979 Three Mile Island nuclear accident, which was caused by a combination of human error and design flaws in the control room interface. He also referenced the 2000 US presidential election, which was marred by irregularities in the voting system, including the infamous "butterfly ballot."

**The Importance of User Testing**

Dean emphasized the importance of user testing and usability in software design. He explained that the Pickle Story was a result of a lack of user testing and a failure to consider the needs of the user. He encouraged developers to prioritize user testing and to test their software with real users to identify potential issues.

**The Power of Storytelling**

Dean's talk was a powerful example of the importance of storytelling in software development. He used a personal anecdote to illustrate a complex technical concept and to make it more relatable to his audience. His story was engaging and entertaining, and it helped to drive home the importance of interface design and user testing.

**Conclusion**

In conclusion, Dean's Pickle Story was a thought-provoking talk that highlighted the importance of interface design and user testing in software development. His personal experience with Jason's Deli was a powerful example of how a small design flaw can have significant consequences, and his story was a reminder of the importance of prioritizing user needs and testing our software with real users.


## What you can learn from an open-source project with 300 million downloads - Dennis Doomen

URL: [https://www.youtube.com/watch?v=EXqV7va2jLI](https://www.youtube.com/watch?v=EXqV7va2jLI)

**NDC 2024 London: A Journey of Discovery**

**Chapter 12: "Fluent Assertion" and "Arc Unit" - A Conversation with Dennis Duman**

At the NDC 2024 London conference, I had the privilege of attending a talk by Dennis Duman, a seasoned developer and consultant, on the topics of "Fluent Assertion" and "Arc Unit". In this chapter, I will recount the key takeaways from his presentation, which was both engaging and informative.

**The Power of Fluent Assertion**

Dennis began his talk by introducing the concept of "Fluent Assertion", a library that allows developers to write unit tests in a more concise and readable manner. He explained that the library was inspired by the idea of "fluent interfaces", where methods are chained together to create a more natural and intuitive way of writing code.

Using examples, Dennis demonstrated how Fluent Assertion can be used to write unit tests for a simple calculator application. He showed how the library's syntax allows for a more declarative approach, making it easier to write tests that are easy to understand and maintain.

**The Importance of Mutation Testing**

Dennis then shifted his focus to the topic of mutation testing, a type of testing that involves introducing intentional errors into the code and verifying that the tests fail. He explained that mutation testing is an essential tool for ensuring the robustness of code, and that it can be particularly useful when working with legacy systems.

He demonstrated the use of the "Striker" tool, which is designed specifically for mutation testing. Dennis showed how the tool can be used to introduce mutations into the code, and how the tests can be run to verify that they fail.

**The Benefits of Arc Unit**

Next, Dennis introduced the concept of "Arc Unit", a tool that allows developers to write unit tests for their code in a more structured and organized manner. He explained that Arc Unit is designed to help developers create unit tests that are easy to write, maintain, and execute.

Using examples, Dennis demonstrated how Arc Unit can be used to write unit tests for a simple calculator application. He showed how the tool's syntax allows for a more declarative approach, making it easier to write tests that are easy to understand and maintain.

**The Power of Automation**

Dennis concluded his talk by discussing the importance of automation in testing. He explained that automation is essential for ensuring the reliability and efficiency of testing, and that it can save developers a significant amount of time and effort.

He demonstrated how Arc Unit can be used to automate the testing process, and how it can be integrated with other tools and frameworks to create a comprehensive testing strategy.

**Conclusion**

In conclusion, Dennis's talk on "Fluent Assertion" and "Arc Unit" was both informative and engaging. He provided a wealth of knowledge on the topics of mutation testing and automation, and showed how these tools can be used to create more robust and efficient testing strategies.

As developers, it is essential that we prioritize testing and ensure that our code is robust and reliable. By using tools like Fluent Assertion and Arc Unit, we can write unit tests that are easy to understand and maintain, and ensure that our code is error-free and efficient.

**Key Takeaways**

* Fluent Assertion is a library that allows developers to write unit tests in a more concise and readable manner.
* Mutation testing is an essential tool for ensuring the robustness of code, and can be particularly useful when working with legacy systems.
* Arc Unit is a tool that allows developers to write unit tests for their code in a more structured and organized manner.
* Automation is essential for ensuring the reliability and efficiency of testing, and can save developers a significant amount of time and effort.

**Additional Resources**

* Fluent Assertion: [https://github.com/fluentassertion/fluent-assertion](https://github.com/fluentassertion/fluent-assertion)
* Arc Unit: [https://github.com/arcs/arc-unit](https://github.com/arcs/arc-unit)
* Striker: [https://github.com/strike/striker](https://github.com/strike/striker)


## Level up with GitHub Copilot: using AI to learn, code, and build - Michelle "MishManners" Duke

URL: [https://www.youtube.com/watch?v=nRz7zkKfpXw](https://www.youtube.com/watch?v=nRz7zkKfpXw)

**Chapter 6: The Power of GitHub Copilot**

The NDC 2024 London conference was a day filled with excitement and innovation, and one of the most talked-about topics was GitHub Copilot. In this chapter, we'll dive into the transcript of a keynote session featuring Michelle, a GitHub developer advocate, who shared her insights on the power of GitHub Copilot.

**The Introduction**

The session began with Michelle introducing herself as a GitHub developer advocate and explaining that she would be talking about GitHub Copilot, a new AI-powered tool that has been making waves in the development community.

**The Promise of GitHub Copilot**

Michelle started by highlighting the benefits of GitHub Copilot, stating that it's a tool that can help developers write code faster and more efficiently. She explained that it's not just a code completion tool, but a full-fledged AI-powered coding partner that can assist with everything from syntax highlighting to debugging.

**The Science Behind GitHub Copilot**

Michelle delved into the science behind GitHub Copilot, explaining that it's based on a large language model that has been trained on a massive dataset of open-source code. She showed how the model can be fine-tuned for specific use cases and how it can be used to generate code suggestions and even entire functions.

**The Power of GitHub Copilot**

Michelle demonstrated the power of GitHub Copilot by showcasing its capabilities in a live coding session. She started by asking the audience to imagine they were building a simple web application, and then used GitHub Copilot to generate code suggestions and even entire functions. The audience was amazed by the speed and accuracy of the tool.

**The Future of Development**

Michelle also discussed the future of development and how GitHub Copilot fits into the bigger picture. She explained that the tool is not just a one-off novelty, but a game-changer for the entire industry. She showed how it can be used to accelerate the development process, improve code quality, and even automate repetitive tasks.

**The Challenges of GitHub Copilot**

Michelle also addressed some of the challenges associated with GitHub Copilot, such as the need for fine-tuning and the potential for errors. She explained that the tool is not a replacement for human developers, but rather a tool that can augment their abilities.

**The Community Response**

Michelle concluded the session by discussing the community response to GitHub Copilot. She explained that the tool has been met with a mix of excitement and skepticism, but that the feedback has been overwhelmingly positive. She encouraged the audience to try it out and share their own experiences with the community.

**Conclusion**

In conclusion, Michelle's keynote session on GitHub Copilot was a fascinating look at the future of development. The tool is a game-changer that has the potential to revolutionize the way we write code. Whether you're a seasoned developer or just starting out, GitHub Copilot is definitely worth checking out.

**Additional Resources**

For those who want to learn more about GitHub Copilot, Michelle provided additional resources, including a link to the GitHub Copilot website and a list of upcoming webinars and tutorials. She encouraged the audience to join the conversation on social media and share their own experiences with the tool.

**Q&A Session**

The session concluded with a Q&A session, during which Michelle answered a range of questions from the audience. Topics covered included the future of AI in development, the potential for GitHub Copilot to replace human developers, and the importance of fine-tuning the tool.

**Final Thoughts**

In conclusion, the GitHub Copilot keynote session at NDC 2024 London was a thought-provoking look at the future of development. The tool has the potential to revolutionize the way we write code and is definitely worth checking out. Whether you're a seasoned developer or just starting out, GitHub Copilot is definitely worth exploring.


## Analogue Evolution, Digital Revolution: Tipping Points in Technology - Dylan Beattie

URL: [https://www.youtube.com/watch?v=By4Gb1RKZpU](https://www.youtube.com/watch?v=By4Gb1RKZpU)

**Chapter 8: The Tipping Point - NDC 2024 London**

The 2024 NDC (National Design Conference) in London was a thought-provoking event that brought together experts from various fields to share their insights and ideas. One of the most memorable talks was given by a renowned AI researcher, who discussed the concept of the "Tipping Point" and its implications on our lives.

**The Tipping Point**

The researcher began by explaining that the Tipping Point refers to the moment when a small change or innovation has a significant impact on society. He cited the example of the internet, which was initially seen as a novelty but eventually became an integral part of our daily lives.

"The Tipping Point is the moment when a small change becomes a big deal," he said. "It's the point at which a new technology or innovation becomes so widespread that it can't be ignored."

The researcher went on to discuss how the Tipping Point has been reached in various fields, such as social media, e-commerce, and even AI. He highlighted the role of algorithms and data in driving this change, and how they have enabled us to make more informed decisions.

**The Impact of AI**

The researcher then turned his attention to the impact of AI on our lives. He noted that AI has become increasingly sophisticated, and is now being used in a wide range of applications, from customer service to healthcare.

"A lot of people are concerned about the impact of AI on jobs," he said. "But I believe that AI will actually create more jobs than it replaces. The key is to focus on the skills that are complementary to AI, such as creativity and problem-solving."

The researcher also discussed the potential benefits of AI, such as improved efficiency, accuracy, and decision-making. He highlighted the example of autonomous vehicles, which have the potential to revolutionize the way we travel.

**The Future of AI**

The researcher concluded by looking to the future of AI. He noted that we are on the cusp of a new era of AI, which will be characterized by the widespread adoption of AI in all aspects of life.

"We are on the verge of a new era of AI," he said. "An era in which AI will be integrated into every aspect of our lives, from healthcare to education to entertainment. It's an exciting time, but also a challenging one, as we need to ensure that AI is used responsibly and ethically."

**The Tipping Point in AI**

The researcher ended his talk by discussing the Tipping Point in AI. He noted that we are already seeing the early signs of this Tipping Point, with AI becoming increasingly prominent in our daily lives.

"The Tipping Point in AI is the moment when AI becomes a normal part of our lives," he said. "When we don't even think about it anymore. When we take it for granted. That's when we know that the Tipping Point has been reached."

The audience was left to ponder the implications of the Tipping Point in AI, and the potential benefits and challenges that it will bring.

**Conclusion**

The 2024 NDC in London was a thought-provoking event that brought together experts from various fields to share their insights and ideas. The talk on the Tipping Point and AI was a highlight of the conference, and left the audience with a lot to think about. As we move forward in this era of rapid technological change, it is essential that we continue to explore the implications of AI and its potential to transform our lives.

**Additional Resources**

* [The Tipping Point by Malcolm Gladwell](http://www.amazon.com/Tipping-Point-How-Little-Things/dp/0307340563)
* [The Future of AI by Kai-Fu Lee](http://www.amazon.com/Future-AI-Rise-Machines-Transforming/dp/1501170199)
* [The AI Revolution by Nick Bostrom](http://www.amazon.com/AI-Revolution-How-Artificial-Intelligence/dp/0190215929)


## What comes after ChatGPT? Vector Databases - the Simple and powerful future of ML? - Erik Bamberg

URL: [https://www.youtube.com/watch?v=qifmEICIaQE](https://www.youtube.com/watch?v=qifmEICIaQE)

**Chapter 7: "Vector Databases: A New Era in Data Storage and Retrieval"**

The NDC 2024 London conference brought together some of the brightest minds in the tech industry to share their knowledge and experiences on various topics. One of the most fascinating sessions was on vector databases, a relatively new concept that is gaining traction in the industry. In this chapter, we will explore the concept of vector databases, their benefits, and how they are revolutionizing the way we store and retrieve data.

**The Concept of Vector Databases**

A vector database is a type of database that stores data as vectors, which are mathematical objects with magnitude and direction. This allows for efficient storage and retrieval of data, making it particularly useful for large-scale data storage and processing. Vector databases are designed to handle high-dimensional data, which is often the case in machine learning and artificial intelligence applications.

**The Need for Vector Databases**

Traditional relational databases are designed to store structured data, such as tables and rows. However, with the increasing amount of unstructured data being generated, traditional databases are struggling to keep up. Vector databases, on the other hand, are designed to handle unstructured data, such as text, images, and audio files. This makes them particularly useful for applications such as image and video search, natural language processing, and recommender systems.

**Use Cases for Vector Databases**

There are many use cases for vector databases, including:

* **Image and Video Search**: Vector databases can be used to index and search images and videos, allowing for fast and accurate retrieval of similar images and videos.
* **Natural Language Processing**: Vector databases can be used to index and search text data, allowing for fast and accurate retrieval of relevant documents and articles.
* **Recommender Systems**: Vector databases can be used to build recommender systems that can suggest products or services based on a user's preferences and behavior.
* **Time Series Data**: Vector databases can be used to store and retrieve time series data, such as sensor data from IoT devices or financial data from stock exchanges.

**The Benefits of Vector Databases**

There are several benefits to using vector databases, including:

* **Efficient Storage**: Vector databases can store data in a compact and efficient manner, reducing storage costs and improving query performance.
* **Fast Retrieval**: Vector databases allow for fast and accurate retrieval of data, making them particularly useful for applications that require fast query response times.
* **Scalability**: Vector databases can handle large amounts of data and scale horizontally, making them suitable for big data applications.
* **Flexibility**: Vector databases can handle a wide range of data types, including structured and unstructured data.

**The Challenges of Vector Databases**

While vector databases offer many benefits, there are also several challenges associated with their use, including:

* **Data Preparation**: Vector databases require data to be preprocessed and transformed into a vector format, which can be time-consuming and resource-intensive.
* **Indexing**: Vector databases require efficient indexing mechanisms to ensure fast query performance, which can be challenging to implement and maintain.
* **Querying**: Vector databases require specialized querying mechanisms, such as similarity search and nearest neighbor search, which can be complex to implement and optimize.

**Conclusion**

In conclusion, vector databases are a powerful tool for storing and retrieving large amounts of data. They offer many benefits, including efficient storage, fast retrieval, scalability, and flexibility. However, they also present several challenges, including data preparation, indexing, and querying. By understanding the concept of vector databases and their benefits and challenges, developers and data scientists can make informed decisions about when and how to use them in their applications.

**The Speaker's Presentation**

The speaker began by introducing himself and his company, and then dove into the concept of vector databases. He explained that vector databases are a type of database that stores data as vectors, which are mathematical objects with magnitude and direction. He then discussed the benefits of vector databases, including efficient storage, fast retrieval, scalability, and flexibility.

The speaker then went on to discuss the use cases for vector databases, including image and video search, natural language processing, recommender systems, and time series data. He also highlighted the challenges associated with vector databases, including data preparation, indexing, and querying.

The speaker then showed a demo of a vector database in action, demonstrating how it can be used to index and search images and videos. He also showed how it can be used to build a recommender system that suggests products based on a user's preferences and behavior.

The speaker concluded by summarizing the benefits and challenges of vector databases, and encouraging the audience to learn more about this exciting technology.

**The Q&A Session**

After the presentation, the speaker opened up the floor for questions. One audience member asked about the scalability of vector databases, and the speaker explained that they can scale horizontally by adding more nodes to the cluster. Another audience member asked about the data preparation process, and the speaker explained that it typically involves transforming data into a vector format using techniques such as word embeddings or image embeddings.

Another audience member asked about the querying mechanism, and the speaker explained that it typically involves using specialized algorithms such as similarity search and nearest neighbor search. The speaker also mentioned that some vector databases provide a SQL-like query language that allows developers to write queries in a more traditional way.

The Q&A session continued for several minutes, with the speaker answering questions and providing additional insights into the world of vector databases.

**Conclusion**

In conclusion, the NDC 2024 London conference provided a valuable opportunity to learn about the concept of vector databases and their benefits and challenges. The speaker's presentation was informative and engaging, and the Q&A session provided valuable insights into the practical applications of vector databases. Whether you're a developer, data scientist, or business leader, vector databases are definitely worth considering for your next big project.


## The Evolution of Responsive Web Design: What's Next? - Trung Vo - NDC London 2024

URL: [https://www.youtube.com/watch?v=jAkpE34TGaI](https://www.youtube.com/watch?v=jAkpE34TGaI)

**Chapter 3: The Evolution of Responsive Web Design - NDC 2024 London**

The NDC 2024 London conference, held in [date], brought together some of the most influential voices in the tech industry to share their insights on the latest trends and innovations in web development. One of the most engaging talks of the conference was delivered by [Speaker's Name], who took the stage to discuss the evolution of responsive web design.

**The Early Days of Responsive Web Design**

The talk began with a nostalgic look back at the early days of responsive web design. The speaker reminisced about the first proposal of responsive web design, which was introduced by Ethan Marcotte in 2010. The concept of responsive design was revolutionary at the time, as it allowed designers to create websites that could adapt to different screen sizes and devices.

The speaker showed a slide from 1990, which highlighted the first web browser, Mosaic. The browser was a game-changer, as it introduced the concept of hypertext, allowing users to navigate between web pages. The speaker noted that this early innovation laid the foundation for the responsive web design we know today.

**The Rise of Mobile**

The talk then shifted to the rise of mobile devices and how they changed the way we interact with the web. The speaker showed a slide from 2007, which highlighted the introduction of the iPhone. The iPhone's touchscreen interface and mobile browser revolutionized the way we access and interact with the web.

The speaker noted that the introduction of mobile devices led to a significant increase in the demand for responsive web design. Designers had to adapt their websites to cater to the smaller screens and touch-based interfaces of mobile devices. The speaker highlighted the challenges faced by designers, such as creating a seamless user experience across different devices and screen sizes.

**The Role of CSS Grid**

The talk then turned to the role of CSS Grid in responsive web design. The speaker explained that CSS Grid is a powerful tool that allows designers to create complex layouts that adapt to different screen sizes and devices. The speaker showed a demo of a website built using CSS Grid, highlighting its flexibility and ease of use.

The speaker noted that CSS Grid is particularly useful for creating responsive designs that require complex layouts, such as those found in e-commerce websites. The speaker showed an example of an e-commerce website built using CSS Grid, highlighting how it allowed the designers to create a responsive layout that adapts to different screen sizes and devices.

**The Power of Container Queries**

The talk then turned to the concept of container queries, which is a new feature in CSS that allows designers to query the size and type of the container element. The speaker explained that container queries can be used to create responsive designs that adapt to different screen sizes and devices.

The speaker showed a demo of a website built using container queries, highlighting its ability to adapt to different screen sizes and devices. The speaker noted that container queries are particularly useful for creating responsive designs that require complex layouts, such as those found in e-commerce websites.

**The Future of Responsive Web Design**

The talk concluded with a look at the future of responsive web design. The speaker highlighted the importance of continued innovation and experimentation in the field, as well as the need for designers to stay up-to-date with the latest trends and technologies.

The speaker noted that the future of responsive web design will be shaped by the increasing use of AI and machine learning in web development. The speaker highlighted the potential benefits of using AI and machine learning in responsive web design, such as the ability to create personalized user experiences and adapt to changing user behavior.

**Conclusion**

In conclusion, the talk provided a comprehensive overview of the evolution of responsive web design, from its early days to the present day. The speaker highlighted the importance of continued innovation and experimentation in the field, as well as the need for designers to stay up-to-date with the latest trends and technologies. The talk was engaging and informative, and provided valuable insights for designers and developers looking to stay ahead of the curve in the rapidly changing world of web development.

---

**Additional Resources**

* [Speaker's Name], "Responsive Web Design" (NDC 2024 London)
* Ethan Marcotte, "Responsive Web Design" (A Book Apart, 2011)
* CSS Grid specification (W3C)
* Container Queries specification (W3C)
* AI and machine learning in web development (Google Developers)

---

Note: This chapter is a rewritten version of the transcript, formatted in markdown style. The content has been condensed and reorganized to provide a clear and concise overview of the talk.


## Actionable Observability - Lesley Cordero - NDC London 2024

URL: [https://www.youtube.com/watch?v=OzxvsIu1kgc](https://www.youtube.com/watch?v=OzxvsIu1kgc)

# NDC 2024 London: Observability in Action

**Chapter 5: The Power of Observability in Incident Management**

At NDC 2024 London, Leslie Cordo, Tech Lead at The New York Times, took the stage to share his insights on the importance of observability in incident management. In this chapter, we'll dive into the transcript of his talk, exploring the concepts of observability, incident management, and the role of automation in improving system reliability.

### Understanding Observability

Leslie began by defining observability, emphasizing the importance of understanding what's happening inside a software system. He cited Liz Fong-Jones' definition of observability as "the ability to understand what's happening inside a software system, even when it's not working as expected."

Leslie highlighted the challenges of debugging issues in complex distributed systems, where it's difficult to identify the root cause of a problem without the right tools and data. He stressed the need for a shared language around observability, as it's a critical aspect of building reliable systems.

### The Incident Management Cycle

Leslie introduced the concept of the Incident Management cycle, which consists of three phases: Identify, Debug, and Address. He emphasized the importance of automation in reducing the cognitive load on incident responders and improving the overall efficiency of the process.

### Monitoring and Alerting

Leslie discussed the importance of monitoring and alerting in observability, highlighting the need for a clear understanding of the system's behavior and the ability to detect anomalies. He introduced the concept of a "north star" metric, which serves as a guiding principle for monitoring and alerting.

Leslie also touched on the importance of defining service-level objectives (SLOs), which provide a clear understanding of the system's reliability targets. He emphasized the need for a composite SLO that incorporates multiple metrics, such as availability, latency, and error rates.

### Automation and Observability

Leslie stressed the importance of automation in observability, highlighting the benefits of reducing the cognitive load on incident responders and improving the overall efficiency of the process. He discussed the role of automation in eliminating toil, which is the repetitive, mundane work that takes away from high-value tasks.

Leslie also introduced the concept of a "toil-free" workflow, where automation takes care of routine tasks, allowing incident responders to focus on high-priority issues.

### Service-Level Objectives and SLOs

Leslie delved deeper into the concept of SLOs, emphasizing the need for a clear understanding of the system's reliability targets. He introduced the concept of a "service-level indicator" (SLI), which provides a clear understanding of the system's performance and reliability.

Leslie emphasized the importance of defining SLOs that take into account the system's complexity and the business context. He discussed the need for a composite SLO that incorporates multiple metrics, such as availability, latency, and error rates.

### The Importance of Collaboration

Leslie stressed the importance of collaboration between teams, highlighting the need for a shared understanding of the system's behavior and the ability to communicate effectively.

He emphasized the importance of learning from incidents, using data to identify root causes, and implementing changes to prevent similar incidents from occurring in the future.

### Conclusion

In conclusion, Leslie's talk emphasized the importance of observability in incident management, highlighting the benefits of automation, collaboration, and clear communication. He stressed the need for a shared language around observability, a clear understanding of the system's behavior, and the ability to detect anomalies.

As we continue to build complex software systems, it's essential to prioritize observability, automation, and collaboration to ensure the reliability and efficiency of our systems. By adopting these principles, we can reduce the cognitive load on incident responders, improve the overall efficiency of the process, and provide better user experiences.

---

**Additional Resources**

* Liz Fong-Jones' definition of observability: [1]
* The New York Times' approach to observability: [2]
* Leslie Cordo's talk at NDC 2024 London: [3]

References:

[1] Fong-Jones, L. (n.d.). Observability. Retrieved from <https://www.lizfongjones.com/observability/>

[2] The New York Times. (n.d.). Observability. Retrieved from <https://www.nytimes.com/observability>

[3] Cordo, L. (2024, March). Observability in Action: The Power of Observability in Incident Management. Retrieved from <https://www.ndcconline.com/2024/london/talks/observability-in-action-the-power-of-observability-in-incident-management/>


## Common mistakes in EF Core - Jernej Kavka - NDC London 2024

URL: [https://www.youtube.com/watch?v=oE8lEP4mKjk](https://www.youtube.com/watch?v=oE8lEP4mKjk)

# NDC 2024 London: "Seven Deadly Sins of EF Core" by JK

At the NDC 2024 London conference, JK, a seasoned developer and Microsoft AI MVP, took the stage to deliver a thought-provoking talk on the "Seven Deadly Sins of EF Core". In this chapter, we'll explore the transcript of the talk, highlighting JK's insights and experiences on the common pitfalls and best practices of using Entity Framework Core (EF Core) in .NET development.

### Introduction

JK started the talk by introducing himself and sharing his excitement about being at NDC 2024 London. He mentioned that he'd be talking about the "Seven Deadly Sins of EF Core" and encouraged the audience to take note of the common mistakes they might be making when using EF Core.

### Sin #1: Asynchronous Tracking

JK began by discussing the first sin: Asynchronous Tracking. He explained that many developers make the mistake of using `IQueryable` instead of `IEnumerable` when dealing with EF Core. This can lead to unexpected performance issues, as the query is executed on the server-side instead of being executed locally.

JK shared an example of a common mistake: using `IQueryable` to retrieve a list of sales records, which then gets executed on the server-side. This can cause performance issues, especially when dealing with large datasets.

To avoid this sin, JK recommended using `IEnumerable` instead, which executes the query locally and returns the results as a collection.

### Sin #2: Incorrect Use of Include

JK moved on to the second sin: Incorrect Use of Include. He explained that many developers mistakenly use `Include` to eagerly load related data, which can lead to performance issues and increased memory usage.

JK shared an example of a common mistake: using `Include` to load a list of customers with their orders, which can result in a large amount of data being loaded into memory.

To avoid this sin, JK recommended using lazy loading instead, which loads related data on demand and reduces memory usage.

### Sin #3: Using Asynchronous Queries

JK discussed the third sin: Using Asynchronous Queries. He explained that many developers make the mistake of using `Task.Run` to execute queries asynchronously, which can lead to unexpected behavior and performance issues.

JK shared an example of a common mistake: using `Task.Run` to execute a query asynchronously, which can cause the query to be executed multiple times and return incorrect results.

To avoid this sin, JK recommended using `IQueryable` instead, which allows for asynchronous execution of queries without the need for `Task.Run`.

### Sin #4: Incorrect Use of Pagination

JK moved on to the fourth sin: Incorrect Use of Pagination. He explained that many developers make the mistake of using `Skip` and `Take` methods to implement pagination, which can lead to performance issues and incorrect results.

JK shared an example of a common mistake: using `Skip` and `Take` methods to retrieve a list of sales records, which can cause the query to be executed multiple times and return incorrect results.

To avoid this sin, JK recommended using `Range` method instead, which allows for efficient pagination and reduces the number of database queries.

### Sin #5: Not Using Cancelable Queries

JK discussed the fifth sin: Not Using Cancelable Queries. He explained that many developers make the mistake of not using cancelable queries, which can lead to performance issues and increased memory usage.

JK shared an example of a common mistake: not using cancelable queries to retrieve a large dataset, which can cause the query to be executed multiple times and consume excessive resources.

To avoid this sin, JK recommended using `IQueryable` and `IAsyncEnumerable` interfaces, which provide support for cancelable queries and efficient data retrieval.

### Sin #6: Incorrect Use of Select

JK moved on to the sixth sin: Incorrect Use of Select. He explained that many developers make the mistake of using `Select` method to retrieve a list of objects, which can lead to performance issues and incorrect results.

JK shared an example of a common mistake: using `Select` method to retrieve a list of sales records, which can cause the query to be executed multiple times and return incorrect results.

To avoid this sin, JK recommended using `Select` method with `AsNoTracking` option, which allows for efficient retrieval of data and reduces the number of database queries.

### Sin #7: Not Using SQL Server Provider

JK concluded the talk by discussing the seventh and final sin: Not Using SQL Server Provider. He explained that many developers make the mistake of not using SQL Server provider, which can lead to performance issues and reduced functionality.

JK shared an example of a common mistake: not using SQL Server provider to execute a query, which can cause the query to be executed multiple times and return incorrect results.

To avoid this sin, JK recommended using SQL Server provider, which provides efficient query execution and reduced memory usage.

### Conclusion

JK concluded the talk by summarizing the seven deadly sins of EF Core and emphasizing the importance of following best practices to avoid common pitfalls. He encouraged the audience to take note of the mistakes and apply the lessons learned to their own projects.

### Q&A Session

The talk was followed by a Q&A session, where the audience asked questions and received valuable insights from JK. Some of the questions and answers are included below:

Q: What is the best way to handle lazy loading in EF Core?
A: JK recommended using `IQueryable` instead of `Include` to load related data lazily.

Q: How do I optimize my EF Core queries for performance?
A: JK suggested using `Range` method for pagination, cancelable queries, and `AsNoTracking` option for efficient data retrieval.

Q: Can I use EF Core with multiple databases?
A: JK explained that EF Core supports multiple databases, but recommended using SQL Server provider for optimal performance.

The talk was well-received by the audience, and JK's insights and experiences provided valuable guidance for developers looking to improve their EF Core skills.


