## OpenShift Serverless: Using Knative in next-generation AI applications |DevNation Tech Talk

URL: [https://www.youtube.com/watch?v=ck1DKgmPSsQ](https://www.youtube.com/watch?v=ck1DKgmPSsQ)

# Exploring the Future of AI, OpenShift Serverless, and Next-Generation Neural Networks

During a captivating tech talk moderated by Kevin Hoffman, Marcus Ranum and I delved into the intricacies of modern Artificial Intelligence (AI), leveraging OpenShift serverless technology, and envisioning the potentials of next-generation neural networks. The conversation spanned over 35 insightful minutes, encompassing a wide array of thought-provoking ideas. Here, we've distilled the essence into an 800-word blog post, preserving key takeaways and intellectual musings.

## Defining AI, ML, and Opinions

The session began with dissecting the terms 'Artificial Intelligence', 'Machine Learning', and the ever-evolving opinions surrounding these technologies. While AI often refers to the concept of simulating human intelligence in machines, Machine Learning is a specific technique within this realm that enables systems to learn from data patterns instead of being explicitly programmed.

Ranum voiced concerns over labeling certain cutting-edge computations as 'AI', expressing discomfort in how the term gets misused and overhyped, creating a fear of an impending Terminator-like future. Hoffman countered by emphasizing that AI, particularly in its adaptive form, can be metaphorically viewed as a set of heuristics based on repeated number sequences—an arbitrary rule base evolving with time into something more sophisticated, but still far from reaching the mythical 'stale entropy' often associated with true randomness.

## OpenShift Serverless and AI Interface

The talk transitioned to discuss how technologies like OpenShift and its serverless implementations could interface with Artificial Intelligence. Hoffman highlighted OpenShift as a Red Hat Enterprise platform built upon Kubernetes, enabling developers to manage containerized applications easily in a scalable manner. This serverless approach allows applications to run efficiently while consuming only the required resources at any given time.

The discussion emphasized how OpenShift, as an enterprise-grade tool, could provide a solid foundation for building and deploying AI applications, often requiring massive computational power or handling enormous amounts of data. Hoffman shared his ongoing fascination with OpenShift's 'knative' feature set, which includes K native (Kubernetes Native), providing an event-driven, serverless experience built specifically for modern containerized applications.

## Next-Generation AI Problem Space and Atomic Neural Components

The conversation then delved into the concept of a next-generation Artificial Intelligence problem space and the idea of atomic neural components as building blocks for complex AI systems. Hoffman used an example of creating an AI system to analyze artistic images, highlighting how today's AI models require massive training sets to achieve even moderate accuracy, often leading to diminishing returns with each new dataset addition. In contrast, he envisioned a future where an adaptive, atomic neural component could be assembled into more complex AI systems, providing the foundation for a versatile, scalable, and efficient artificial intelligence landscape.

Hoffman introduced 'K native' as a potential framework for building such adaptive, event-driven AI architectures. He presented a simplistic demo where a 'neuron' event system processed data based on configurable thresholds, creating an adaptive behavior within the system using simple atomic components. This system, Hoffman argued, would significantly reduce complexity in building advanced AI applications by harnessing the power of these tiny, efficient neural 'atoms'.

## The Potential of Cloud Events and Autoscaling

The conversation then steered towards the potential of cloud events to enhance AI applications' scalability and performance. Hoffman pointed out how traditional machine learning models often struggle with massive datasets or real-time responsiveness due to their resource-intensive nature. He lauded the concept of 'Cloud Events', a standardized, interoperable mechanism for event distribution within a distributed system, as an enabler for more efficient and scalable AI applications.

Hoffman also emphasized the importance of autoscaling in the serverless world. OpenShift's ability to scale applications up or down based on demand (zero-replica mode) was highlighted, showcasing how such technologies could make a significant difference when handling event-driven AI workloads. In this context, K native emerged as an interesting candidate, offering eventing and autoscaling capabilities within the AI problem space.

## Thought Engine – An Evolving Idea

As the discussion wound down, Hoffman revisited his 'Thought Engine' concept, a seven-year-old idea rooted in mimetics and memory modeling. Despite never writing the whitepaper due to other career commitments, he expressed his fascination with the possibility of a 'baby Thought Engine' emerging as a practical application in the future. The Thought Engine concept revolved around building an adaptive system based on mimicking human thought processes through a memory-based mechanism, which could learn from data without the need for extensive training sets or rule-heavy programming—a tantalizing prospect within the realm of AI development.

The discourse ended with mutual gratitude and optimism surrounding the evolution of technology, acknowledging the fascinating path of Artificial Intelligence, serverless architectures, and the potential breakthroughs in neural network adaptability. While much remains to be explored, these conversations spark curiosity and fuel innovation towards an exciting future for AI applications.

---

From a tech talk brimming with intriguing ideas, we've distilled this essence into an 800-word blog post, preserving the key thoughts while propelling the conversation around OpenShift serverless, next-generation neural networks, and the intersection of AI and cloud events. May these insights spark curiosity and inspire further exploration in the ever-evolving world of technology.


